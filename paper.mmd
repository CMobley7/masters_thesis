Base Header Level: 2
latex input: metadata
latex author: Christopher James Mobley
latex title: TITLE OF THESIS
bibtex: masters_thesis
myreporttype: Thesis
mydegree: Masters of Science
myyear: 2016
mydepartment: Mechanical Engineering
latex input: vtthesis/setup.tex
latex footer: vtthesis/footer.tex

# Introduction

Unlike the substantial benefits seen in the manufacturing industry through automation and robotics, automation and robotics in construction (ARC) has lagged far behind in adoption [#Balaguer]. Consequently, when compared with other industries, construction has seen a significant decrease in productivity, as well as an increase in workplace injuries/fatalities over the last several decades [#Rojas2003]. While several technical complexities inherent in construction have hindered the development and adoption of field construction robots [4], through the capitalization of advances made by other industries, ARC can quickly close this gap. Thereby allowing dangerous and or mundane repetitive tasks to be accomplished autonomously. Thus, causing an increase in productivity and a decrease in workplace injuries/fatalities [1]. However, ARC faces two unique challenges when compared to other industries. Unlike manufacturing environments, which are tightly controlled, typical construction sites tend to lack structure and are continuously evolving. In addition, the reversal in relationship between the part and manipulator has dramatically increased the complexity of the problem to be solved [#Feng2015]. Instead of the part appearing at a fixed manipulator, the manipulator must now move to and localize itself with respect to the part. The remainder of this paper is structured as follows: In Section 2, the author’s technical approach is outlined, with particular focus on problem two, and experimental results are shown. Conclusions are then drawn and future work discussed in Section 3.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/future_transformable_production_system.png}
  \caption{Vision for future transformable production systems.}
  \label{fig:vision_for_future_transformable_production_systems}
\end{figure}
-->

# Background

Insert text outlining upcoming sections.

## ROS Concepts

Insert Text Defining ROS.

### ROS Communication

Insert Text Regarding Node/Topic/Service Communication.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/simple_ros_node_diagram.png}
  \caption{Simple ROS Node Flowchart}
  \label{fig:simple_ros_node_flowchart}
\end{figure}
-->

### Unified Robot Description Format and Transformations

Insert Regarding How To Setup Up TFs on a Robot and How TF Communication Works.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/robots_with_without_tfs.png}
  \caption{Robot Model with TFs}
  \label{fig:robot_model_with_tfs}
\end{figure}
-->

## Simultaneous Localization and Mapping Concepts

Insert Text Outlining the Basic Intuition Behind SLAM.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/overview_of_slam_framework.png}
  \caption{Overview of SLAM Framework. [Riisgaard et al. 2005]}
  \label{fig:overview_of_slam_framework}
\end{figure}
-->

Insert Text Explaining How ROS Uses SLAM to Create 2-D Occupancy Grid Maps and What How They Are Used.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/2d_occupancy_grid_map.png}
  \caption{Example of 2-D Occupancy Grid Map Produced.}
  \label{fig:example_of 2-D Occupancy Grid Map Produced}
\end{figure}
-->

Insert Text Specifying Available 2-D SLAM Techniques and Why KartoSLAM was Chosen.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/real_world_performance_analysis_ros_slam_algorithms.png}
  \caption{Real World Performance Analysis of ROS Available SLAM Algorithms. [Santos et al. 2013]}
  \label{fig:real_world_performance_analysis_of_ros_available_slam_algorithms}
\end{figure}
-->

<!--
\begin{table}[H]
\centering
\caption{Real World Error Estimation for ROS Available SLAM Algorithm. [Santos et al. 2013]}
\label{real_world_error_estimation_for_ros_available_slam_algorithm}
\resizebox{0.75\linewidth}{!}{\begin{tabular}{@{}|c|c|c|c|c|@{}}
\toprule
\multicolumn{5}{|c|}{Real World Experiments}            \\ \midrule
HectorSLAM & GMapping & KartoSLAM & CoreSLAM & LagoSLAM \\ \midrule
1.1972     & 2.1716   & 1.0318    & 14.75333 & 3.0264   \\ \midrule
0.5094     & 0.6945   & 0.3742    & 7.9463   & 0.8181   \\ \midrule
1.0656     & 1.6354   & 0.9080    & 7.5824   & 2.5236   \\ \bottomrule
\end{tabular}}
\end{table}
-->

## Localization and Path Planning Concepts

Insert Text Outlining The Navigation Stack's Framework.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/ros_navigation_stack_setup.png}
  \caption{ROS Navigation Stack setup.}
  \label{ros_navigation_stack_setup}
\end{figure}
-->

Insert Text Explaining How Adaptive Monte Carlo Localization Works. Specifically Start With An Overall Review.

Explain the Basics of a Particle Filter in 1D.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/particle_filter_1d_example.png}
  \caption{1D Particle Filter Example}
  \label{fig:1d_particle_filter_example}
\end{figure}
-->

Explain How the Particle Filter works in 2D. Refer to 2D Map Figure. The Equations Below are Used to Calculated Scan Correlation and Then How to Weight Each Particle.

<!--
\begin{equation}
s = \frac{\sum_{m}\sum_{n}(A_{mn} - \bar{A})(B_{mn} - \bar{B})}{\sqrt{(\sum_{m}\sum_{n}(A_{mn} - \bar{A})^2)(\sum_{m}\sum_{n}(B_{mn} - \bar{B})^2)}}
\label{eq:weight_each_particle}
\end{equation}
-->

<!--
\begin{equation}
W_t \leftarrow W_{t-1}s
\label{eq:laser_scan_correlation}
\end{equation}
-->

Explain the Basics of Resampling in a Particle Filter and Why It is Done.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/amcl_resampling.png}
  \caption{Particle Filter Resampling Example}
  \label{fig:particle_filter_resampling_example}
\end{figure}
-->

Explain KLD Sampling and Why it is Used.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/amcl_visual_in_rviz.png}
  \caption{Visual of AMCL in RVIZ}
  \label{fig:visual_of_amcl_in_rviz}
\end{figure}
-->

Insert Text Explaining How Trajectory Rollout Works for Path Planning.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/trajectory_rollout_depiction.png}
  \caption{Trajectory Rollout Path Planning Framework}
  \label{fig:trajectory_rollout_path_planning_framework}
\end{figure}
-->

## Manipulation Concepts

Insert Text Outlining Moveit!'s System Architecture.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/moveit_system_architecture.png}
  \caption{Moveit!'s System Architecture.}
  \label{fig:moveit!'s_system_architecture}
\end{figure}
-->

Insert Text Explaining the Basic Intuition Behind Forward and Inverse Kinematics.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/forward_and_inverse_kinematics_depiction.png}
  \caption{Forward and Inverse Kinematics Example}
  \label{fig:forward_and_inverse_kinematics_example}
\end{figure}
-->

Insert Test Explaining the Difference Between KDL, KDL-RR and Trac_IK and Why Trac_IK was chosen to be used.

<!--
\begin{table}[H]
\centering
\caption{Comparison Between ROS Available Moveit! Inverse Kinematic Plugins. [Beeson et al. 2015]}
\label{comparison_between_ros_available_moveit!_inverse_kinematic_plugins}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}|c|c|c|c|c|c|c|c|@{}}
\toprule
\multicolumn{2}{|c|}{Kinematics Chain}                     & \multicolumn{6}{c|}{IK Technique}                                                                                                                                                                                                 \\ \midrule
\multirow{2}{*}{Robot}             & \multirow{2}{*}{DOFs} & \multicolumn{2}{c|}{Orocos' KDL}                                          & \multicolumn{2}{c|}{KDL-RR}                                               & \multicolumn{2}{c|}{TRAC-IK}                                              \\ \cmidrule(l){3-8}
                                   &                       & \multicolumn{1}{l|}{Solve Rate (\%)} & \multicolumn{1}{l|}{Avg Time (ms)} & \multicolumn{1}{l|}{Solve Rate (\%)} & \multicolumn{1}{l|}{Avg Time (ms)} & \multicolumn{1}{l|}{Solve Rate (\%)} & \multicolumn{1}{l|}{Avg Time (ms)} \\ \midrule
Atlas 2013 Arm                     & 6                     & 75.54                                & 1.35                               & 97.13                                & 0.39                               & 99.97                                & 0.33                               \\ \midrule
Atlas 2015 Arm                     & 7                     & 75.71                                & 1.50                               & 93.13                                & 0.81                               & 99.18                                & 0.48                               \\ \midrule
Baxter Arm                         & 7                     & 61.07                                & 2.21                               & 89.52                                & 1.02                               & 99.17                                & 0.60                               \\ \midrule
Denso VS-068                       & 6                     & 27.92                                & 3.69                               & 98.13                                & 0.42                               & 99.78                                & 0.38                               \\ \midrule
Fanuc M-430iA/2F                   & 5                     & 21.07                                & 3.99                               & 88.34                                & 0.92                               & 99.16                                & 0.58                               \\ \midrule
Fetch Arm                          & 7                     & 92.49                                & 0.73                               & 93.82                                & 0.72                               & 99.96                                & 0.44                               \\ \midrule
Jaco2                              & 6                     & 26.23                                & 3.79                               & 97.66                                & 0.58                               & 99.51                                & 0.58                               \\ \midrule
LBR IIWA 14 R820                   & 7                     & 37.71                                & 3.37                               & 94.02                                & 0.73                               & 99.63                                & 0.56                               \\ \midrule
KUKA LWR 4+                        & 7                     & 67.80                                & 1.88                               & 95.40                                & 0.62                               & 99.95                                & 0.38                               \\ \midrule
PR2 Arm                            & 7                     & 83.14                                & 1.37                               & 86.96                                & 1.27                               & 99.84                                & 0.59                               \\ \midrule
NASA Robonaut2 'Grasping Leg'      & 7                     & 61.27                                & 2.29                               & 87.57                                & 1.10                               & 99.31                                & 0.67                               \\ \midrule
NASA Robonaut2 'Leg + Waist + Arm' & 15                    & 97.99                                & 0.80                               & 98.00                                & 0.84                               & 99.86                                & 0.79                               \\ \midrule
NASA Robonaut2 Arm                 & 7                     & 86.28                                & 1.02                               & 94.26                                & 0.73                               & 99.25                                & 0.50                               \\ \midrule
NASA Robosimian Arm                & 7                     & 61.74                                & 2.44                               & 99.87                                & 0.36                               & 99.93                                & 0.44                               \\ \midrule
TRACLabs Modular Arm               & 7                     & 79.11                                & 1.35                               & 95.12                                & 0.63                               & 99.80                                & 0.53                               \\ \midrule
UR10                               & 6                     & 36.16                                & 3.29                               & 88.05                                & 0.82                               & 99.47                                & 0.49                               \\ \midrule
UR5                                & 6                     & 35.88                                & 3.30                               & 88.69                                & 0.78                               & 99.55                                & 0.42                               \\ \midrule
NASA Valkyrie Arm                  & 7                     & 45.18                                & 3.01                               & 90.05                                & 1.29                               & 99.63                                & 0.61                               \\ \bottomrule
\end{tabular}}
\end{table}
-->

## State Machines Concepts

Insert Text Explaining the Basics of State Machine and How They are Implemented in SMACH.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/graphical_view_of_state_machine_using_smach.png}
  \caption{Graphical View of State Machine using SMACH}
  \label{fig:graphical_view_of_state_machine_using_smach}
\end{figure}
-->

## Camera Concepts

Insert Text Explaining How Digital Image Are Recorded and Stored.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/digital_camera_diagram.png}
  \caption{Digital Image Formation.}
  \label{fig:digital_image_formation}
\end{figure}
-->

Insert Text Explaining Ground Sample Distance and It's Effects on Image Quality Thereby Effecting Ones Camera Choice.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/spatial_resolution.png}
  \caption{Ground Sample Distance Effects on Image Quality.}
  \label{fig:ground_sample_distance_effects_on_image_quality}
\end{figure}
-->

Insert Text Explaining The Effects of Different Shutter Type on Image Quality Thereby Effecting Ones Camera Choice.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/global_vs_rolling_shutter.png}
  \caption{Shutter Type Effects on Image Quality.}
  \label{fig:shutter_type_effects_on_image_quality}
\end{figure}
-->

## Computer Vision Concepts

Insert Text Regarding Upcoming Sections.

### Color Spaces

Insert Text Explaining Both RGB Color Space and HSV/HSL Color Space and Why One Would Be Chosen Over The Other.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/rgb_and_hsv_color_space_models.png}
  \caption{RGB and HSV color space models}
  \label{fig:RGB_and_HSV_color_space_models}
\end{figure}
-->

### Linear and Non-Linear Filters

Insert Text Explaining the Basics behind Linear Filters, Which Ones Were Chosen and Why.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/common_linear_filters.png}
  \caption{Effects of common linear filters.}
  \label{fig:common_linear_filters}
\end{figure}
-->

Insert Text Explaining the Basics behind Histogram Equalization.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/histogram_equalization.png}
  \caption{Histogram equalization depiction.}
  \label{fig:histogram_equalization_depiction}
\end{figure}
-->

Insert Text Explaining The Effect of Histogram Equalization on Images.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/image_equalization.png}
  \caption{Effect of histogram equalization on an image.}
  \label{fig:effects_of_histogram_equalization_on_an_image}
\end{figure}
-->

### Binary Operations

Insert Text Explaining the Basics behind Binary Operations, Which Ones Were Chosen and Why.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/binary_operations.png}
  \caption{Effects of common binary operations.}
  \label{fig:common_binary_operations}
\end{figure}
-->

### Hough Circle

Insert Text Explaining The Basic Preprocessing Operation Performed by Hough Circle Transform In Order to Get Edges.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/edge_detection_hough_circle.png}
  \caption{Summary of preprocessing operation performed by Hough Circle detector.}
  \label{fig:summary_of_preprocessing_operation_performed_by_Hough_Circle_detector}
\end{figure}
-->


Insert Text Explaining the Basic Intuition Behind Hough Circle with Know Radius.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/hough_circle_summary_known_r.png}
  \caption{Summary of Hough Circle detector with known radius.}
  \label{fig:summary_of_hough_circle_detector_with_known_radius}
\end{figure}
-->

Insert Text Expanding Basic Intuition Behind Hough Circle with Unknown Radius.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/hough_circle_summary_unknown_r.png}
  \caption{Summary of Hough Circle detector with unknown radius.}
  \label{fig:summary_of_hough_circle_detector_with_unknown_radius}
\end{figure}
-->


### Good Feature To Track

Insert Text Explaining the Basic Intuition Behind Good Feature.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/good_feature_basic_intuition.png}
  \caption{Summary of Good Feature detector.}
  \label{fig:summary_of_good_feature_detector}
\end{figure}
-->

Insert Text Explaining an Image Gradient/Derivative and Why We Look for Them.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/image_gradient_example.png}
  \caption{Image Gradient Example.}
  \label{fig:image_gradient_example}
\end{figure}
-->

Insert Text Explaining Math.

<!--
\begin{equation}
E(u,v) = \sum_{x,y}w(x,y)[\textit{I}(x+u,y+v)-\textit{I}(x,y)]^2
\label{eq:change_of_intensity_for_the_shift_uv}
\end{equation}
-->

<!--
\begin{equation}
\sum_{x,y}[\textit{I}(x+u,y+v)-\textit{I}(x,y)]^2
\label{eq:maximize_intensity}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\approx\sum_{x,y}[\textit{I}(x,y)+uI_{x}+vI_{y}-\textit{I}(x,y)]^2
\label{eq:taylor_series_expansion}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\approx\sum_{x,y}u^2I_{x}^2+2uvI_{x}I_{y}+v^2I_{y}^2
\label{eq:expanding_and_cancelling_properly}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\approx\begin{bmatrix}
u & v
\end{bmatrix}\bigg(\sum_{x,y}w(x,y)\begin{bmatrix}
I_{x}^2 & I_{x}I_{y}\\
I_{x}I_{y} & I_{y}^2
\end{bmatrix} \bigg)\begin{bmatrix}
u\\
v
\end{bmatrix}
\label{eq:express_in_matrix_form}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\cong\begin{bmatrix}
u & v
\end{bmatrix}M\begin{bmatrix}
u\\
v
\end{bmatrix}
\label{eq:simplify_matrix}
\end{equation}
-->

Where $$M=\sum_{x,y}w(x,y)\begin{bmatrix}
I_{x}^2 & I_{x}I_{y}\\
I_{x}I_{y} & I_{y}^2
\end{bmatrix}$$

Insert Text Explaining How Both Harris Corner and Good Feature Scoring Functions Work and Why Good Feature Performs Slightly Higher.

<!--
\begin{equation}
R = \textup{det}(M) - k(\textup{trace}(M))^2
\label{eq:harris_corner_scoring_function}
\end{equation}
-->

Where $\textup{det}(M) = \lambda_{1}\lambda_{2}$ and $\textup{trace}(M) = \lambda_{1} + \lambda_{2}$. So,

<!--
\begin{equation}
R = \lambda_{1}\lambda_{2} - k(\lambda_{1}+\lambda_{2})^2
\label{eq:simplified_harris_corner_scoring_function}
\end{equation}
-->

<!--
\begin{equation}
R = \textup{min}(\lambda_{1},\lambda_{2})
\label{eq:good_feature_scoring_function}
\end{equation}
-->


<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/good_feature_harris_scoring_comparison.png}
  \caption{Difference Between Good Feature and Harris Corner Scoring Functions.}
  \label{fig:difference_between_good_feature_and_harris_corner_scoring_functions}
\end{figure}
-->

### Optical Flow

Insert Text Explaining the Basic Intuition Behind Optical Flow.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/optical_flow_example.png}
  \caption{Example of optical flow.}
  \label{fig:example_of_optical_flow}
\end{figure}
-->

Insert Text Explaining the Basic Intuition Behind Kanade-Lucas-Tomasi Feature Tracker.

<!--
\begin{equation}
\sum_{x} [\textit{T}(\textbf{W}(\textbf{x;}\Delta\textbf{p}))-\textit{I}(\textbf{W(x;p}))]^2
\label{eq:klt1}
\end{equation}
-->

<!--
\begin{equation}
\sum_{x} \bigg[\textit{T}(\textbf{W}(\textbf{x;0}))+\nabla \textbf{T}\frac{\partial\textbf{W}}{\partial\textbf{p}}\Delta\textbf{p}-\textit{I}(\textbf{W}(\textbf{x;p}))\bigg]^2
\label{eq:klt2}
\end{equation}
-->

<!--
\begin{equation}
\Delta\textbf{p}=\textit{H}^{-1}\sum_{x} \bigg[\nabla \textbf{T}\frac{\partial\textbf{W}}{\partial\textbf{p}} \bigg]^T[\textit{I}(\textbf{W}(\textbf{x;p}))-\textit{T}(\textbf{x})]
\label{eq:klt3}
\end{equation}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/klt_optimization.png}
  \caption{Summary of Kanade-Lucas-Tomasi feature tracker.}
  \label{fig:summary_of_Kanade-Lucas-Tomasi_feature_tracker}
\end{figure}
-->

### Augmented Reality Tag Detection, Pose Estimation, and Tracking

Insert Text Describing How AR Tag Detection, Pose Estimation, and Tracking Works with AR_Track_Alvar.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/ar_track_alvar_framework.png}
  \caption{Summary of ALVAR AR Tag Detection Framework.}
  \label{fig:summary_of_alvar_ar_tag_detection_framework}
\end{figure}
-->

# Related Work

This section will discuss related work.

# System Overview

The generic framework of the system is depicted in Figure <!--\ref{fig:system_overview}-->. The system allows the user to input predefined tasks. Given a priori knowledge of each tasks and their global start locations, the mobile system navigates to and performs the requested operation(s) while monitoring its battery level to ensure mission completion. The framework shown was implemented using a hierarchical state machine and was written in such a way as to make it compatible with the Robot Operating System (ROS) to ensure ease of use across differing robotic platforms.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/general_system_overview.png}
  \caption{General system overview.}
  \label{fig:system_overview}
\end{figure}
-->

# System Implementation

Insert Text Outlining the Upcoming Sections.

## Global Map Creation and Task Location Specification

The purpose of this section is reiterate why KartoSLAM was chosen, as well as describe how maps are created, the map files themselves, and how global locations are specified.

Before the manipulator can localize itself with respect to the part, the system must first navigate to the general vicinity in which the work will take place. In order to achieve this, the system utilizes odometry data, given by wheel encoders and an onboard inertial measurement unit (IMU), as well as sensor data, such as laser scans from a LIDAR or point clouds from an RGB-D sensor, to output safe velocity commands that will be sent to the mobile base of the system.

First, a Simultaneous Localization and Mapping (SLAM) technique named KartoSLAM uses the system’s odometry and laser scan or point cloud data, to create a 2-D map of the environment in which the operation(s) will take place. After which, the global location of specific operation(s) are defined, as shown in Figure <!--\ref{fig:global_map}-->.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/2d_map_with_specified_start_locations.png}
  \caption{2-D map environment with specified start locations.}
  \label{fig:global_map}
\end{figure}
-->

## Autonomous Localization and Navigation

Adaptive Monte Carlo localization (amcl) is used to localize the system within the map. Subsequently, odometry data is combined with a global and local cost map, in which obstacles and a specific distance around them represent a cost. These maps are used to plan optimal and obstacle free paths through the environment. The global path is computed before the system begins moving and takes into account all known obstacles, while the local path monitors incoming sensor data to compute suitable linear and angular velocities for the system to complete the current section of the global path. The local path is typically computed at a rate of 20 Hz; however, this parameter is adjustable given the needs of the system.

## Task Association and A Priori Knowledge

Once arriving in the general vicinity of the task to be accomplished, the system then locates an augmented reality (AR) tag [#Siltanen], which allows it to localize and transform points of interest (POIs) associated with the AR tag into the system’s frame of reference. This general localization framework is presented in Figure <!--\ref{fig:task_association}-->.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/initial_feature_localization_framework.png}
  \caption{Initial Feature Localization Framework.}
  \label{fig:initial_feature_localization_framework}
\end{figure}
-->

## Generic Framework for Multi-Stage Computer Vision Algorithm

Insert Text Outlining the Upcoming Sections.

### Initial Feature Location Prediction

This subsection will go through AR tags and how they are used to populate point.

### Corrected Feature Locations

Currently the sponsor of this work uses a hole template on the object to be drilled to ensure accuracy within $\pm$ 0.3 mm. Using the predicted hole locations, given by the AR tag, an inverse kinematic solver is used to move the manipulator to the specified Cartesian location. A camera mounted on the manipulator is then used to further correct the positon of the end-effector. Canny Edge Detection, Hough Transforms, as well as the camera’s intrinsic characteristics and a priori knowledge of each hole’s size is used to output an adjusted Cartesian location of the circle on the templet closest to the predicted position [#Alter1992]. If a hole to be drilled is not found or is outside the range of the manipulator, that hole will be added to a list and the customer notified of all such holes after the operation is completed. In addition to the high precision achieved by the above technique, it can also provide a video log of all work done for inspection.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/placeholder_fig.png}
  \caption{Feature Location General Correction Framework.}
  \label{fig:feature_location_general_correction_framework}
\end{figure}
-->

## Specific Task Implementation

Outline the upcoming sections.

### Drilling Framework

This section will go in depth into how the robot simulates drilling.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/drilling_operation_framework.png}
  \caption{Drilling operation framework.}
  \label{fig:drilling_operation_framework}
\end{figure}
-->

### Sealant Application Framework

This section will go in depth into how the robot simulates sealant application.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/sealant_application_framework.png}
  \caption{Sealant Application Framework.}
  \label{fig:sealant_application_framework.}
\end{figure}
-->

# Experiments and Results

Outline the upcoming sections.

## Hardware Architecture

Insert Text Regarding Robot, Sensors, etc.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/husky_and_fetch_hardware_architecture.png}
  \caption{Clearpath Robotics' Husky and Fetch Robotics' Fetch Mobile Manipulator}
  \label{fig:clearpath_robotics'_husky_and_fetch_robotics'_fetch_mobile_manipulator}
\end{figure}
-->

## Software Architecture

Insert Text Regarding The Software Architecture.

## Experiments

Insert Text Regarding Upcoming Sections.

### Camera Calibration Setup

Insert text regarding methods used for camera calibration (citing paper).

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/placeholder_fig.png}
  \caption{Camera Calibration Setup.}
  \label{fig:camera_calibration_setup}
\end{figure}
-->

Insert Text and Math Regarding Calibration of Laser.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/placeholder_fig.png}
  \caption{Laser Calibration Setup.}
  \label{fig:laser_calibration_setup}
\end{figure}
-->

### Navigation System Experimental Setup

Insert Text Regarding the Navigation System Experimental Setup.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/navigation_system_experimental_setup.png}
  \caption{Navigation System Experimental Setup.}
  \label{fig:navigation_system_experimental_setup}
\end{figure}
-->

### Drilling Operation Experimental Setup

Insert Text Regarding the Drilling Operations Experimental Setup.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/drilling_operation_experimental_setup.png}
  \caption{Drilling Operations Experimental Setup.}
  \label{fig:drilling_operations_experimental_setup}
\end{figure}
-->

### Sealant Application Experimental Setup

Insert Text Regarding the Sealant Application Experimental Setup.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/sealant_application_experimental_setup.png}
  \caption{Sealant Application Experimental Setup.}
  \label{fig:sealant_application_experimental_setup}
\end{figure}
-->

## Results

### Camera Calibration Accuracy Achieved

Insert Text Regarding Pixel Error of Kinect V2.

Insert Text Regarding Pixel Error of Basler ACA150-UC.

Insert Text Regarding Pixel Error of Laser Calibration.

### Navigation System Accuracy Achieved

Insert Text Regarding Accuracy of Navigation System and Ways To Improve Accuracy.

<!--
\begin{table}[H]
\centering
\caption{Global Localization Accuracy Comparison Given Distance From Global Location.}
\label{global_localization_accuracy_comparison_given_distance_from_global_location}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}|c|c|c|c|c|@{}}
\toprule
                                              & Distance from Global Position & x (mm) & y (mm) & $\psi$ (rad) \\ \midrule
\multirow{4}{*}{Global Position Localization} & 5                             & 0      & 0      & 0            \\ \cmidrule(l){2-5}
                                              & 10                            & 0      & 0      & 0            \\ \cmidrule(l){2-5}
                                              & 20                            & 0      & 0      & 0            \\ \cmidrule(l){2-5}
                                              & 40                            & 0      & 0      & 0            \\ \bottomrule
\end{tabular}}
\end{table}
-->

### Discussion of Drilling Operation Accuracy

Insert Text Regarding Accuracy of Drilling Operations and Ways To Improve Accuracy.

<!--
\begin{table}[H]
\centering
\caption{Drilling Operation Accuracy Comparison Given Distance From Work Surface.}
\label{drilling_operation_accuracy_comparison_given_distance_from_work_surface}
\resizebox{0.75\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|c|@{}}
\cmidrule(l){2-5}
                                                            & Distance from Global Position (m) & x (mm) & y (mm) & z (mm) \\ \midrule
\multicolumn{1}{|c|}{\multirow{4}{*}{AR Tag Localization}}  & 5                                 & 0      & 0      & 0      \\ \cmidrule(l){2-5}
\multicolumn{1}{|c|}{}                                      & 10                                & 0      & 0      & 0      \\ \cmidrule(l){2-5}
\multicolumn{1}{|c|}{}                                      & 20                                & 0      & 0      & 0      \\ \cmidrule(l){2-5}
\multicolumn{1}{|c|}{}                                      & 40                                & 0      & 0      & 0      \\ \midrule
\multicolumn{1}{|c|}{\multirow{4}{*}{Feature Localization}} & 5                                 & 0      & 0      & 0      \\ \cmidrule(l){2-5}
\multicolumn{1}{|c|}{}                                      & 10                                & 0      & 0      & 0      \\ \cmidrule(l){2-5}
\multicolumn{1}{|c|}{}                                      & 20                                & 0      & 0      & 0      \\ \cmidrule(l){2-5}
\multicolumn{1}{|c|}{}                                      & 40                                & 0      & 0      & 0      \\ \bottomrule
\end{tabular}}
\end{table}
-->

### Discussion of Sealant Application Accuracy

Insert Text Regarding Accuracy of Sealant Application and Ways To Improve Accuracy.

<!--
\begin{table}[H]
\centering
\caption{Sealant Application Accuracy Comparison Given Distance From Work Surface.}
\label{sealant_application_accuracy_comparison_given_distance_from_work_surface}
\resizebox{0.75\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|c|@{}}
\cmidrule(l){2-5}
                                                            & Distance from Global Position (m) & x (mm) & y (mm) & z (mm) \\ \midrule
\multicolumn{1}{|c|}{\multirow{4}{*}{AR Tag Localization}}  & 5                                 & 0      & 0      & 0      \\ \cmidrule(l){2-5}
\multicolumn{1}{|c|}{}                                      & 10                                & 0      & 0      & 0      \\ \cmidrule(l){2-5}
\multicolumn{1}{|c|}{}                                      & 20                                & 0      & 0      & 0      \\ \cmidrule(l){2-5}
\multicolumn{1}{|c|}{}                                      & 40                                & 0      & 0      & 0      \\ \midrule
\multicolumn{1}{|c|}{\multirow{4}{*}{Feature Localization}} & 5                                 & 0      & 0      & 0      \\ \cmidrule(l){2-5}
\multicolumn{1}{|c|}{}                                      & 10                                & 0      & 0      & 0      \\ \cmidrule(l){2-5}
\multicolumn{1}{|c|}{}                                      & 20                                & 0      & 0      & 0      \\ \cmidrule(l){2-5}
\multicolumn{1}{|c|}{}                                      & 40                                & 0      & 0      & 0      \\ \bottomrule
\end{tabular}}
\end{table}
-->

# Conclusion and Future Work

This paper showed the general framework necessary in order to solve the two main problems preventing the development and widespread adoption of ARC. These problems have caused a decrease in productivity and increase in workplace injuries/fatalities over the past several decades in comparison to other industries [#Rojas2003]. These problems include the fact that typical construction sites tend to be unstructured and are continuously evolving versus the highly controlled environments found in manufacturing. Also, the relationship between the part and manipulator has been reversed, causing increased complexity not seen in manufacturing environments where the part arrives at a fixed manipulator [#Feng2015]. The techniques presented allow systems to create a 2-D map of their environment, localize themselves and complete the task(s) assigned. After localizing an AR tag at the work site, the system is able to use a priori knowledge to localize POIs and complete a plethora of operations achieving an accuracy of approximately $\pm$ 2 mm based on a multifaceted computer vision approach with only a USB webcam.

Future work to be explored includes increasing the accuracy of the computer vision system to the sub-millimeter levels through the use of a machine vision camera, as well as a relatively new calibration technique developed by Feng et al [#Feng2015]. In addition, automated approaches to create 3-D maps are being looked into in order to provide updated data about the robot’s surroundings and task(s) automatically without human intervention. Also, human and robot collaboration over a distributed network is being explored.
