Base Header Level: 2
latex input: metadata
latex author: Christopher James Mobley
latex title: Multistage Localization for High Precision Mobile Manipulation Tasks
bibtex: masters_thesis
myreporttype: Thesis
mydegree: Masters of Science
myyear: 2016
mydepartment: Mechanical Engineering
latex input: vtthesis/setup.tex
latex footer: vtthesis/footer.tex

# Introduction

## Background

Unlike the substantial benefits seen in the manufacturing industry through automation and robotics, automation and robotics in construction (ARC) has lagged far behind in adoption [#Balaguer]. Consequently, when compared with other industries, construction has seen a significant decrease in productivity, as well as an increase in workplace injuries/fatalities over the last several decades [#Rojas2003]. While several technical complexities inherent in construction have hindered the development and adoption of field construction robots [4], through the capitalization of advances made by other industries, ARC can quickly close this gap. Thereby allowing dangerous and or mundane repetitive tasks to be accomplished autonomously. Thus, causing an increase in productivity and a decrease in workplace injuries/fatalities [1]. However, ARC faces two unique challenges when compared to other industries. Unlike manufacturing environments, which are tightly controlled, typical construction sites tend to lack structure and are continuously evolving. In addition, the reversal in relationship between the part and manipulator has dramatically increased the complexity of the problem to be solved [#Feng2015]. Instead of the part appearing at a fixed manipulator, the manipulator must now move to and localize itself with respect to the part. The remainder of this paper is structured as follows: In Section 2, the author’s technical approach is outlined, with particular focus on problem two, and experimental results are shown. Conclusions are then drawn and future work discussed in Section 3.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/future_transformable_production_system.png}
  \caption{Vision for future transformable production systems.}
  \label{fig:vision_for_future_transformable_production_systems}
\end{figure}
-->

## Objectives

Insert Text Regarding the Objectives of This Work.

## Summary of Original Contributions

Insert Text Regarding the Originals Contributions Presented in this Work.

## Outline

Insert Text Outlining the Upcoming Chapters.

# Literature Review

## Localization and Mapping For Autonomous Mobile Manipulators in Manufacturing and Construction

Insert Text Summarizing Current SLAM and Localization Techniques Used.

## Task Association and A Priori Knowledge for Mobile Manipulators

Insert Text Summarizing Current Methods Used to Associate Mobile Manipulator to a Specific Task and How Prior Knowledge is Conveyed About the Task to be Performed.

## Feature Localization Techniques for Mobile Manipulators

Insert Text Summarizing Current Methods Used to Localize a Feature and Have a Mobile Manipulator Perform a Set Operation.

## Summary

Insert Text Summarizing This Chapter and Transiting to the Next.

# Fundamentals of Autonomous Robotics

The purpose of this chapter is to briefly explain all concepts needed to understand the work presented in the chapters thereafter. The following subsection will explain
the basic concepts of the Robot Operating System, Simultaneous Localization and Mapping (SLAM), Localization and Path Planning, as well as Manipulation. In addition,
Task Execution using State Machines and the specific Computer Vision Algorithms implemented will be expanded upon.

## ROS Concepts

The Robot Operating System (ROS) [#Quigley2009] is an open-source set of software libraries and tools that aim to simplify the task of creating robotics applications that can
be used across a wide variety of platforms. ROS was originally developed in 2007 by Willow Garage as an extension of switchyard, a collection
of robotics software developed by Stanford Artificial Intelligence Laboratory in support of the Stanford AI Robot (STAIR) and Personal Robotics (PR) projects.
The first distribution of ROS, Box Turtle, was released in 2010. ROS currently has had ten major releases. The most current being Kinetic Kame Turtle, which was released
on 23MAY16. In addition ROS boasts tens of thousands of users around the world ranging from hobbyists and researcher to the commercial and industrial industries, as well
as hundreds of packages which provide everything from hardware drivers to algorithms for autonomous navigations and manipulation [#ROSWebsite]. ROS’s large support base act as a force
multiplier allows individuals, Labs, or company to concentrate on one particular aspect while capitalizing on work that has already been done.


### ROS Communication

ROS uses a name server, called the ROS Master, to maintain a list of nodes and available topics. Nodes communicate with the master server using the XML-RPC protocol. While
peer-to-peer communications generally use TCP/IP sockets through the TCPROS protocol. Figure _ shows a simple diagram of two ROS nodes communicating with messages and service topics.
In addition to the concepts of messages and services, ROS also employs actions. Actions are similar to service calls, but are designed for long duration tasks that are capable of
providing feedback. These communication interfaces provide ROS a great deal of flexibility for robotic applications [#ROSConcepts] [#Burton2015].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/simple_ros_node_diagram.png}
  \caption{Simple ROS Node Flowchart}
  \label{fig:simple_ros_node_flowchart}
\end{figure}
-->

### Rigid Body Transformations

Insert Text Explaining Rigid Body Transformations.

<!--
\begin{equation}
x_a=T_{b}^{a}x_b
\label{eq:conversion_from_coordinate_frame_a_to_b}
\end{equation}
-->

where $T_{b}^{a}$ is equal to
<!--
\begin{equation}
\begin{bmatrix}
R_{b}^{a} & t_{b}^{a}\\
0^T & 1
\end{bmatrix}
\label{eq:simplified_transformation_matrix}
\end{equation}
-->

where $R_{b}^{a}$ is the rotation matrix, which performs the rotation part of moving frame $b$ into alignment with frame $a$ and $t_{b}^{a}$ is the translation matrix, which performs
the translation part of moving frame $b$ origin to frame $a$.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/conversion_from_frame_a_b.png}
  \caption{Conversion from Coordinate Frame A to B}
  \label{fig:conversion_from_coordinate_frame_a_to_b}
\end{figure}
-->

Insert Text Regarding How a URDF is Setup in ROS so that Transformation Between Coordinates can be Managed with the Package TF.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/robots_with_without_tfs.png}
  \caption{Robot Model with TFs}
  \label{fig:robot_model_with_tfs}
\end{figure}
-->

## Simultaneous Localization and Mapping Concepts

Insert Text Outlining the Basic Intuition Behind SLAM.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/overview_of_slam_framework.png}
  \caption{Overview of SLAM Framework. [Riisgaard et al. 2005]}
  \label{fig:overview_of_slam_framework}
\end{figure}
-->

Insert Text Explaining How ROS Uses SLAM to Create 2-D Occupancy Grid Maps and What How They Are Used.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/2d_occupancy_grid_map.png}
  \caption{Example of 2-D Occupancy Grid Map Produced.}
  \label{fig:example_of_2d_cccupancy_grid_map_produced}
\end{figure}
-->

Insert Text Specifying Available 2-D SLAM Techniques and Why KartoSLAM was Chosen.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/real_world_performance_analysis_ros_slam_algorithms.png}
  \caption{Real World Performance Analysis of ROS Available SLAM Algorithms. [Santos et al. 2013]}
  \label{fig:real_world_performance_analysis_of_ros_available_slam_algorithms}
\end{figure}
-->

<!--
\begin{table}[H]
\centering
\caption{Real World Error Estimation for ROS Available SLAM Algorithm. [Santos et al. 2013]}
\label{real_world_error_estimation_for_ros_available_slam_algorithm}
\resizebox{0.75\linewidth}{!}{\begin{tabular}{@{}|c|c|c|c|c|@{}}
\toprule
\multicolumn{5}{|c|}{Real World Experiments}            \\ \midrule
HectorSLAM & GMapping & KartoSLAM & CoreSLAM & LagoSLAM \\ \midrule
1.1972     & 2.1716   & 1.0318    & 14.75333 & 3.0264   \\ \midrule
0.5094     & 0.6945   & 0.3742    & 7.9463   & 0.8181   \\ \midrule
1.0656     & 1.6354   & 0.9080    & 7.5824   & 2.5236   \\ \bottomrule
\end{tabular}}
\end{table}
-->

## Localization and Path Planning Concepts

ROS’s Navigation Stack is a collection of packages, which uses odometry and laser scan data, as well as a goal position and orientation in
order to output the velocity commands needed to reach the specified goal. Figure <!--\ref{fig:ros_navigation_stack_setup}--> shows an overview of how the individual packages work
together to achieve this objective. The Map_Server node loads a previously generated two-dimensional grid map. Once the AMCL node receives the map,
odometry, and laser scan data, it is able to localize the robot within the provided map, using the Adaptive Monte Carlo Localization technique
for which it gets its name. The Move_Base node maintains both global and local planners and costmaps. Information about obstacles in the world
are stores in these costmap. The global costmap is used for long-term planning, while the local costmap is used for short-term planning and obstacle
avoidance. The global planner computes an optimal path to the goal given the starting state of robot and the global costmap. While the local planner
computes shorter trajectories given the current state of the robot and the local costmap. Once a path is developed, Move_Base outputs the necessary
velocity commands needed in order to reach the specified destination [#Navigation].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/ros_navigation_stack_setup.png}
  \caption{ROS Navigation Stack setup.}
  \label{fig:ros_navigation_stack_setup}
\end{figure}
-->

After building a two-dimensional occupancy grid map, shown in Figure <!--\ref{fig:example_of_2d_cccupancy_grid_map_produced}-->, using SLAM, it becomes crucial to accurately localize the robot within
this predefined map; so that, the robot can both plan and execute appropriate trajectories to reach its destination. Localization involves
estimating the position and orientation of the robot, known collectively as pose, while the robot moves throughout its environment. One routine
localization technique used involves tracking the robot from an initial known starting pose. Through the measurement of wheel rotation and the
integration of accelerations provided by an inertial measurement unit (IMU), the distance traveled by the robot from the initial position can be
calculated and the robot’s pose in the map estimated with some certainty. However, these methods do not account for wheel slippage or measurement
error. As a result, the accuracy of the pose estimate will degrade over time. Consequently, a solution which can compensate for the accumulated
odometry error and inaccuracies in the initial starting pose is needed. One accepted solution to this problem is Monte Carlo Localization (MCL),
which utilizes a particle filters to keep track of the robot’s pose. However, additional options include Kalman Filters and Markov Localization,
which employ Gaussian distributions and histograms respectively.

Figure <!--\ref{fig:one_dimensional_monte_carlo_localization_example}--> depicts MCL using a one-dimensional corridor with a few doors. The robot
initially has no information about where it is in this corridor. As a result, the graph of the robot’s belief states, which defines the probability
of the robot being at a particular position, is drawn from a uniform distribution of discretely sampled positions along the corridor. A measurement
update is performed at each time step. A measurement update involves convolving the measurement model, the probability of receiving a specific sensor
measurement in the corridor, with the belief states to get an updated belief state. The updated belief state is the same as the pervious belief state;
however, the weight of each particle have been updated based on the sensor reading. At step k=1 the robot senses a door; so, the weight of particles at
the three door are increased. At the next step, a motion model update is performed. The odometry indicated that the robot moved forward a specified
distance d. As a result, the belief state is updated by moving the particles forward that specific distance with noise added to account for the
aforementioned odometry errors. It should be noted that the particles at this stage in Figure <!--\ref{fig:one_dimensional_monte_carlo_localization_example}--> were also resampled, which will be covered in the
following paragraphs. The motion model update is followed by a measurement update. The robot again senses a door. As a result, the measurement model is
the same as the pervious time step. After convolving the current measurement model with the current belief state, the cumulative probability mass is
centered at door two indicating that the robot is likely at this location [#Thrun2005].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/particle_filter_1d_example.png}
  \caption{One-Dimensional Monte Carlo Localization Example.}
  \label{fig:one_dimensional_monte_carlo_localization_example}
\end{figure}
-->

Two-dimensional MCL follows the same format of a motion model and then measurement update. The initial particles are drawn from the current odometry
with added noise. At each step, the particles are updated via the odometry and then corrected via a measurement update. For each particle, the correlation
between the two-dimensional occupancy grid map, seen in Figure <!--\ref{fig:example_of_2d_cccupancy_grid_map_produced}-->, and laser scan is calculated using
Equation <!--\ref{eq:weight_each_particle}-->, where $A$ is the predefined map, $B$ is the map created by the current laser scan, and $\bar{A}$ and $\bar{B}$ are the mean
values of the pixels of both maps respectively. Note that while obstacle pixels are black and have a value of 1, free space pixels are grey and have a value of 0. While $m$ and $n$
are the x and y values of the pixel. The particle (pose) with the highest correlations score is chosen as the pose for the current step. The weight of each
particles at step $k$ is found by multiplying the particles weight at step $k-1$ by its normalized correlation score at time step $k$, as seen in Equation <!--\ref{eq:laser_scan_correlation}-->.


<!--
\begin{equation}
s = \frac{\sum_{m}\sum_{n}(A_{mn} - \bar{A})(B_{mn} - \bar{B})}{\sqrt{(\sum_{m}\sum_{n}(A_{mn} - \bar{A})^2)(\sum_{m}\sum_{n}(B_{mn} - \bar{B})^2)}}
\label{eq:weight_each_particle}
\end{equation}
-->

<!--
\begin{equation}
W_k \leftarrow W_{k-1}s
\label{eq:laser_scan_correlation}
\end{equation}
-->

As weights are multiplied over steps, the particles with consecutive small correlation scores are reduced to very small values. As a result, the particles
filter eventually has every few effective particles to ensure that good results are produced. Consequently, re-sampling, show in Figure <!--\ref{fig:particle_filter_resampling_example}-->, is performed when
the number of effective particles become too small. This is done by drawing samples close to the particles that have a higher weights, indicated by their size.
Thus the new sample have a higher density near the position where the particle with higher weight existed. Each particle after resampling has same weight.
As a result, the particle filter begins from scratch.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/amcl_resampling.png}
  \caption{Particle Filter Resampling Example}
  \label{fig:particle_filter_resampling_example}
\end{figure}
-->

Due to the computational complexity inherent in iteratively having to calculate the correlation score for each particle, an optimization technique known as
*KLD-sampling* is used. *KLD-sampling*, derived from *Kullback-Leibler divergence*, is a technique that determine the number of particles needed such that
the error between the sample and true posterior is less than $\epsilon$ [#Thrun2005]. KLD-sampling basically control the number of particles based on the difference
in odometry and particle base location.

Figure <!--\ref{fig:visual_of_amcl_in_rviz}--> shows how KLD-sampling effectively work. Initially when the position is unknown, the particle cloud is large due to the uncertainty in the position
and orientation of the robot. However, as the robot moves, the particle converges and the particle cloud size reduces as KLD-sampling removes the redundant
particles and improves computational performance.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/amcl_visual_in_rviz.png}
  \caption{Visual of AMCL in RVIZ}
  \label{fig:visual_of_amcl_in_rviz}
\end{figure}
-->

After calculating the local and global costmaps, as well as the location of the robot within the given map, the robot must now both plan and execute appropriate trajectories to its destination.
Two frequent techniques used are the Dynamic Window Approach (DWA) and Trajectory Rollout. Both sample the space of feasible controls. For a differential drive robot, such as Clearpath Robotics’
Husky and Fetch Robotics’ Fetch, this controls space is 2D and consists of translations and rotational velocities, $\dot{x}, \dot{\theta}$, which are limited by the robot’s capabilities. Each
sampled velocity is forward simulated from the robot’s current for a short period of time in order to generate simulated trajectories as shown in Figure <!--\ref{fig:trajectory_rollout_path_planning_framework}-->. These simulated trajectories are then
scored using the cost function in Equation <!--\ref{eq:trajectory_rollout_cost_function}-->.

<!--
\begin{equation}
C(\textit{k}) = \alpha{} Obs + \beta{} Gdist +\gamma{} Pdist + \delta{} \frac{1}{\dot{x}^2}
\label{eq:trajectory_rollout_cost_function}
\end{equation}
-->

Where *Obs* is the sum of grid cell cost through which the trajectory passes (taking account of the robot's actual footprint in the grid); *Gdist* and *Pdist* are the estimated shortest distance from
the endpoint of the trajectory to the goal and the optimal path, respectively; and $\dot{x}$ is the translation component of the velocity command that produces the trajectory.

The simulated trajectory that minimizes this cost function is chosen. As a result, chosen trajectories tend to keep obstacles at a distance, proceed towards the goal, remain near the optimal path, as well
as have higher velocities [#Gerkey2008].  DWA and Trajectory Rollout differ in that Trajectory Rollout samples achievable velocities over the entire forward simulation, while DWA sample only from achievable
velocities for just one simulation step [#BaseLocalPlanner].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/trajectory_rollout_depiction.png}
  \caption{Trajectory Rollout Path Planning Framework}
  \label{fig:trajectory_rollout_path_planning_framework}
\end{figure}
-->

## Manipulation Concepts

Insert Text Outlining Moveit!'s System Architecture.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/moveit_system_architecture.png}
  \caption{Moveit!'s System Architecture.}
  \label{fig:moveit!'s_system_architecture}
\end{figure}
-->

Insert Text Explaining the Basic Intuition Behind Forward and Inverse Kinematics.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/forward_and_inverse_kinematics_depiction.png}
  \caption{Forward and Inverse Kinematics Example}
  \label{fig:forward_and_inverse_kinematics_example}
\end{figure}
-->

Insert Test Explaining the Difference Between The Numerical Solvers available in ROS (KDL, KDL-RR and Trac_IK) and Why Trac_IK was chosen to be used.

<!--
\begin{table}[H]
\centering
\caption{Comparison Between ROS Available Moveit! Inverse Kinematic Plugins. [Beeson et al. 2015]}
\label{comparison_between_ros_available_moveit!_inverse_kinematic_plugins}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}|c|c|c|c|c|c|c|c|@{}}
\toprule
\multicolumn{2}{|c|}{Kinematics Chain}                     & \multicolumn{6}{c|}{IK Technique}                                                                                                                                                                                                 \\ \midrule
\multirow{2}{*}{Robot}             & \multirow{2}{*}{DOFs} & \multicolumn{2}{c|}{Orocos' KDL}                                          & \multicolumn{2}{c|}{KDL-RR}                                               & \multicolumn{2}{c|}{TRAC-IK}                                              \\ \cmidrule(l){3-8}
                                   &                       & \multicolumn{1}{l|}{Solve Rate (\%)} & \multicolumn{1}{l|}{Avg Time (ms)} & \multicolumn{1}{l|}{Solve Rate (\%)} & \multicolumn{1}{l|}{Avg Time (ms)} & \multicolumn{1}{l|}{Solve Rate (\%)} & \multicolumn{1}{l|}{Avg Time (ms)} \\ \midrule
Atlas 2013 Arm                     & 6                     & 75.54                                & 1.35                               & 97.13                                & 0.39                               & 99.97                                & 0.33                               \\ \midrule
Atlas 2015 Arm                     & 7                     & 75.71                                & 1.50                               & 93.13                                & 0.81                               & 99.18                                & 0.48                               \\ \midrule
Baxter Arm                         & 7                     & 61.07                                & 2.21                               & 89.52                                & 1.02                               & 99.17                                & 0.60                               \\ \midrule
Denso VS-068                       & 6                     & 27.92                                & 3.69                               & 98.13                                & 0.42                               & 99.78                                & 0.38                               \\ \midrule
Fanuc M-430iA/2F                   & 5                     & 21.07                                & 3.99                               & 88.34                                & 0.92                               & 99.16                                & 0.58                               \\ \midrule
Fetch Arm                          & 7                     & 92.49                                & 0.73                               & 93.82                                & 0.72                               & 99.96                                & 0.44                               \\ \midrule
Jaco2                              & 6                     & 26.23                                & 3.79                               & 97.66                                & 0.58                               & 99.51                                & 0.58                               \\ \midrule
LBR IIWA 14 R820                   & 7                     & 37.71                                & 3.37                               & 94.02                                & 0.73                               & 99.63                                & 0.56                               \\ \midrule
KUKA LWR 4+                        & 7                     & 67.80                                & 1.88                               & 95.40                                & 0.62                               & 99.95                                & 0.38                               \\ \midrule
PR2 Arm                            & 7                     & 83.14                                & 1.37                               & 86.96                                & 1.27                               & 99.84                                & 0.59                               \\ \midrule
NASA Robonaut2 'Grasping Leg'      & 7                     & 61.27                                & 2.29                               & 87.57                                & 1.10                               & 99.31                                & 0.67                               \\ \midrule
NASA Robonaut2 'Leg + Waist + Arm' & 15                    & 97.99                                & 0.80                               & 98.00                                & 0.84                               & 99.86                                & 0.79                               \\ \midrule
NASA Robonaut2 Arm                 & 7                     & 86.28                                & 1.02                               & 94.26                                & 0.73                               & 99.25                                & 0.50                               \\ \midrule
NASA Robosimian Arm                & 7                     & 61.74                                & 2.44                               & 99.87                                & 0.36                               & 99.93                                & 0.44                               \\ \midrule
TRACLabs Modular Arm               & 7                     & 79.11                                & 1.35                               & 95.12                                & 0.63                               & 99.80                                & 0.53                               \\ \midrule
UR10                               & 6                     & 36.16                                & 3.29                               & 88.05                                & 0.82                               & 99.47                                & 0.49                               \\ \midrule
UR5                                & 6                     & 35.88                                & 3.30                               & 88.69                                & 0.78                               & 99.55                                & 0.42                               \\ \midrule
NASA Valkyrie Arm                  & 7                     & 45.18                                & 3.01                               & 90.05                                & 1.29                               & 99.63                                & 0.61                               \\ \bottomrule
\end{tabular}}
\end{table}
-->

## State Machines Concepts

Insert Text Explaining the Basics of State Machine and How They are Implemented in SMACH.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/graphical_view_of_state_machine_using_smach.png}
  \caption{Graphical View of State Machine using SMACH}
  \label{fig:graphical_view_of_state_machine_using_smach}
\end{figure}
-->

## Camera Concepts

Insert Text Explaining How Digital Image Are Formed and Stored.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/photometric_image_formation.png}
  \caption{Photmetric Image Formation.}
  \label{fig:photmetric_image_formation}
\end{figure}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/digital_camera_diagram.png}
  \caption{Digital Camera Diagram.}
  \label{fig:digital_camera_diagram}
\end{figure}
-->

Insert Text Explaining Ground Sample Distance and It's Effects on Image Quality Thereby Effecting Ones Camera Choice.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/spatial_resolution.png}
  \caption{Ground Sample Distance Effects on Image Quality.}
  \label{fig:ground_sample_distance_effects_on_image_quality}
\end{figure}
-->

Insert Text Explaining The Effects of Different Shutter Type on Image Quality Thereby Effecting Ones Camera Choice.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/global_vs_rolling_shutter.png}
  \caption{Shutter Type Effects on Image Quality.}
  \label{fig:shutter_type_effects_on_image_quality}
\end{figure}
-->

## Computer Vision Concepts

Insert Text Regarding Upcoming Sections.

### Color Spaces

Insert Text Explaining Both RGB Color Space and HSV/HSL Color Space and Why One Would Be Chosen Over The Other.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/rgb_and_hsv_color_space_models.png}
  \caption{RGB and HSV color space models}
  \label{fig:RGB_and_HSV_color_space_models}
\end{figure}
-->

### Linear and Non-Linear Filters

Insert Text Explaining the Basics behind Linear Filters, Which Ones Were Chosen and Why.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/common_linear_filters.png}
  \caption{Effects of common linear filters.}
  \label{fig:common_linear_filters}
\end{figure}
-->

Insert Text Explaining the Basics behind Histogram Equalization.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/histogram_equalization.png}
  \caption{Histogram equalization depiction.}
  \label{fig:histogram_equalization_depiction}
\end{figure}
-->

Insert Text Explaining The Effect of Histogram Equalization on Images.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/image_equalization.png}
  \caption{Effect of histogram equalization on an image.}
  \label{fig:effects_of_histogram_equalization_on_an_image}
\end{figure}
-->

### Binary Operations

Insert Text Explaining the Basics behind Binary Operations, Which Ones Were Chosen and Why.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/binary_operations.png}
  \caption{Effects of common binary operations.}
  \label{fig:common_binary_operations}
\end{figure}
-->

### Hough Circle Detection

Insert Text Explaining The Basic Preprocessing Operation Performed by Hough Circle Transform In Order to Get Edges.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/edge_detection_hough_circle.png}
  \caption{Summary of preprocessing operation performed by Hough Circle detector.}
  \label{fig:summary_of_preprocessing_operation_performed_by_Hough_Circle_detector}
\end{figure}
-->


Insert Text Explaining the Basic Intuition Behind Hough Circle with Know Radius.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/hough_circle_summary_known_r.png}
  \caption{Summary of Hough Circle detector with known radius.}
  \label{fig:summary_of_hough_circle_detector_with_known_radius}
\end{figure}
-->

Insert Text Expanding Basic Intuition Behind Hough Circle with Unknown Radius.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/hough_circle_summary_unknown_r.png}
  \caption{Summary of Hough Circle detector with unknown radius.}
  \label{fig:summary_of_hough_circle_detector_with_unknown_radius}
\end{figure}
-->


### Good Feature To Track

Insert Text Explaining the Basic Intuition Behind Good Feature.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/good_feature_basic_intuition.png}
  \caption{Summary of Good Feature detector.}
  \label{fig:summary_of_good_feature_detector}
\end{figure}
-->

Insert Text Explaining an Image Gradient/Derivative and Why We Look for Them.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/image_gradient_example.png}
  \caption{Image Gradient Example.}
  \label{fig:image_gradient_example}
\end{figure}
-->

Insert Text Explaining Math.

<!--
\begin{equation}
E(u,v) = \sum_{x,y}w(x,y)[\textit{I}(x+u,y+v)-\textit{I}(x,y)]^2
\label{eq:change_of_intensity_for_the_shift_uv}
\end{equation}
-->

<!--
\begin{equation}
\sum_{x,y}[\textit{I}(x+u,y+v)-\textit{I}(x,y)]^2
\label{eq:maximize_intensity}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\approx\sum_{x,y}[\textit{I}(x,y)+uI_{x}+vI_{y}-\textit{I}(x,y)]^2
\label{eq:taylor_series_expansion}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\approx\sum_{x,y}u^2I_{x}^2+2uvI_{x}I_{y}+v^2I_{y}^2
\label{eq:expanding_and_cancelling_properly}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\approx\begin{bmatrix}
u & v
\end{bmatrix}\bigg(\sum_{x,y}w(x,y)\begin{bmatrix}
I_{x}^2 & I_{x}I_{y}\\
I_{x}I_{y} & I_{y}^2
\end{bmatrix} \bigg)\begin{bmatrix}
u\\
v
\end{bmatrix}
\label{eq:express_in_matrix_form}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\cong\begin{bmatrix}
u & v
\end{bmatrix}M\begin{bmatrix}
u\\
v
\end{bmatrix}
\label{eq:simplify_matrix}
\end{equation}
-->

Where $$M=\sum_{x,y}w(x,y)\begin{bmatrix}
I_{x}^2 & I_{x}I_{y}\\
I_{x}I_{y} & I_{y}^2
\end{bmatrix}$$

Insert Text Explaining How Both Harris Corner and Good Feature Scoring Functions Work and Why Good Feature Performs Slightly Higher.

<!--
\begin{equation}
R = \textup{det}(M) - k(\textup{trace}(M))^2
\label{eq:harris_corner_scoring_function}
\end{equation}
-->

Where $\textup{det}(M) = \lambda_{1}\lambda_{2}$ and $\textup{trace}(M) = \lambda_{1} + \lambda_{2}$. So,

<!--
\begin{equation}
R = \lambda_{1}\lambda_{2} - k(\lambda_{1}+\lambda_{2})^2
\label{eq:simplified_harris_corner_scoring_function}
\end{equation}
-->

<!--
\begin{equation}
R = \textup{min}(\lambda_{1},\lambda_{2})
\label{eq:good_feature_scoring_function}
\end{equation}
-->


<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/good_feature_harris_scoring_comparison.png}
  \caption{Difference Between Good Feature and Harris Corner Scoring Functions.}
  \label{fig:difference_between_good_feature_and_harris_corner_scoring_functions}
\end{figure}
-->

### Optical Flow

Insert Text Explaining the Basic Intuition Behind Optical Flow.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/optical_flow_example.png}
  \caption{Example of optical flow.}
  \label{fig:example_of_optical_flow}
\end{figure}
-->

Insert Text Explaining the Basic Intuition Behind Kanade-Lucas-Tomasi Feature Tracker.

<!--
\begin{equation}
\sum_{x} [\textit{T}(\textbf{W}(\textbf{x;}\Delta\textbf{p}))-\textit{I}(\textbf{W(x;p}))]^2
\label{eq:klt1}
\end{equation}
-->

<!--
\begin{equation}
\sum_{x} \bigg[\textit{T}(\textbf{W}(\textbf{x;0}))+\nabla \textbf{T}\frac{\partial\textbf{W}}{\partial\textbf{p}}\Delta\textbf{p}-\textit{I}(\textbf{W}(\textbf{x;p}))\bigg]^2
\label{eq:klt2}
\end{equation}
-->

<!--
\begin{equation}
\Delta\textbf{p}=\textit{H}^{-1}\sum_{x} \bigg[\nabla \textbf{T}\frac{\partial\textbf{W}}{\partial\textbf{p}} \bigg]^T[\textit{I}(\textbf{W}(\textbf{x;p}))-\textit{T}(\textbf{x})]
\label{eq:klt3}
\end{equation}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/klt_optimization.png}
  \caption{Summary of Kanade-Lucas-Tomasi feature tracker.}
  \label{fig:summary_of_Kanade-Lucas-Tomasi_feature_tracker}
\end{figure}
-->

### Pose Estimation and Tracking Through Augmented Reality Tag Detection

Insert Text Describing How AR Tag Detection, Pose Estimation, and Tracking Works with AR_Track_Alvar.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/ar_track_alvar_framework.png}
  \caption{Summary of ALVAR AR Tag Detection Framework.}
  \label{fig:summary_of_alvar_ar_tag_detection_framework}
\end{figure}
-->

## Summary

Insert Text Summarizing This Chapter and Transiting to the Next.

# Multistage Localization for High Precision Mobile Manipulation Tasks

Insert Text Outlining the Upcoming Sections.

## Approach Overview

Insert Text Outlining the Steps of the Approach.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/multistage_localization_approach.png}
  \caption{Multistage Localization Approach Overview.}
  \label{fig:multistage_localization_approach_overview}
\end{figure}
-->

## Global Map Creation and Task Location Specification

The purpose of this section is reiterate why KartoSLAM was chosen, as well as describe how maps are created, the map files themselves, and how global locations are specified.

Before the manipulator can localize itself with respect to the part, the system must first navigate to the general vicinity in which the work will take place. In order to achieve this, the system utilizes odometry data, given by wheel encoders and an onboard inertial measurement unit (IMU), as well as sensor data, such as laser scans from a LIDAR or point clouds from an RGB-D sensor, to output safe velocity commands that will be sent to the mobile base of the system.

First, a Simultaneous Localization and Mapping (SLAM) technique named KartoSLAM uses the system’s odometry and laser scan or point cloud data, to create a 2-D map of the environment in which the operation(s) will take place. After which, the global location of specific operation(s) are defined, as shown in Figure <!--\ref{fig:global_map}-->.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/2d_map_with_specified_start_locations.png}
  \caption{2-D map environment with specified start locations.}
  \label{fig:global_map}
\end{figure}
-->

## Autonomous Localization and Navigation

Adaptive Monte Carlo localization (amcl) is used to localize the system within the map. Subsequently, odometry data is combined with a global and local cost map, in which obstacles and a specific distance around them represent a cost. These maps are used to plan optimal and obstacle free paths through the environment. The global path is computed before the system begins moving and takes into account all known obstacles, while the local path monitors incoming sensor data to compute suitable linear and angular velocities for the system to complete the current section of the global path. The local path is typically computed at a rate of 20 Hz; however, this parameter is adjustable given the needs of the system.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/robot_localization_framework.png}
  \caption{Robot Localization Framework.}
  \label{fig:robot_localization_framework}
\end{figure}
-->

## Task Association and A Priori Knowledge

Once arriving in the general vicinity of the task to be accomplished, the system then locates an augmented reality (AR) tag [#Siltanen], which allows it to localize and transform points of interest (POIs) associated with the AR tag into the system’s frame of reference. This general localization framework is presented in Figure <!--\ref{fig:task_association}-->.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/initial_feature_localization_framework.png}
  \caption{Initial Feature Localization Framework.}
  \label{fig:initial_feature_localization_framework}
\end{figure}
-->

## Generic Framework for Multi-Stage Computer Vision Algorithm

Insert Text Outlining the Upcoming Sections.

### Initial Feature Location Prediction

This subsection will go through AR tags and how they are used to populate point.

### Corrected Feature Locations

Currently the sponsor of this work uses a hole template on the object to be drilled to ensure accuracy within $\pm$ 0.3 mm. Using the predicted hole locations, given by the AR tag, an inverse kinematic solver is used to move the manipulator to the specified Cartesian location. A camera mounted on the manipulator is then used to further correct the positon of the end-effector. Canny Edge Detection, Hough Transforms, as well as the camera’s intrinsic characteristics and a priori knowledge of each hole’s size is used to output an adjusted Cartesian location of the circle on the templet closest to the predicted position [#Alter1992]. If a hole to be drilled is not found or is outside the range of the manipulator, that hole will be added to a list and the customer notified of all such holes after the operation is completed. In addition to the high precision achieved by the above technique, it can also provide a video log of all work done for inspection.

Insert Text Regarding Computer Vision Detection and Tracking System.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth, height=\textheight, keepaspectratio]{figures/feature_detection_and_tracking_pipeline.png}
  \caption{Feature Detection and Tracking Pipeline.}
  \label{fig:feature_detection_and_tracking_pipeline}
\end{figure}
-->

Insert Text Regarding Control Loop.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/manipulator_control_loop.png}
  \caption{Manipulator Correction Control Loop.}
  \label{fig:manipulator_correction_control_loop}
\end{figure}
-->

## Approach Implementation

Outline the upcoming sections.

### System Overview

The generic framework of the system is depicted in Figure <!--\ref{fig:general_system_overview}-->. The system allows the user to input predefined tasks. Given a priori knowledge of each tasks and their global start locations, the mobile system navigates to and performs the requested operation(s) while monitoring its battery level to ensure mission completion. The framework shown was implemented using a hierarchical state machine and was written in such a way as to make it compatible with the Robot Operating System (ROS) to ensure ease of use across differing robotic platforms.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/general_system_overview.png}
  \caption{General system overview.}
  \label{fig:system_overview}
\end{figure}
-->

### Drilling Framework

This section will go in depth into how the robot simulates drilling.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/drilling_operation_framework.png}
  \caption{Drilling operation framework.}
  \label{fig:drilling_operation_framework}
\end{figure}
-->

### Sealant Application Framework

This section will go in depth into how the robot simulates sealant application.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/sealant_application_framework.png}
  \caption{Sealant Application Framework.}
  \label{fig:sealant_application_framework.}
\end{figure}
-->

## Summary

Insert Text Summarizing This Chapter and Transiting to the Next.

# Experiments and Results

Outline the upcoming sections.

## Hardware Architecture

Insert Text Regarding Robot, Sensors, etc.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/husky_and_fetch_hardware_architecture.png}
  \caption{Clearpath Robotics' Husky and Fetch Robotics' Fetch Mobile Manipulator}
  \label{fig:clearpath_robotics'_husky_and_fetch_robotics'_fetch_mobile_manipulator}
\end{figure}
-->

## Software Architecture

Insert Text Regarding The Software Architecture.

## Experiments

Insert Text Regarding Upcoming Sections.

### Camera Calibration Setup

Insert text setup for camera calibration.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/camera_calibration_setup.png}
  \caption{Camera Calibration Setup.}
  \label{fig:camera_calibration_setup}
\end{figure}
-->

Insert text regarding methods used for camera calibration (citing paper)

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/image_extracted_corners.png}
  \caption{Extracted Corners of Calibration Pattern.}
  \label{fig:extracted_corners_of_calibration_pattern.}
\end{figure}
-->

Insert Text and Math Regarding Calibration of Laser.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/laser_extracted_corners_laser_center.png}
  \caption{Extracted Corners and Laser Center}
  \label{fig:extracted_corners_and_laser_center}
\end{figure}
-->

### Navigation System Experimental Setup

Insert Text Regarding the Navigation System Experimental Setup.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/navigation_system_experimental_setup.png}
  \caption{Navigation System Experimental Setup.}
  \label{fig:navigation_system_experimental_setup}
\end{figure}
-->

### Drilling Operation Experimental Setup

Insert Text Regarding the Drilling Operations Experimental Setup.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/drilling_operation_experimental_setup.png}
  \caption{Drilling Operations Experimental Setup.}
  \label{fig:drilling_operations_experimental_setup}
\end{figure}
-->

### Sealant Application Experimental Setup

Insert Text Regarding the Sealant Application Experimental Setup.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/sealant_application_experimental_setup.png}
  \caption{Sealant Application Experimental Setup.}
  \label{fig:sealant_application_experimental_setup}
\end{figure}
-->

## Results

### Camera Calibration Accuracy Achieved

Insert Text Regarding Pixel Error of Kinect V2.

Insert Text Regarding Pixel Error of Basler ACA150-UC.

Insert Text Regarding Pixel Error of Laser Calibration.

### Navigation System Accuracy Achieved

Insert Text Regarding Accuracy of Navigation System and Ways To Improve Accuracy.

<!--
\begin{table}[H]
\centering
\caption{Global Localization Accuracy Given Distance From Start - Clearpath Robotics' Husky.}
\label{global_localization_accuracy_given_distance_from_start_clearpath_robotics_husky}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|l|l|c|@{}}
\cmidrule(l){2-7}
                           & x (cm) & x std (cm) & y (cm) & y std (cm) & $\psi$ (rad) & $\psi$ std (rad) \\ \midrule
\multicolumn{1}{|c|}{5 m}  & 6.29   & 4.20       & 4.50   & 3.58       & 0.03         & 0.02             \\ \midrule
\multicolumn{1}{|c|}{10 m} & 4.16   & 3.16       & 2.88   & 2.97       & 0.03         & 0.03             \\ \midrule
\multicolumn{1}{|c|}{20 m} & 6.35   & 4.55       & 2.37   & 1.89       & 0.02         & 0.01             \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/husky_navigation_xy_accuracy.png}
  \caption{Global X and Y Localization Accuracy Given Distance From Start - Clearpath Robotics' Husky.}
  \label{fig:global_x_and_y_localization_accuracy_given_distance_from_start_clearpath_robotics_husky}
\end{figure}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/husky_navigation_yaw_accuracy.png}
  \caption{Global $\psi$ Localization Accuracy Given Distance From Start - Clearpath Robotics' Husky.}
  \label{fig:global_psi_localization_accuracy_given_distance_from_start_clearpath_robotics_husky}
\end{figure}
-->

<!--
\begin{table}[H]
\centering
\caption{Global Localization Accuracy Given Distance From Start - Fetch Robotics' Fetch.}
\label{global_localization_accuracy_given_distance_from_start_fetch_robotics_fetch}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|l|l|c|@{}}
\cmidrule(l){2-7}
                           & x (cm) & x std (cm) & y (cm) & y std (cm) & $\psi$ (rad) & $\psi$ std (rad) \\ \midrule
\multicolumn{1}{|c|}{5 m}  & 4.25   & 2.50       & 1.48   & 1.23       & 0.03         & 0.02             \\ \midrule
\multicolumn{1}{|c|}{10 m} & 3.93   & 2.62       & 2.57   & 1.66       & 0.03         & 0.02             \\ \midrule
\multicolumn{1}{|c|}{20 m} & 5.82   & 3.43       & 1.52   & 1.11       & 0.03         & 0.02             \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/fetch_navigation_xy_accuracy.png}
  \caption{Global X and Y Localization Accuracy Given Distance From Start - Fetch Robotics' Fetch.}
  \label{fig:global_x_and_y_localization_accuracy_given_distance_from_start_fetch_robotics_fetch}
\end{figure}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/fetch_navigation_yaw_accuracy.png}
  \caption{Global $\psi$ Localization Accuracy Given Distance From Start - Fetch Robotics' Fetch.}
  \label{fig:global_psi_localization_accuracy_given_distance_from_start_fetch_robotics_fetch}
\end{figure}
-->

### Augmented Reality Tag Accuracy Achieved

<!--
\begin{table}[H]
\centering
\caption{Augmented Reality Accuracy Given Specific Start Conditions - Primesense Carmine 1.09.}
\label{augmented_reality_accuracy_given_specific_start_conditions_primesense_carmine_1_09}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|c|c|c|@{}}
\cmidrule(l){2-7}
                        & x     & x std (mm) & y      & y std (mm) & z      & z std (mm) \\ \midrule
\multicolumn{1}{|c|}{1} & 50.41 & 22.81      & 148.87 & 12.33      & 150.96 & 12.61      \\ \midrule
\multicolumn{1}{|c|}{2} & 23.71 & 16.06      & 151.82 & 6.86       & 117.57 & 12.81      \\ \midrule
\multicolumn{1}{|c|}{3} & 23.06 & 14.85      & 128.39 & 11.48      & 173.24 & 23.24      \\ \midrule
\multicolumn{1}{|c|}{4} & 37.19 & 14.85      & 183.21 & 7.30       & 115.85 & 16.27      \\ \midrule
\multicolumn{1}{|c|}{5} & 24.36 & 17.47      & 138.66 & 6.77       & 42.80  & 16.37      \\ \midrule
\multicolumn{1}{|c|}{6} & 39.16 & 23.52      & 7.27   & 3.70       & 105.24 & 9.16       \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/ar_tag_accuracy_primesense.png}
  \caption{Augmented Reality Accuracy Given Specific Start Conditions - Primesense Carmine 1.09.}
  \label{fig:augmented_reality_accuracy_given_specific_start_conditions_primesense_carmine_1_09}
\end{figure}
-->

<!--
\begin{table}[H]
\centering
\caption{Augmented Reality Accuracy Given Specific Start Conditions - Microsoft Kinect V2.}
\label{augmented_reality_accuracy_given_specific_start_conditions_microsoft_kinect_v2}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|l|l|c|@{}}
\cmidrule(l){2-7}
                             & x (mm) & x std (mm) & y (mm) & y std (mm) & z (mm) & z std (mm) \\ \midrule
\multicolumn{1}{|c|}{Test 1} & 14.32  & 9.75       & 4.04   & 1.65       & 84.77  & 2.50       \\ \midrule
\multicolumn{1}{|c|}{Test 2} & 163.85 & 57.58      & 36.32  & 9.65       & 25.31  & 12.46      \\ \midrule
\multicolumn{1}{|c|}{Test 3} & 84.33  & 44.37      & 50.31  & 5.48       & 55.24  & 8.57       \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/ar_tag_accuracy_kinectv2.png}
  \caption{Augmented Reality Accuracy Given Specific Start Conditions - Microsoft Kinect V2.}
  \label{fig:augmented_reality_accuracy_given_specific_start_conditions_microsoft_kinect_v2}
\end{figure}
-->


### Drilling Operation Accuracy Achieved

Insert Text Regarding Accuracy of Drilling Operations and Ways To Improve Accuracy.

<!--
\begin{table}[H]
\centering
\caption{Drilling Operation Accuracy Given Distance From Work Surface.}
\label{drilling_operation_accuracy_given_distance_from_work_surface}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|c|c|c|@{}}
\cmidrule(l){2-7}
                                                                                                                 & x (mm) & x std (mm) & y (mm) & y std (mm) & z (mm) & z std (mm) \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Less Than 5 mm from\\   Hole Center\end{tabular}}       & 13.25  & 1.43       & 0.70   & 0.43       & 0.82   & 0.56       \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Between 5 and 10 mm\\   from Hole Center\end{tabular}}  & 13.62  & 2.71       & 0.66   & 0.39       & 1.89   & 0.46       \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Between 10 and 20 mm\\   from Hole Center\end{tabular}} & 10.78  & 2.55       & 0.53   & 0.30       & 4.06   & 0.70       \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Between 20 and 30 mm\\   from Hole Center\end{tabular}} & 10.55  & 2.48       & 0.31   & 0.21       & 10.25  & 1.38       \\ \midrule
\multicolumn{1}{|c|}{Mean}                                                                                       & 12.05  & 2.55       & 0.55   & 0.38       & 4.25   & 3.75       \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/drill_accuracy.png}
  \caption{Drilling Operation Accuracy Given Distance From Feature Point.}
  \label{fig:drilling_operation_accuracy_given_distance_from_feature_point}
\end{figure}
-->

### Sealant Application Accuracy Achieved

Insert Text Regarding Accuracy of Sealant Application and Ways To Improve Accuracy.

<!--
\begin{table}[H]
\centering
\caption{Sealant Application Accuracy Given Distance From Work Surface.}
\label{sealant_application_accuracy_given_distance_from_work_surface}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|c|c|c|@{}}
\cmidrule(l){2-7}
                                                                                                            & x (mm) & x std (mm) & y (mm) & y std (mm) & z (mm) & z std (mm) \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Less Than 5 mm from\\   Corner\end{tabular}}       & 10.90  & 2.44       & 1.46   & 0.21       & 1.17   & 0.71       \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Between 5 and 10 mm\\   from Corner\end{tabular}}  & 11.95  & 3.21       & 1.27   & 0.22       & 1.48   & 0.91       \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Between 10 and 20 mm\\   from Corner\end{tabular}} & 16.32  & 2.56       & 0.78   & 0.34       & 3.81   & 0.32       \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Between 20 and 30 mm\\   from Corner\end{tabular}} & 13.92  & 3.34       & 0.79   & 0.60       & 8.21   & 0.40       \\ \midrule
\multicolumn{1}{|c|}{Mean}                                                                                  & 13.27  & 3.33       & 1.08   & 0.48       & 3.67   & 2.88       \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/seal_accuracy.png}
  \caption{Sealant Application Accuracy Given Distance From Feature Point.}
  \label{fig:sealant_application_accuracy_given_distance_from_feature_point}
\end{figure}
-->

## Summary

Insert Text Summarizing This Chapter and Transiting to the Next.

# Conclusion and Future Work

This paper showed the general framework necessary in order to solve the two main problems preventing the development and widespread adoption of ARC. These problems have caused a decrease in productivity and increase in workplace injuries/fatalities over the past several decades in comparison to other industries [#Rojas2003]. These problems include the fact that typical construction sites tend to be unstructured and are continuously evolving versus the highly controlled environments found in manufacturing. Also, the relationship between the part and manipulator has been reversed, causing increased complexity not seen in manufacturing environments where the part arrives at a fixed manipulator [#Feng2015]. The techniques presented allow systems to create a 2-D map of their environment, localize themselves and complete the task(s) assigned. After localizing an AR tag at the work site, the system is able to use a priori knowledge to localize POIs and complete a plethora of operations achieving an accuracy of approximately $\pm$ 2 mm based on a multifaceted computer vision approach with only a USB webcam.

Future work to be explored includes increasing the accuracy of the computer vision system to the sub-millimeter levels through the use of a machine vision camera, as well as a relatively new calibration technique developed by Feng et al [#Feng2015]. In addition, automated approaches to create 3-D maps are being looked into in order to provide updated data about the robot’s surroundings and task(s) automatically without human intervention. Also, human and robot collaboration over a distributed network is being explored.
