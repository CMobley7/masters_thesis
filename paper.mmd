Base Header Level: 2
latex input: metadata
latex author: Christopher James Mobley
latex title: TITLE OF THESIS
bibtex: masters_thesis
myreporttype: Thesis
mydegree: Masters of Science
myyear: 2016
mydepartment: Mechanical Engineering
latex input: vtthesis/setup.tex
latex footer: vtthesis/footer.tex

# Introduction

Unlike the substantial benefits seen in the manufacturing industry through automation and robotics, automation and robotics in construction (ARC) has lagged far behind in adoption [#Balaguer]. Consequently, when compared with other industries, construction has seen a significant decrease in productivity, as well as an increase in workplace injuries/fatalities over the last several decades [#Rojas2003]. While several technical complexities inherent in construction have hindered the development and adoption of field construction robots [4], through the capitalization of advances made by other industries, ARC can quickly close this gap. Thereby allowing dangerous and or mundane repetitive tasks to be accomplished autonomously. Thus, causing an increase in productivity and a decrease in workplace injuries/fatalities [1]. However, ARC faces two unique challenges when compared to other industries. Unlike manufacturing environments, which are tightly controlled, typical construction sites tend to lack structure and are continuously evolving. In addition, the reversal in relationship between the part and manipulator has dramatically increased the complexity of the problem to be solved [#Feng2015]. Instead of the part appearing at a fixed manipulator, the manipulator must now move to and localize itself with respect to the part. The remainder of this paper is structured as follows: In Section 2, the authorâ€™s technical approach is outlined, with particular focus on problem two, and experimental results are shown. Conclusions are then drawn and future work discussed in Section 3.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{future_transformable_production_system.png}
  \caption{Vision for future transformable production systems.}
  \label{fig:vision_for_future_transformable_production_systems}
\end{figure}
-->

# Background

Insert text outlining upcoming sections.

## ROS Concepts

Insert Text Defining ROS.

### ROS Communication

Insert Text Regarding Node/Topic/Service Communication.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{simple_ros_node_diagram.png}
  \caption{Simple ROS Node Flowchart}
  \label{fig:simple_ros_node_flowchart}
\end{figure}
-->

### URDF and TF

Insert Regarding How To Setup Up TFs on a Robot and How TF Communication Works.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Robot Model with TFs}
  \label{fig:robot_model_with_tfs}
\end{figure}
-->

### SLAM

Insert Text Outlining the Basic Intuition Behind SLAM.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Overview of SLAM Process. [Riisgaard et al. 2005]}
  \label{fig:overview_of_slam_process}
\end{figure}
-->

Insert Text Explaining How ROS Uses SLAM to Create 2-D Occupancy Grid Maps and What How They Are Used.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Example of 2-D Occupancy Grid Map Produced.}
  \label{fig:example_of 2-D Occupancy Grid Map Produced}
\end{figure}
-->

Insert Text Specifying Available 2-D SLAM Techniques and Why KartoSLAM was Chosen.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Real World Performance Analysis of ROS Available SLAM Algorithms. [Santos et al. 2013]}
  \label{fig:real_world_performance_analysis_of_ros_available_slam_algorithms}
\end{figure}
-->

|            |          | Real World Experiments |          |          |
|:----------:|:--------:|:----------------------:|:--------:|:--------:|
| HectorSLAM | GMapping | KartoSLAM              | CoreSLAM | LagoSLAM |
| 1.1972     | 2.1716   | 1.0318                 | 14.75333 | 3.0264   |
| 0.5094     | 0.6945   | 0.3742                 | 7.9463   | 0.8181   |
| 1.0656     | 1.6354   | 0.9080                 | 7.5824   | 2.5236   |
[Real World Error Estimation for ROS Available SLAM Algorithm. [Santos et al. 2013]]

### Navigation Stack

Insert Text Outlining The Navigation Stack's Framework.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{ROS Navigation Stack setup.}
  \label{ros_navigation_stack_setup}
\end{figure}
-->

Insert Text Explaining How Adaptive Monte Carlo Localization Works.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Adaptive Monte Carlo Localization Flowchart.}
  \label{fig:adaptive_monte_carlo_localization_flowchart}
\end{figure}
-->

Insert Text Explaining How Trajectory Rollout Works for Path Planning.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Trajectory Rollout Path Planning Framework}
  \label{fig:trajectory_rollout_path_planning_framework}
\end{figure}
-->

### Moveit!

Insert Text Outlining Moveit!'s System Architecture.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Moveit!'s System Architecture.}
  \label{fig:moveit!'s_system_architecture}
\end{figure}
-->

Insert Text Explaining the Basic Intuition Behind Forward and Inverse Kinematics.

Insert Test Explaining the Difference Between KDL, KDL-RR and Trac_IK and Why Trac_IK was chosen to be used.

| Chain                              | DOFs | Orocos' KDL solve rate | Orocos' KDL Avg Time | KDL-RR solve rate | KDL-RR Avg Time | TRAC-IK solve rate | TRAC-IK Avg Time |
|:----------------------------------:|:----:|:----------------------:|:--------------------:|:-----------------:|:---------------:|:------------------:|:----------------:|
| Atlas 2013 arm                     | 6    | 75.54%                 | 1.35ms               | 97.13%            | 0.39ms          | 99.97%             | 0.33ms           |
| Atlas 2015 arm                     | 7    | 75.71%                 | 1.50ms               | 93.13%            | 0.81ms          | 99.18%             | 0.48ms           |
| Baxter arm                         | 7    | 61.07%                 | 2.21ms               | 89.52%            | 1.02ms          | 99.17%             | 0.60ms           |
| Denso VS-068                       | 6    | 27.92%                 | 3.69ms               | 98.13%            | 0.42ms          | 99.78%             | 0.38ms           |
| Fanuc M-430iA/2F                   | 5    | 21.07%                 | 3.99ms               | 88.34%            | 0.92ms          | 99.16%             | 0.58ms           |
| Fetch arm                          | 7    | 92.49%                 | 0.73ms               | 93.82%            | 0.72ms          | 99.96%             | 0.44ms           |
| Jaco2                              | 6    | 26.23%                 | 3.79ms               | 97.66%            | 0.58ms          | 99.51%             | 0.58ms           |
| KUKA LBR iiwa 14 R820              | 7    | 37.71%                 | 3.37ms               | 94.02%            | 0.73ms          | 99.63%             | 0.56ms           |
| KUKA LWR 4+                        | 7    | 67.80%                 | 1.88ms               | 95.40%            | 0.62ms          | 99.95%             | 0.38ms           |
| PR2 arm                            | 7    | 83.14%                 | 1.37ms               | 86.96%            | 1.27ms          | 99.84%             | 0.59ms           |
| NASA Robonaut2 'grasping leg'      | 7    | 61.27%                 | 2.29ms               | 87.57%            | 1.10ms          | 99.31%             | 0.67ms           |
| NASA Robonaut2 'leg' + waist + arm | 15   | 97.99%                 | 0.80ms               | 98.00%            | 0.84ms          | 99.86%             | 0.79ms           |
| NASA Robonaut2 arm                 | 7    | 86.28%                 | 1.02ms               | 94.26%            | 0.73ms          | 99.25%             | 0.50ms           |
| NASA Robosimian arm                | 7    | 61.74%                 | 2.44ms               | 99.87%            | 0.36ms          | 99.93%             | 0.44ms           |
| TRACLabs modular arm               | 7    | 79.11%                 | 1.35ms               | 95.12%            | 0.63ms          | 99.80%             | 0.53ms           |
| UR10                               | 6    | 36.16%                 | 3.29ms               | 88.05%            | 0.82ms          | 99.47%             | 0.49ms           |
| UR5                                | 6    | 35.88%                 | 3.30ms               | 88.69%            | 0.78ms          | 99.55%             | 0.42ms           |
| NASA Valkyrie arm                  | 7    | 45.18%                 | 3.01ms               | 90.05%            | 1.29ms          | 99.63%             | 0.61ms           |
[Comparison Between ROS Available Moveit! Inverse Kinematic Plugins. [Beeson et al. 2015]]

### SMACH

Insert Text Explaining the Basics of State Machine and How They are Implemented in SMACH.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Graphical View of State Machine using SMACH}
  \label{fig:graphical_view_of_state_machine_using_smach}
\end{figure}
-->

## Camera Concepts

Insert Text Explaining How Digital Image Are Recorded and Stored.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Digital Image Formation.}
  \label{fig:digital_image_formation}
\end{figure}
-->

Insert Text Explaining Ground Sample Distance and It's Effects on Image Quality Thereby Effecting Ones Camera Choice.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Ground Sample Distance Effects on Image Quality.}
  \label{fig:ground_sample_distance_effects_on_image_quality}
\end{figure}
-->

Insert Text Explaining The Effects of Different Shutter Type on Image Quality Thereby Effecting Ones Camera Choice.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Shutter Type Effects on Image Quality.}
  \label{fig:shutter_type_effects_on_image_quality}
\end{figure}
-->

## Computer Vision Concepts

Insert Text Regarding Upcoming Sections.

### Color Spaces

Insert Text Explaining Both RGB Color Space and HSV/HSL Color Space and Why One Would Be Chosen Over The Other.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{rgb_and_hsv_color_space_models.png}
  \caption{RGB and HSV color space models}
  \label{fig:RGB_and_HSV_color_space_models}
\end{figure}
-->

### Linear and Non-Linear Filters

Insert Text Explaining the Basics behind Linear Filters, Which Ones Were Chosen and Why.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Effects of common linear filters.}
  \label{fig:common_linear_filters}
\end{figure}
-->

Insert Text Explaining the Basics behind Non-Linear Filters, Which Ones Were Chosen and Why.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Effect of histogram equalization.}
  \label{fig:effects_of_histogram_equalization}
\end{figure}
-->

### Binary Operations

Insert Text Explaining the Basics behind Binary Operations, Which Ones Were Chosen and Why.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Effects of common binary operations.}
  \label{fig:common_binary_operations}
\end{figure}
-->

### Hough Circle

Insert Text Explaining the Basic Intuition Behind Hough Circle.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Summary of Hough Circle detector.}
  \label{fig:summary_of_hough_circle_detector}
\end{figure}
-->

### Good Feature To Track

Insert Text Explaining the Basic Intuition Behind Good Feature.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Summary of Good Feature to Track detector.}
  \label{fig:summary_of_good_feature_to_track_detector}
\end{figure}
-->

### Optical Flow

Insert Text Explaining the Basic Intuition Behind Optical Flow.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Example of optical flow.}
  \label{fig:example_of_optical_flow}
\end{figure}
-->

Insert Text Explaining the Basic Intuition Behind Kanade-Lucas-Tomasi Feature Tracker.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Summary of Kanade-Lucas-Tomasi feature tracker.}
  \label{fig:summary_of_Kanade-Lucas-Tomasi_feature_tracker}
\end{figure}
-->

# Related Work

This section will discuss related work.

# System Overview

The generic framework of the system is depicted in Figure <!--\ref{fig:system_overview}-->. The system allows the user to input predefined tasks. Given a priori knowledge of each tasks and their global start locations, the mobile system navigates to and performs the requested operation(s) while monitoring its battery level to ensure mission completion. The framework shown was implemented using a hierarchical state machine and was written in such a way as to make it compatible with the Robot Operating System (ROS) to ensure ease of use across differing robotic platforms.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{placeholder_fig.png}
  \caption{General system overview.}
  \label{fig:system_overview}
\end{figure}
-->

# System Implementation

Insert Text Outlining the Upcoming Sections.

## Global Map Creation and Task Location Specification

The purpose of this section is reiterate why KartoSLAM was chosen, as well as describe how maps are created, the map files themselves, and how global locations are specified.

Before the manipulator can localize itself with respect to the part, the system must first navigate to the general vicinity in which the work will take place. In order to achieve this, the system utilizes odometry data, given by wheel encoders and an onboard inertial measurement unit (IMU), as well as sensor data, such as laser scans from a LIDAR or point clouds from an RGB-D sensor, to output safe velocity commands that will be sent to the mobile base of the system.

First, a Simultaneous Localization and Mapping (SLAM) technique named KartoSLAM uses the systemâ€™s odometry and laser scan or point cloud data, to create a 2-D map of the environment in which the operation(s) will take place. After which, the global location of specific operation(s) are defined, as shown in Figure <!--\ref{fig:global_map}-->.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{2-D map environment with specified start locations.}
  \label{fig:global_map}
\end{figure}
-->

## Autonomous Localization and Navigation

Adaptive Monte Carlo localization (amcl) is used to localize the system within the map. Subsequently, odometry data is combined with a global and local cost map, in which obstacles and a specific distance around them represent a cost. These maps are used to plan optimal and obstacle free paths through the environment. The global path is computed before the system begins moving and takes into account all known obstacles, while the local path monitors incoming sensor data to compute suitable linear and angular velocities for the system to complete the current section of the global path. The local path is typically computed at a rate of 20 Hz; however, this parameter is adjustable given the needs of the system.

## Task Association and A Priori Knowledge

Once arriving in the general vicinity of the task to be accomplished, the system then locates an augmented reality (AR) tag [#Siltanen], which allows it to localize and transform points of interest (POIs) associated with the AR tag into the systemâ€™s frame of reference. This general localization framework is presented in Figure <!--\ref{fig:task_association}-->.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Task association and localization framework.}
  \label{fig:task_association}
\end{figure}
-->

## Generic Framework for Multi-Stage Computer Vision Algorithm

Insert Text Outlining the Upcoming Sections.

### Sensor Calibration

This subsection will go through the method used for camera calibration (citing paper), as well as specify results in terms of error. In addition, the math behind the laser calibration will be presented.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Sensor Calibration Setup and Results.}
  \label{fig:sensor_calibration_setup_and_results}
\end{figure}
-->

### Initial Feature Location Prediction

This subsection will go through AR tags and how they are used to populate point.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Initial Feature Localization Framework.}
  \label{fig:initial_feature_localization_framework}
\end{figure}
-->

### Corrected Feature Locations

Currently the sponsor of this work uses a hole template on the object to be drilled to ensure accuracy within $\pm$ 0.3 mm. Using the predicted hole locations, given by the AR tag, an inverse kinematic solver is used to move the manipulator to the specified Cartesian location. A camera mounted on the manipulator is then used to further correct the positon of the end-effector. Canny Edge Detection, Hough Transforms, as well as the cameraâ€™s intrinsic characteristics and a priori knowledge of each holeâ€™s size is used to output an adjusted Cartesian location of the circle on the templet closest to the predicted position [#Alter1992]. If a hole to be drilled is not found or is outside the range of the manipulator, that hole will be added to a list and the customer notified of all such holes after the operation is completed. In addition to the high precision achieved by the above technique, it can also provide a video log of all work done for inspection.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Feature Location General Correction Framework.}
  \label{fig:feature_location_general_correction_framework}
\end{figure}
-->

## Specific Task Implementation

Outline the upcoming sections.

### Drilling Framework

This section will go in depth into how the robot simulates drilling.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{drilling_operation_framework.png}
  \caption{Drilling operation framework.}
  \label{fig:drilling_operation_framework}
\end{figure}
-->

### Sealant Application Framework

This section will go in depth into how the robot simulates sealant application.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{drilling_operation_framework.png}
  \caption{Sealant Application Framework.}
  \label{fig:sealant_application_framework.}
\end{figure}
-->

# Experiments and Results

Outline the upcoming sections.

## Hardware Architecture

Insert Text Regarding Robot, Sensors, etc.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Clearpath Robotics' Husky and Fetch Robotics' Fetch Mobile Manipulator}
  \label{fig:clearpath_robotics'_husky_and_fetch_robotics'_fetch_mobile_manipulator}
\end{figure}
-->

## Software Architecture

Insert Text Regarding The Software Architecture.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Software Architecture Diagram}
  \label{fig:software_architecture_diagram}
\end{figure}
-->

## Experiments

Insert Text Regarding Upcoming Sections.

### Navigation System Experimental Setup

Insert Text Regarding the Navigation System Experimental Setup.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Navigation System Experimental Setup.}
  \label{fig:navigation_system_experimental_setup}
\end{figure}
-->

### Drilling Operation Experimental Setup

Insert Text Regarding the Drilling Operations Experimental Setup.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Drilling Operations Experimental Setup.}
  \label{fig:drilling_operations_experimental_setup}
\end{figure}
-->

### Sealant Application Experimental Setup

Insert Text Regarding the Sealant Application Experimental Setup.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{placeholder_fig.png}
  \caption{Sealant Application Experimental Setup.}
  \label{fig:sealant_application_experimental_setup}
\end{figure}
-->

## Results

### Navigation System Accuracy Achieved

Insert Text Regarding Accuracy of Navigation System and Ways To Improve Accuracy.

|                              | Distance from Global Position | x (mm) | y (mm) | z (mm) |
|:----------------------------:|:-----------------------------:|:------:|:------:|:------:|
| Global Position Localization |               5               |    0   |    0   |    0   |
|                              |               10              |    0   |    0   |    0   |
|                              |               20              |    0   |    0   |    0   |
|                              |               40              |    0   |    0   |    0   |
[Global Localization Accuracy Comparison Given Distance From Global Location.]

### Discussion of Drilling Operation Accuracy

Insert Text Regarding Accuracy of Drilling Operations and Ways To Improve Accuracy.

|                      | Distance from Work Surface (m) | x (mm) | y (mm) | z (mm) |
|:--------------------:|:------------------------------:|:------:|:------:|:------:|
| AR Tag Localization  |                5               |    0   |    0   |    0   |
|                      |               10               |    0   |    0   |    0   |
|                      |               20               |    0   |    0   |    0   |
|                      |               40               |    0   |    0   |    0   |
| Feature Localization |                5               |    0   |    0   |    0   |
|                      |               10               |    0   |    0   |    0   |
|                      |               20               |    0   |    0   |    0   |
|                      |               40               |    0   |    0   |    0   |
[Drilling Operation Accuracy Comparison Given Distance From Work Surface.]

### Discussion of Sealant Application Accuracy

Insert Text Regarding Accuracy of Sealant Application and Ways To Improve Accuracy.

|                      | Distance from Work Surface (m) | x (mm) | y (mm) | z (mm) |
|:--------------------:|:------------------------------:|:------:|:------:|:------:|
| AR Tag Localization  |                5               |    0   |    0   |    0   |
|                      |               10               |    0   |    0   |    0   |
|                      |               20               |    0   |    0   |    0   |
|                      |               40               |    0   |    0   |    0   |
| Feature Localization |                5               |    0   |    0   |    0   |
|                      |               10               |    0   |    0   |    0   |
|                      |               20               |    0   |    0   |    0   |
|                      |               40               |    0   |    0   |    0   |
[Sealant Application Accuracy Comparison Given Distance From Work Surface.]

# Conclusion and Future Work

This paper showed the general framework necessary in order to solve the two main problems preventing the development and widespread adoption of ARC. These problems have caused a decrease in productivity and increase in workplace injuries/fatalities over the past several decades in comparison to other industries [#Rojas2003]. These problems include the fact that typical construction sites tend to be unstructured and are continuously evolving versus the highly controlled environments found in manufacturing. Also, the relationship between the part and manipulator has been reversed, causing increased complexity not seen in manufacturing environments where the part arrives at a fixed manipulator [#Feng2015]. The techniques presented allow systems to create a 2-D map of their environment, localize themselves and complete the task(s) assigned. After localizing an AR tag at the work site, the system is able to use a priori knowledge to localize POIs and complete a plethora of operations achieving an accuracy of approximately $\pm$ 2 mm based on a multifaceted computer vision approach with only a USB webcam.

Future work to be explored includes increasing the accuracy of the computer vision system to the sub-millimeter levels through the use of a machine vision camera, as well as a relatively new calibration technique developed by Feng et al [#Feng2015]. In addition, automated approaches to create 3-D maps are being looked into in order to provide updated data about the robotâ€™s surroundings and task(s) automatically without human intervention. Also, human and robot collaboration over a distributed network is being explored.
