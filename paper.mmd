Base Header Level: 2
latex input: metadata
latex author: Christopher James Mobley
latex title: Multistage Localization for High Precision Mobile Manipulation Tasks
bibtex: masters_thesis
myreporttype: Thesis
mydegree: Masters of Science
myyear: 2016
mydepartment: Mechanical Engineering
latex input: vtthesis/setup.tex
latex footer: vtthesis/footer.tex

# Introduction

## Background

Unlike the substantial benefits seen in the manufacturing industry through automation and robotics, automation and robotics in construction (ARC) has lagged far behind in adoption [#Balaguer]. Consequently, when compared with other industries, construction has seen a significant decrease in productivity, as well as an increase in workplace injuries/fatalities over the last several decades [#Rojas2003]. While several technical complexities inherent in construction have hindered the development and adoption of field construction robots [4], through the capitalization of advances made by other industries, ARC can quickly close this gap. Thereby allowing dangerous and or mundane repetitive tasks to be accomplished autonomously. Thus, causing an increase in productivity and a decrease in workplace injuries/fatalities [1]. However, ARC faces two unique challenges when compared to other industries. Unlike manufacturing environments, which are tightly controlled, typical construction sites tend to lack structure and are continuously evolving. In addition, the reversal in relationship between the part and manipulator has dramatically increased the complexity of the problem to be solved [#Feng2015]. Instead of the part appearing at a fixed manipulator, the manipulator must now move to and localize itself with respect to the part. The remainder of this paper is structured as follows: In Section 2, the author’s technical approach is outlined, with particular focus on problem two, and experimental results are shown. Conclusions are then drawn and future work discussed in Section 3.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/future_transformable_production_system.png}
  \caption{Vision for future transformable production systems.}
  \label{fig:vision_for_future_transformable_production_systems}
\end{figure}
-->

## Objectives

Insert Text Regarding the Objectives of This Work.

## Summary of Original Contributions

Insert Text Regarding the Originals Contributions Presented in this Work.

## Outline

Insert Text Outlining the Upcoming Chapters.

# Literature Review

## Localization and Mapping For Autonomous Mobile Manipulators in Manufacturing and Construction

Insert Text Summarizing Current SLAM and Localization Techniques Used.

## Task Association and A Priori Knowledge for Mobile Manipulators

Insert Text Summarizing Current Methods Used to Associate Mobile Manipulator to a Specific Task and How Prior Knowledge is Conveyed About the Task to be Performed.

## Feature Localization Techniques for Mobile Manipulators

Insert Text Summarizing Current Methods Used to Localize a Feature and Have a Mobile Manipulator Perform a Set Operation.

## Summary

Insert Text Summarizing This Chapter and Transiting to the Next.

# Fundamentals of Autonomous Robotics

The purpose of this chapter is to briefly explain all concepts needed to understand the work presented in the chapters thereafter. The following subsection will explain
the basic concepts of the Robot Operating System (ROS), Simultaneous Localization and Mapping (SLAM), Localization and Path Planning, as well as Manipulation. In addition,
Task Execution using State Machines and the specific Computer Vision Algorithms implemented will be expounded upon.

## ROS Concepts

The Robot Operating System (ROS) [#Quigley2009] is an open-source set of software libraries and tools that aim to simplify the task of creating robotics applications that can
be used across a wide variety of platforms. ROS was originally developed in 2007 by Willow Garage as an extension of switchyard, a collection
of robotics software developed by Stanford Artificial Intelligence Laboratory in support of the Stanford AI Robot (STAIR) and Personal Robotics (PR) projects.
The first distribution of ROS, Box Turtle, was released in 2010. ROS currently has had ten major releases. The most current being Kinetic Kame Turtle, which was released
on 23MAY16. In addition ROS boasts tens of thousands of users around the world ranging from hobbyists and researcher to the commercial and industrial industries, as well
as hundreds of packages which provide everything from hardware drivers to algorithms for autonomous navigations and manipulation [#ROSWebsite]. ROS’s large support base act as a force
multiplier allows individuals, Labs, or company to concentrate on one particular aspect while capitalizing on work that has already been done.


### ROS Communication

ROS uses a name server, called the ROS Master, to maintain a list of nodes and available topics. Nodes communicate with the master server using the XML-RPC protocol. While
peer-to-peer communications generally use TCP/IP sockets through the TCPROS protocol. Figure <!--\ref{fig:simple_ros_node_flowchart}--> shows a simple diagram of two ROS nodes
communicating with messages and service topics. In addition to the concepts of messages and services, ROS also employs actions. Actions are similar to service calls, but are
designed for long duration tasks that are capable of providing feedback. These communication interfaces provide ROS a great deal of flexibility for robotic applications [#ROSConcepts], [#Burton2015].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/simple_ros_node_diagram.png}
  \caption{Simple ROS Node Flowchart. Adapted from \protect\cite{Burton2015}.}
  \label{fig:simple_ros_node_flowchart}
\end{figure}
-->

### Rigid Body Transformations

Figure <!--\ref{fig:conversion_from_coordinate_frame_a_to_b}--> shows a purple dot, which represent a point in space. The dot’s coordinates in frames a and b are different. A rigid body
transform, which can be performed using Equation <!--\ref{eq:conversion_from_coordinate_frame_a_to_b}-->, can be used to convert one set of coordinates to another coordinate frame.

<!--
\begin{equation}
x_a=T_{b}^{a}x_b
\label{eq:conversion_from_coordinate_frame_a_to_b}
\end{equation}
-->

where $T_{b}^{a}$ is equal to Equation <!--\ref{eq:simplified_transformation_matrix}-->
<!--
\begin{equation}
\begin{bmatrix}
R_{b}^{a} & t_{b}^{a}\\
0^T & 1
\end{bmatrix}
\label{eq:simplified_transformation_matrix}
\end{equation}
-->

where $R_{b}^{a}$ is the rotation matrix, which performs the rotation part of moving frame $b$ into alignment with frame $a$ and $t_{b}^{a}$ is the translation matrix, which performs
the translation part of moving frame $b$ origin to frame $a$ [#Kelly2013].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/conversion_from_frame_a_b.png}
  \caption{Conversion from Coordinate Frame A to B.}
  \label{fig:conversion_from_coordinate_frame_a_to_b}
\end{figure}
-->

A robotic system, such as Clearpath Robotics’ Husky and Fetch Robotics’ Fetch shows in Figure <!--\ref{fig:robot_model_with_tfs}-->, typical has many three dimensional coordinate frames that change over time as the robot
performs different functions. ROS’s TF [#TF] and TF2 [#TF2] package keep track of coordinate frames and allow for data to be easily convert between these coordinate frames using rigid
body transforms. These coordinate frames, as well as robot’s links/joints they correspond to, and the rigid transformations between them are set up in the Robot’s Unified Robot Description
Format (URDF) File [#URDF].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/robots_with_without_tfs.png}
  \caption{Robot Model with TFs.}
  \label{fig:robot_model_with_tfs}
\end{figure}
-->

## Simultaneous Localization and Mapping Concepts

In order for a mobile robot to operate and perform intricate tasks within a complex, GPS-denied environments, such as that of manufacturing facilities, without modifying said environment,
the robot must be able to create an accurate map of its environment while simultaneously localizing itself within this map using only on-board sensors. Simultaneous localization and mapping
(SLAM) is the problem of building and/or updating a map of an unknown environment while simultaneously localizing the robot within this map [#OpenSLAM]. SLAM was pioneered in the early 1990s by
Hugh F. Durrant-Whyte and John J. Leonard [#Leonard1991], who based their work on research done by Smith and P. Cheeseman in the mid to late-1980s [#Smith1986], [#Smith1990].

Several techniques exist to solve the SLAM problem. Most of these techniques can be categorized into two main paradigms: filtering and optimization-based smoothing [#Grisetti2010a], [#Latif2013]. Filtering techniques
model the SLAM problem as an incremental state estimation, where the state of the system is composed of the robot’s current pose and the map. These estimates are refined at each step by incorporating
current sensor measurements. Due to their incremental nature, these SLAM techniques are typically referred to as on-line SLAM approaches. Popular filtering SLAM techniques include the extended kalman filter, particle filter, and information filters. Filtering SLAM techniques have been used widely in past years
due to their ability to model different sources of noise and their effect on sensor measurements. However, in recent years optimization-based smoothing techniques have proven to be more efficient, scalable,
and robust than that of filtering techniques [#Latif2013]. Unlike filtering techniques, optimization-based smoothing techniques estimate the robot’s entire trajectory and the map. Due to the fact that the final map
is based off the robot’s entire trajectory and world features instead of the most recent pose and map, these techniques are known as full SLAM approaches. These SLAM techniques incorporate a graph-based structure, where
graph nodes represent the robot’s pose and world features, while edges represent a spatial constraint relation between two robot poses given by sensor measurements [#Konolige2010]. The graph is optimized using error
minimization techniques, such as least-squares, in order to refine the robot’s trajectory and map.

In addition to a variety of techniques which can be used to solve the SLAM problem, a wide range of sensors can also be used. Typically sensors include that of a LIDARs, stereo cameras, monocular camera,
and RGB-D sensors.

Figure <!--\ref{fig:overview_of_slam_framework}--> shows the generic framework to solving the SLAM problem. In the front-end, raw sensor inputs are processed in order to extract features and perform scan matching; so that, necessary parameters
and/or constraints, as well as systems state can be estimated. The systems states and necessary parameters and/or constraints are sent to the back-end of the SLAM algorithm where the systems state is
refined and returned based on the parameters and/or constraints.


<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/overview_of_slam_framework.png}
  \caption{Overview of SLAM Framework.}
  \label{fig:overview_of_slam_framework}
\end{figure}
-->

ROS includes several 2-D SLAM packages. These packages are used to build accurate 2-D occupancy grid maps, seen in Figure <!--\ref{fig:example_of_2d_cccupancy_grid_map_produced}-->. These maps are then used to localize the robot within its environment,
as well as to plan and execute appropriate trajectories in order to reach its destination.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/2d_occupancy_grid_map.png}
  \caption{Example of 2-D Occupancy Grid Map Produced.}
  \label{fig:example_of_2d_cccupancy_grid_map_produced}
\end{figure}
-->

Five of the most common 2-D SLAM ROS packages include HectorSLAM, GMapping, KartoSLAM, CoreSLAM, and LagoSLAM. HectorSLAM [#HectorSLAM] is neither a filtering nor an optimization-based smoothing techniques and
relies solely on robust scan matching [#Kohlbrecher2011]. CoreSLAM [#CoreSLAM] and GMapping [#GMapping] are filtering SLAM techniques. Both CoreSLAM and GMapping utilize a particle filter. While CoreSLAM employs a very simple particle
filter [#Steux2010], GMapping uses a more complex and efficient Rao-Blackwellized particle filter [#Grisetti2007]. Both KartoSLAM [#KartoSLAM] and LagoSLAM [#LagoSLAM] are optimization-based smoothing SLAM techniques. However, KartoSLAM, which
was developed by SRI robotics, uses a highly-optimized and non-iterative Cholesky matrix decomposition for sparse linear systems, known as Sparse Pose Adjustment (SPA) [#Konolige2010], [#Vincent2010], while LagoSLAM uses LAGO optimizer developed by
Carlone et al [#Carlone2011].

Figure <!--\ref{fig:real_world_performance_analysis_of_ros_available_slam_algorithms}--> and Table <!--\ref{table:real_world_error_estimation_for_ros_available_slam_algorithms}--> show the aforementioned 2-D SLAM ROS packages real world performance based on testing done by Santos et al [#Santos2013]. KartoSLAM achieved the smallest error demonstrating the robustness of
its sparse pose adjustment (SPA) and that of full SLAM techniques in general. As a result, KartoSLAM was used to generate Figure _ as well as the maps used during testing of the multistage localization
approach presented in this paper.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/real_world_performance_analysis_ros_slam_algorithms.png}
  \caption{Real World Performance Analysis of ROS Available SLAM Algorithms. Adapted from \protect\cite{Santos2013}.}
  \label{fig:real_world_performance_analysis_of_ros_available_slam_algorithms}
\end{figure}
-->

<!--
\begin{table}[H]
\centering
\caption{Real World Error Estimation for ROS Available SLAM Algorithm. Adapted from \protect\cite{Santos2013}.}
\label{table:real_world_error_estimation_for_ros_available_slam_algorithms}
\resizebox{0.75\linewidth}{!}{\begin{tabular}{@{}|c|c|c|c|c|@{}}
\toprule
\multicolumn{5}{|c|}{Real World Experiments}            \\ \midrule
HectorSLAM & GMapping & KartoSLAM & CoreSLAM & LagoSLAM \\ \midrule
1.1972     & 2.1716   & 1.0318    & 14.75333 & 3.0264   \\ \midrule
0.5094     & 0.6945   & 0.3742    & 7.9463   & 0.8181   \\ \midrule
1.0656     & 1.6354   & 0.9080    & 7.5824   & 2.5236   \\ \bottomrule
\end{tabular}}
\end{table}
-->

## Localization and Path Planning Concepts

ROS’s Navigation Stack [#Navigation] is a collection of packages, which uses odometry and laser scan data, as well as a goal position and orientation in
order to output the velocity commands needed to reach the specified goal. Figure <!--\ref{fig:ros_navigation_stack_setup}--> shows an overview of how the individual packages work
together to achieve this objective. The Map_Server node loads a previously generated two-dimensional grid map. Once the AMCL node receives the map,
odometry, and laser scan data, it is able to localize the robot within the provided map, using the Adaptive Monte Carlo Localization technique
for which it gets its name. The Move_Base node maintains both global and local planners and costmaps. Information about obstacles in the world
are stores in these costmap. The global costmap is used for long-term planning, while the local costmap is used for short-term planning and obstacle
avoidance. The global planner computes an optimal path to the goal given the starting state of robot and the global costmap. While the local planner
computes shorter trajectories given the current state of the robot and the local costmap. Once a path is developed, Move_Base outputs the necessary
velocity commands needed in order to reach the specified destination [#Navigation].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/ros_navigation_stack_setup.png}
  \caption{ROS Navigation Stack setup \protect\cite{Navigation}.}
  \label{fig:ros_navigation_stack_setup}
\end{figure}
-->

After building a two-dimensional occupancy grid map, shown in Figure <!--\ref{fig:example_of_2d_cccupancy_grid_map_produced}-->, using SLAM, it
becomes crucial to accurately localize the robot within this predefined map; so that, the robot can both plan and execute appropriate trajectories
to reach its destination. Localization involves estimating the position and orientation of the robot, known collectively as pose, while the robot
moves throughout its environment. One routine localization technique used involves tracking the robot from an initial known starting pose. Through
the measurement of wheel rotation and the integration of accelerations provided by an inertial measurement unit (IMU), the distance traveled by the
robot from the initial position can be calculated and the robot’s pose in the map estimated with some certainty. However, these methods do not account
for wheel slippage or measurement error. As a result, the accuracy of the pose estimate will degrade over time. Consequently, a solution which can
compensate for the accumulated odometry error and inaccuracies in the initial starting pose is needed. One accepted solution to this problem is Monte
Carlo Localization (MCL), which utilizes a particle filters to keep track of the robot’s pose. However, additional options include Kalman Filters and
Markov Localization, which employ Gaussian distributions and histograms respectively.

Figure <!--\ref{fig:one_dimensional_monte_carlo_localization_example}--> depicts MCL using a one-dimensional corridor with a few doors. The robot
initially has no information about where it is in this corridor. As a result, the graph of the robot’s belief states, which defines the probability
of the robot being at a particular position, is drawn from a uniform distribution of discretely sampled positions along the corridor. A measurement
update is performed at each time step. A measurement update involves convolving the measurement model, the probability of receiving a specific sensor
measurement in the corridor, with the belief states to get an updated belief state. The updated belief state is the same as the pervious belief state;
however, the weight of each particle have been updated based on the sensor reading. At step k=1 the robot senses a door; so, the weight of particles at
the three door are increased. At the next step, a motion model update is performed. The odometry indicated that the robot moved forward a specified
distance d. As a result, the belief state is updated by moving the particles forward that specific distance with noise added to account for the
aforementioned odometry errors. It should be noted that the particles at this stage in Figure <!--\ref{fig:one_dimensional_monte_carlo_localization_example}-->
were also resampled, which will be covered in the following paragraphs. The motion model update is followed by a measurement update. The robot again senses a
door. As a result, the measurement model is the same as the pervious time step. After convolving the current measurement model with the current belief state,
the cumulative probability mass is centered at door two indicating that the robot is likely at this location [#Thrun2005].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/particle_filter_1d_example.png}
  \caption{One-Dimensional Monte Carlo Localization Example. Adapted from \protect\cite{Thrun2005}.}
  \label{fig:one_dimensional_monte_carlo_localization_example}
\end{figure}
-->

Two-dimensional MCL follows the same format of a motion model and then measurement update. The initial particles are drawn from the current odometry
with added noise. At each step, the particles are updated via the odometry and then corrected via a measurement update. For each particle, the correlation
between the two-dimensional occupancy grid map, seen in Figure <!--\ref{fig:example_of_2d_cccupancy_grid_map_produced}-->, and laser scan is calculated using
Equation <!--\ref{eq:weight_each_particle}-->, where $A$ is the predefined map, $B$ is the map created by the current laser scan, and $\bar{A}$ and $\bar{B}$ are the mean
values of the pixels of both maps respectively. Note that while obstacle pixels are black and have a value of 1, free space pixels are grey and have a value of 0. While $m$ and $n$
are the x and y values of the pixel. The particle (pose) with the highest correlations score is chosen as the pose for the current step. The weight of each
particles at step $k$ is found by multiplying the particles weight at step $k-1$ by its normalized correlation score at time step $k$, as seen in Equation <!--\ref{eq:laser_scan_correlation}-->.


<!--
\begin{equation}
s = \frac{\sum_{m}\sum_{n}(A_{mn} - \bar{A})(B_{mn} - \bar{B})}{\sqrt{(\sum_{m}\sum_{n}(A_{mn} - \bar{A})^2)(\sum_{m}\sum_{n}(B_{mn} - \bar{B})^2)}}
\label{eq:weight_each_particle}
\end{equation}
-->

<!--
\begin{equation}
W_k \leftarrow W_{k-1}s
\label{eq:laser_scan_correlation}
\end{equation}
-->

As weights are multiplied over steps, the particles with consecutive small correlation scores are reduced to very small values. As a result, the particles
filter eventually has every few effective particles to ensure that good results are produced. Consequently, re-sampling, show in Figure <!--\ref{fig:particle_filter_resampling_example}-->,
is performed when the number of effective particles become too small. This is done by drawing samples close to the particles that have a higher weights,
indicated by their size. Thus the new sample have a higher density near the position where the particle with higher weight existed. Each particle after
resampling has same weight. As a result, the particle filter begins from scratch.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/amcl_resampling.png}
  \caption{Particle Filter Resampling Example}
  \label{fig:particle_filter_resampling_example}
\end{figure}
-->

Due to the computational complexity inherent in iteratively having to calculate the correlation score for each particle, an optimization technique known as
*KLD-sampling* is used. *KLD-sampling*, derived from *Kullback-Leibler divergence*, is a technique that determine the number of particles needed such that
the error between the sample and true posterior is less than $\epsilon$ [#Thrun2005]. KLD-sampling basically control the number of particles based on the difference
in odometry and particle base location.

Figure <!--\ref{fig:visual_of_amcl_in_rviz}--> shows how KLD-sampling effectively work. Initially when the position is unknown, the particle cloud is large due to the uncertainty in the position
and orientation of the robot. However, as the robot moves, the particle converges and the particle cloud size reduces as KLD-sampling removes the redundant
particles and improves computational performance.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/amcl_visual_in_rviz.png}
  \caption{Visual of AMCL in RVIZ}
  \label{fig:visual_of_amcl_in_rviz}
\end{figure}
-->

After calculating the local and global costmaps, as well as the location of the robot within the given map, the robot must now both plan and execute appropriate trajectories to its destination.
Two frequent techniques used are the Dynamic Window Approach (DWA) and Trajectory Rollout. Both sample the space of feasible controls. For a differential drive robot, such as Clearpath Robotics’
Husky and Fetch Robotics’ Fetch, this controls space is 2D and consists of translations and rotational velocities, $\dot{x}, \dot{\theta}$, which are limited by the robot’s capabilities. Each
sampled velocity is forward simulated from the robot’s current for a short period of time in order to generate simulated trajectories as shown in Figure <!--\ref{fig:trajectory_rollout_path_planning_framework}-->.
These simulated trajectories are then scored using the cost function in Equation <!--\ref{eq:trajectory_rollout_cost_function}-->.

<!--
\begin{equation}
C(\textit{k}) = \alpha{} Obs + \beta{} Gdist +\gamma{} Pdist + \delta{} \frac{1}{\dot{x}^2}
\label{eq:trajectory_rollout_cost_function}
\end{equation}
-->

Where *Obs* is the sum of grid cell cost through which the trajectory passes (taking account of the robot's actual footprint in the grid); *Gdist* and *Pdist* are the estimated shortest distance from
the endpoint of the trajectory to the goal and the optimal path, respectively; and $\dot{x}$ is the translation component of the velocity command that produces the trajectory.

The simulated trajectory that minimizes this cost function is chosen. As a result, chosen trajectories tend to keep obstacles at a distance, proceed towards the goal, remain near the optimal path, as well
as have higher velocities [#Gerkey2008].  DWA and Trajectory Rollout differ in that Trajectory Rollout samples achievable velocities over the entire forward simulation, while DWA sample only from achievable
velocities for just one simulation step [#BaseLocalPlanner].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/trajectory_rollout_depiction.png}
  \caption{Trajectory Rollout Path Planning Framework.}
  \label{fig:trajectory_rollout_path_planning_framework}
\end{figure}
-->

## Manipulation Concepts

ROS’s MoveIt! [MoveIt!] is a collection of packages for mobile manipulation, which incorporate the latest advances in motion planning, manipulation, 3-D perception, kinematics, control, and navigation.
Figure <!--\ref{fig:moveit!'s_system_architecture}--> shows Moveit’s overall system architecture. The move_group node integrates all individual packages together in order to provide the user a set of
ROS actions and services. The user interface allows the user to interact with the move_group node through the ROS interface by using C++, Python, or RVIZ [#RVIZ], which is ROS’s graphical user interface
and 3-D visualization tool. ROS’s Parameter Server [#ParamServer] provides the move_group node the robot’s URDF and Semantic Robot Description Format (SRDF) files, as well as Moveit! specific configuration
files. The SRDF is used to represent information about the robot that is not included in the URDF file, such as a set of links or joints, known collectively as a group, that make up the manipulator,
predefined group states, and a lists of links between which collision checking should be disabled [#SRDF]. Moveit! configuration files include ones that set joint limits, as well as kinematics, motion
planning and perception parameters. The SRDF, as well as necessary Moveit! configuration files are setup through Moveit!’s Setup Assistant, which is Moveit!’s graphical user interface for configuring any
robot for use with Moveit!. The robot interface allows Moveit! to send command to and receive feedback from the robot. Moveit!’s Planning Scene Monitor, which monitors information from the robot’s sensors,
is used to maintain a planning scene, which represent the world around the robot and stores the state of the robot itself. This information is used by Moveit!’s Flexible Collision Library, as well as the
motion and kinematic planning plugins to plan and execute obstacle free paths for the manipulator [#MoveItConcepts].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/moveit_system_architecture.png}
  \caption{Moveit!'s System Architecture. Adapted from \protect\cite{MoveItConcepts}}
  \label{fig:moveit!'s_system_architecture}
\end{figure}
-->

Moveit! allows the user to plan trajectories either in joint space, forward kinematic (FK), or Cartesian space, inverse kinematics (IK). Forward kinematics uses the manipulator’s kinematic equations to compute
the positon of the end-effector given specific predetermined joint angles [#FK]. Whereas, inverse kinematics uses the manipulator’s kinematic equation to determine the joint angles necessary for the end-effector
to arrive at the desired position [#IK]. Figure <!--\ref{fig:forward_and_inverse_kinematics_example}--> illustrated the difference between forward and inverse kinematics. In most cases, including the multistage
localization approach presented in this paper, the target joint angles are not known ahead of time. As a result, a target pose is specified for the manipulator’s end-effector in Cartesian space and Moveit!’s IK
solver will determine the appropriate joint angles to reach the desired pose [#Goebel2015].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/forward_and_inverse_kinematics_depiction.png}
  \caption{Forward and Inverse Kinematics Example}
  \label{fig:forward_and_inverse_kinematics_example}
\end{figure}
-->

There are two main approaches for solving IK problems: numerical and analytical (close form) solutions. Due to the recursive nature of numerical solvers, they tend to be slower than that of analytical solvers
and are prone to get trapped in local minima [#Asadi2015]. However, analytical solvers require that a separate kinematic solver be created for each manipulator and manipulator configuration. In order to ensure
cross platform functionality with minimum overhead, only available numerical kinematic solvers were examined. However, OpenRAVE (Open Robotics Automation Virtual Environment ) [#Diankov2008], which is software
for simulating and deploying planning algorithm, does provided a tool called IKFast [#IKFast], which allows users to generate custom analytical kinematic solver, which can produce solution on the order of
approximately four microseconds. Table <!--\ref{table:comparison_between_ros_available_moveit!_inverse_kinematic_plugins}--> shows three common ROS Moveit! numerical kinematic solvers implementations, Orocos
KDL [#KDL], as well as the TRACLab’s [#TRACLab] KDL-RR and TRAC_IK [#TracIK]. Orocos Kinematics and Dynamics Library (KDL) is a joint-limit-constrained pseudoinverse Jacobian solver, whose convergence algorithms
are based on Newton’s method. However, KDL suffers from the following issues, which results in the high failure rates and slow average speed seen in Table
<!--\ref{table:comparison_between_ros_available_moveit!_inverse_kinematic_plugins}-->.

1. Frequent convergence failures due to manipulators with joint limits
2. No action is taken when the search gets trapped in a local minima
3. Lack of tolerance support and/or utilization in the solver itself

The TRACLab’s KDL-RR kinematic solver implementation attempts to solve issues two and three by altering Orocos’s KDL implementation to detect and “untick” the iterative convergence algorithms, as well as to loop
for a maximum time versus a maximum number of iteration. KDL-RR is able to detect local minima by monitoring when the difference between joint angles across iterations become approximately zero. When this situation
occurs, random seed angles are introduced, which “unstick” the convergence algorithms. In addition, KDL-RR loops for maximum time versus maximum iteration due to the fact that the maximum number of iteration is an
arbitrary number, while maximum time can be computed based on the size and complexity of the each manipulator. In addition, using maximum time makes comparing implementations easier. Table
<!--\ref{table:comparison_between_ros_available_moveit!_inverse_kinematic_plugins}--> shows that KDL-RR had a dramatic increase in solve rate percentage, as well as a modest decrease in runtime compared to KDL.
However, KDL-RR’s doesn’t solve issue one. Consequently, the failure rate is still unacceptably high. TRACLab TRAC-IK kinematic solver attempts to solve issue one by using two separate IK solvers concurrently.
It uses KLD-RR, as well as a sequential quadratic programming (SQP) IK implementation, which is an iterative algorithm for nonlinear optimization. The SQP implementation, which incorporates the same local minimum
detection, is able to deal with issue one. However, SQP can have a much longer solve rate than KDL or KDL-RR. As a result, TRAC-IK implement both of these IK method concurrently and waiting for either to converge.
As a result, the solve rate range between 99.1% and 99.9 % and the average convergence times are well below one millisecond [#Beeson2015].


<!--
\begin{table}[H]
\centering
\caption{Comparison Between ROS Available Moveit! Inverse Kinematic Plugins. Adapted from \protect\cite{Beeson2015}.}
\label{table:comparison_between_ros_available_moveit!_inverse_kinematic_plugins}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}|c|c|c|c|c|c|c|c|@{}}
\toprule
\multicolumn{2}{|c|}{Kinematics Chain}                     & \multicolumn{6}{c|}{IK Technique}                                                                                                                                                                                                 \\ \midrule
\multirow{2}{*}{Robot}             & \multirow{2}{*}{DOFs} & \multicolumn{2}{c|}{Orocos' KDL}                                          & \multicolumn{2}{c|}{KDL-RR}                                               & \multicolumn{2}{c|}{TRAC-IK}                                              \\ \cmidrule(l){3-8}
                                   &                       & \multicolumn{1}{l|}{Solve Rate (\%)} & \multicolumn{1}{l|}{Avg Time (ms)} & \multicolumn{1}{l|}{Solve Rate (\%)} & \multicolumn{1}{l|}{Avg Time (ms)} & \multicolumn{1}{l|}{Solve Rate (\%)} & \multicolumn{1}{l|}{Avg Time (ms)} \\ \midrule
Atlas 2013 Arm                     & 6                     & 75.54                                & 1.35                               & 97.13                                & 0.39                               & 99.97                                & 0.33                               \\ \midrule
Atlas 2015 Arm                     & 7                     & 75.71                                & 1.50                               & 93.13                                & 0.81                               & 99.18                                & 0.48                               \\ \midrule
Baxter Arm                         & 7                     & 61.07                                & 2.21                               & 89.52                                & 1.02                               & 99.17                                & 0.60                               \\ \midrule
Denso VS-068                       & 6                     & 27.92                                & 3.69                               & 98.13                                & 0.42                               & 99.78                                & 0.38                               \\ \midrule
Fanuc M-430iA/2F                   & 5                     & 21.07                                & 3.99                               & 88.34                                & 0.92                               & 99.16                                & 0.58                               \\ \midrule
Fetch Arm                          & 7                     & 92.49                                & 0.73                               & 93.82                                & 0.72                               & 99.96                                & 0.44                               \\ \midrule
Jaco2                              & 6                     & 26.23                                & 3.79                               & 97.66                                & 0.58                               & 99.51                                & 0.58                               \\ \midrule
LBR IIWA 14 R820                   & 7                     & 37.71                                & 3.37                               & 94.02                                & 0.73                               & 99.63                                & 0.56                               \\ \midrule
KUKA LWR 4+                        & 7                     & 67.80                                & 1.88                               & 95.40                                & 0.62                               & 99.95                                & 0.38                               \\ \midrule
PR2 Arm                            & 7                     & 83.14                                & 1.37                               & 86.96                                & 1.27                               & 99.84                                & 0.59                               \\ \midrule
NASA Robonaut2 'Grasping Leg'      & 7                     & 61.27                                & 2.29                               & 87.57                                & 1.10                               & 99.31                                & 0.67                               \\ \midrule
NASA Robonaut2 'Leg + Waist + Arm' & 15                    & 97.99                                & 0.80                               & 98.00                                & 0.84                               & 99.86                                & 0.79                               \\ \midrule
NASA Robonaut2 Arm                 & 7                     & 86.28                                & 1.02                               & 94.26                                & 0.73                               & 99.25                                & 0.50                               \\ \midrule
NASA Robosimian Arm                & 7                     & 61.74                                & 2.44                               & 99.87                                & 0.36                               & 99.93                                & 0.44                               \\ \midrule
TRACLabs Modular Arm               & 7                     & 79.11                                & 1.35                               & 95.12                                & 0.63                               & 99.80                                & 0.53                               \\ \midrule
UR10                               & 6                     & 36.16                                & 3.29                               & 88.05                                & 0.82                               & 99.47                                & 0.49                               \\ \midrule
UR5                                & 6                     & 35.88                                & 3.30                               & 88.69                                & 0.78                               & 99.55                                & 0.42                               \\ \midrule
NASA Valkyrie Arm                  & 7                     & 45.18                                & 3.01                               & 90.05                                & 1.29                               & 99.63                                & 0.61                               \\ \bottomrule
\end{tabular}}
\end{table}
-->

## Task Execution Concepts

It is relatively straightforward to program a robot to execute an individual action. However, programming a fully autonomous robot that will be expected to select which individual action it will
perform depending upon the task at hand and the current conditions isn’t so straight forward. The task execution system must be able to rank tasks by priority, break down tasks into subtasks,
execute tasks in parallel, monitor conditions and react according, as well as pause and resume tasks at a later if necessary [#Goebel2015]. A commonly used construct to achieve the aforementioned
requirements is a hierarchical state machine.

Finite state machines are used to model control and sequences in a system [#Gomaa2016]. In a state machine, the robot occupies one state, which has set behaviors or tasks associated with it. As
long as the robot remains in that states, it will continue to carry out the same behavior or tasks. States are connected together by transitions. When certain predefined conditions are met, a
transition is triggered and the system changes from its current state to the target state. State machines are a powerful tool; however, it can be difficult to express some behaviors, such as
“alarm behaviors”. Image an autonomous industrial mobile manipulator (AIMM) that is responsible for drilling specific patterns on different sections of plane wings. This can be easily implemented
using a normal state machine. The AIMM will need to navigate to the specific starting location of each wing, determine the appropriate drill locations based on the associated pattern, and finally
carry out the individual drilling operations. This can be easily implemented using a normal state machine. Unfortunately, the AIMM’s battery does not provide power indefinitely. When the AIMM’s
power level drops to a certain level, it will need to stop and navigate to the nearest charging location to be recharged, regardless of what it is doing at the time. When it is fully recharged,
it will need to pick up exactly where it left off. The recharging periods is an alarm mechanics: something that interrupts normal behavior to respond to something important. Representing this in
a state machine leads to a doubling the number of states. With only one level of “alarm behaviors’, this is no problem; however, if we add levels of “alarm behaviors”, the number of states will
increase exponentially. So, rather than combining all the logic into a single state machine, the logic can be separated it into several. Each alarm mechanism should have its own state machine,
along with the original behavior. These individual state machines are arrange into a hierarchy, so the next state machine down is only considered when the higher level state machine is not responding
to its alarm. The nesting of states machines inside another makes what is called a hierarchical state machine [#Millington2016].

ROS’s SMACH [#SMACH] package includes a standalone python library for programming hierarchical state machines, as well as a ROS wrapper for integrating the library with ROS topics, services, and actions.
Figure <!--\ref{table:comparison_between_ros_available_moveit!_inverse_kinematic_plugins}--> shows a graphical view of a hierarchical state machine set up using SMACH. This hierarchical state machine
simulates a robot performing tasks such as drilling and sealing in a manufacturing environment while having the option to change its tool or recharge if necessary.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/graphical_view_of_state_machine_using_smach.png}
  \caption{Graphical View of State Machine using SMACH}
  \label{fig:graphical_view_of_state_machine_using_smach}
\end{figure}
-->

## Camera Concepts

An image is the optical representation of an object illuminated by a radiating source. Figure <!--\ref{fig:photmetric_image_formation}--> shows a simplified model of how photometric images are formed.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/photometric_image_formation.png}
  \caption{Photmetric Image Formation. Adapted from \protect\cite{Szeliski2010}.}
  \label{fig:photmetric_image_formation}
\end{figure}
-->

Visible light is emitted by one or more sources, which is then reflected off an object’s surface. This reflected light, is the optical image which is the input of a digital image formation systems [#Pitas2000],
such as a digital camera, depicted in Figure <!--\ref{fig:digital_camera_diagram}-->.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/digital_camera_diagram.png}
  \caption{Digital Camera Diagram. Adapted from \protect\cite{DigitalCamera}.}
  \label{fig:digital_camera_diagram}
\end{figure}
-->

The light passes through a convex lens which focuses all of the rays through the optical center point of the lens and onto a silicon sensor. This sensor, also referred to as the sensor plane, is essentially
a 2-D array, which consists of thousands to millions of solar cells that convert the light into electrons. The accumulated charge of each cell is transmitted to an onboard computer which digitizes this
information; so that, a computer can render the digital image of the world [#HowDigitalCamerasWork]. Since the sensor plane is vertically rotated $180^{\circ}$, due to the principle of the pinhole camera model
[#Hartley2004], the image plane is introduced for mathematically convenience. The image plane, also known as the focal plane, is a virtual plane that is located in front of the optical center of the camera lens
and is equivalent to the sensor plane, in that it represents a projection of the world that is mapped onto a plane. In addition, it accounts for the rotation of the image in the sensor plane.

In order for the multistage localization technique presented in this paper to achieve the desired accuracy, a digital RGB camera with the appropriate sensor size, resolution, focal length, frame rate,
and shutter type must be chosen. The focal length, which is the distance between the optical center point of the camera lens to the image plane [#Jacobs2012], and sensor size of the RGB camera are used
to determine both the angular horizontal and vertical field of view (FOV), which can be calculated using Equations <!--\ref{eq:angular_horizontal_field_of_view}--> and
<!--\ref{eq:angular_vertical_field_of_view}--> respectively,

<!--
\begin{equation}
HFOV_{angular} = 2\tan^{-1}(\frac{w_{s}}{2f})
\label{eq:angular_horizontal_field_of_view}
\end{equation}
-->

<!--
\begin{equation}
VFOV_{angular} = 2\tan^{-1}(\frac{h_{s}}{2f})
\label{eq:angular_vertical_field_of_view}
\end{equation}
-->

where $w_{s}$ and $h_{s}$ are the weight and heights of the sensor, and $f$ is the focal length. These angular FOVs can be converted from angular to distance measurements, using Equations
<!--\ref{eq:distance_horizontal_field_of_view}--> and <!--\ref{eq:distance_vertical_field_of_view}-->,

<!--
\begin{equation}
HFOV_{distance} = 2d\tan(\frac{HFOV_{angular}}{2})
\label{eq:distance_horizontal_field_of_view}
\end{equation}
-->

<!--
\begin{equation}
VFOV_{distance} = 2d\tan(\frac{VFOV_{angular}}{2})
\label{eq:distance_vertical_field_of_view}
\end{equation}
-->

where $d$ is distance from camera to work surface. In addition, Equations <!--\ref{eq:horizontal_ground_sample_distance}--> and <!--\ref{eq:vertical_ground_sample_distance}-->, where
$r_{h}$ and $r_{v}$ is horizontal and vertical resolution respectively, can be used to calculate the horizontal and vertical ground sample distance (GSD). GSD is the distance
between two consecutive pixels centers. Whereas, spatial resolution is the area of each pixels, which is a function of the horizontal and vertical GSD [#Schowengerdt2012]. The ground sample
distance is the highest accuracy a localization system can achieve. Figure <!--\ref{fig:ground_sample_distance_effects_on_image_quality}--> shows the effect that GSD can have on the detail
of an image and consequently the accuracy of a localization system.

<!--
\begin{equation}
HGSD = \frac{w_{s}d}{f*r_{h}}
\label{eq:horizontal_ground_sample_distance}
\end{equation}
-->

<!--
\begin{equation}
VGSD = \frac{h_{s}d}{f*r_{v}}
\label{eq:vertical_ground_sample_distance}
\end{equation}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/spatial_resolution.png}
  \caption{Ground Sample Distance Effects on Image Quality. Adapted from \protect\cite{Quadri}.}
  \label{fig:ground_sample_distance_effects_on_image_quality}
\end{figure}
-->

Equations <!--\ref{eq:distance_horizontal_field_of_view}--> and <!--\ref{eq:vertical_ground_sample_distance}-->, all depend on $d$, the distance from the work surface. Knowing the desired accuracy,
as well as horizontal and vertical FOV of a certain application, an appropriate $d$ can be calculated by rearranging the equations above.

In order for the localization technique to be able to update in real time, even as the systems moves, two additional camera features much be considered, frame rate and shutter type. An appropriate
frame rate is necessary to ensure an adequate image overlap; so that, features points can be tracked using optical flow, which will be discussed in Section 3.7.7, instead of having to redetect the
feature each frame. The required frame rate can be calculated using Equation <!--\ref{eq:required_frame_rate}-->,

<!--
\begin{equation}
FR = \frac{v}{FOV_{distance}(1-o)}
\label{eq:required_frame_rate}
\end{equation}
-->

where $v$ is velocity and $o$ is percent overlap. However, due to the fact that while localizing the system may move in both the horizontal and vertical planes, Equation <!--\ref{eq:required_frame_rate_both_planes}-->,
where $v_{h}$ and $v_{v}$ are horizontal and vertical velocity respectively and $o_{h}$ is horizontal percent overlap, while $o_{v}$ is vertical percent overlap, should be used to ensure an adequate frame rate in both planes [#Smith2016].

<!--
\begin{equation}
FR = \sqrt[]{\frac{v_{h}}{HFOV_{distance}(1-o_{h})}+\frac{v_{v}}{VFOV_{distance}(1-o_{v})}}
\label{eq:required_frame_rate_both_planes}
\end{equation}
-->

There are two types of camera shutters, rolling shutter (RS), in which the horizontal rows of the sensor array are scanned at different time, and global shutter (GS), in which all pixel are exposed at the same time.
Due to the RS not exposing all pixels simultaneously, they are susceptible to motion blur [#Fleet2014] as seen in Figure <!--\ref{fig:shutter_type_effects_on_image_quality}-->. Consequently, in order to ensure
features are not lost between frames as the system move, a digital RGB camera with a GS is needed.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/global_vs_rolling_shutter.png}
  \caption{Shutter Type Effects on Image Quality. Adapted from \protect\cite{RSGSDiff}.}
  \label{fig:shutter_type_effects_on_image_quality}
\end{figure}
-->


## Computer Vision Concepts

The purpose of this subsection is to briefly explain all the computer vision concepts needed to understand the multistage localization approach presented in the subsequent chapters. The following subsections will
explain the basic concepts and rational behind the use of certain color spaces, filters, and binary operations. In addition, Canny Edge Detection, Hough Circle Detection, Good Features to Track, and Optical Flow,
as well as Augmented Reality tag detection and pose estimation will be expounded upon.

### Color Spaces

While the RGB color space is the primary color space used to describe spectral content of color signals, a variety of other representations have been developed
[#Szeliski2010]. Each color space has unique advantages and disadvantages that must be considered before using a specific color space for a given application.
The two color spaces used in the multistage localization approach presented in this paper are the RGB and the HSV color spaces, depicted in Figure
<!--\ref{fig:RGB_and_HSV_color_space_models}-->. The RGB color space is an additive color space based on the RGB color model, in which red, green, and blue
light is added together to produce any color that is the triangle defined by those three primary colors [#RGBColorSpace], [#RGBColorModel]. The HSV color space,
which was developed by Alvy Ray Smith in 1978, is a cylindrical-coordinate representations of points in the RGB color model [#HSVColorSpace]. In some situations,
such as color picking, the HSV model is more intuitive, as it mirror traditional color mixing methods [#Smith1978]. HSV stands for hue, saturation, and value.
Hue represent angular position on the color wheel, where red starts at $0^{\circ}$, green at $120^{\circ}$, and blue at $240^{\circ}$. Saturation, or distance from the center axis, indicates
the amount of grey in a color, where 0 is the color grey and 1 is the primary color. Value, or distance along the center axis, represent the brightness of a color,
where 0 is black and 1 is white [#HSVColorSpace].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/rgb_and_hsv_color_space_models.png}
  \caption{RGB and HSV color space models}
  \label{fig:RGB_and_HSV_color_space_models}
\end{figure}
-->

### Linear and Non-Linear Filters

Insert Text Explaining the Basics behind Linear Filters, Which Ones Were Chosen and Why.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/common_linear_filters.png}
  \caption{Effects of common linear filters.}
  \label{fig:common_linear_filters}
\end{figure}
-->

### Binary Operations

Insert Text Explaining the Basics behind Binary Operations, Which Ones Were Chosen and Why.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/binary_operations.png}
  \caption{Effects of common binary operations.}
  \label{fig:common_binary_operations}
\end{figure}
-->

### Canny Edge Detection

Insert Text Explaining the Basic Intuition Behind Canny Edge Detection.

<!--
\begin{equation}
G_{x} = \begin{bmatrix}
-1 & 0 & +1\\
-2 & 0 & +2\\
-1 & 0 & +1
\end{bmatrix}
\label{eq:sobel_x}
\end{equation}
-->

<!--
\begin{equation}
G_{y} = \begin{bmatrix}
-1 & -2 & -1\\
 0 & 0 & 0\\
+1 & +2 & +1
\end{bmatrix}
\label{eq:sobel_y}
\end{equation}
-->

<!--
\begin{equation}
G = \sqrt{G_{x}^2+G_{y}^2}
\label{eq:edge_gradient}
\end{equation}
-->

<!--
\begin{equation}
\theta = \tan^{-1}(\frac{G_{y}}{G_{x}})
\label{eq:edge_direction}
\end{equation}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/canny_edge_detection.png}
  \caption{Summary of Canny Edge Detection Framework.}
  \label{fig:summary_of_canny_edge_detection_framework}
\end{figure}
-->

### Hough Circle Detection

Insert Text Explaining The Basic Preprocessing Operation Performed by Hough Circle Transform In Order to Get Edges.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/edge_detection_hough_circle.png}
  \caption{Summary of preprocessing operation performed by Hough Circle detector.}
  \label{fig:summary_of_preprocessing_operation_performed_by_Hough_Circle_detector}
\end{figure}
-->


Insert Text Explaining the Basic Intuition Behind Hough Circle with Know Radius.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/hough_circle_summary_known_r.png}
  \caption{Summary of Hough Circle detector with known radius.}
  \label{fig:summary_of_hough_circle_detector_with_known_radius}
\end{figure}
-->

Insert Text Expanding Basic Intuition Behind Hough Circle with Unknown Radius.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/hough_circle_summary_unknown_r.png}
  \caption{Summary of Hough Circle detector with unknown radius.}
  \label{fig:summary_of_hough_circle_detector_with_unknown_radius}
\end{figure}
-->


### Good Feature To Track

Insert Text Explaining the Basic Intuition Behind Good Feature.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/good_feature_basic_intuition.png}
  \caption{Summary of Good Feature detector.}
  \label{fig:summary_of_good_feature_detector}
\end{figure}
-->

Insert Text Explaining an Image Gradient/Derivative and Why We Look for Them.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/image_gradient_example.png}
  \caption{Image Gradient Example.}
  \label{fig:image_gradient_example}
\end{figure}
-->

Insert Text Explaining Math.

<!--
\begin{equation}
E(u,v) = \sum_{x,y}w(x,y)[\textit{I}(x+u,y+v)-\textit{I}(x,y)]^2
\label{eq:change_of_intensity_for_the_shift_uv}
\end{equation}
-->

<!--
\begin{equation}
\sum_{x,y}[\textit{I}(x+u,y+v)-\textit{I}(x,y)]^2
\label{eq:maximize_intensity}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\approx\sum_{x,y}[\textit{I}(x,y)+uI_{x}+vI_{y}-\textit{I}(x,y)]^2
\label{eq:taylor_series_expansion}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\approx\sum_{x,y}u^2I_{x}^2+2uvI_{x}I_{y}+v^2I_{y}^2
\label{eq:expanding_and_cancelling_properly}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\approx\begin{bmatrix}
u & v
\end{bmatrix}\bigg(\sum_{x,y}w(x,y)\begin{bmatrix}
I_{x}^2 & I_{x}I_{y}\\
I_{x}I_{y} & I_{y}^2
\end{bmatrix} \bigg)\begin{bmatrix}
u\\
v
\end{bmatrix}
\label{eq:express_in_matrix_form}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\cong\begin{bmatrix}
u & v
\end{bmatrix}M\begin{bmatrix}
u\\
v
\end{bmatrix}
\label{eq:simplify_matrix}
\end{equation}
-->

Where $$M=\sum_{x,y}w(x,y)\begin{bmatrix}
I_{x}^2 & I_{x}I_{y}\\
I_{x}I_{y} & I_{y}^2
\end{bmatrix}$$

Insert Text Explaining How Both Harris Corner and Good Feature Scoring Functions Work and Why Good Feature Performs Slightly Higher.

<!--
\begin{equation}
R = \textup{det}(M) - k(\textup{trace}(M))^2
\label{eq:harris_corner_scoring_function}
\end{equation}
-->

Where $\textup{det}(M) = \lambda_{1}\lambda_{2}$ and $\textup{trace}(M) = \lambda_{1} + \lambda_{2}$. So,

<!--
\begin{equation}
R = \lambda_{1}\lambda_{2} - k(\lambda_{1}+\lambda_{2})^2
\label{eq:simplified_harris_corner_scoring_function}
\end{equation}
-->

<!--
\begin{equation}
R = \textup{min}(\lambda_{1},\lambda_{2})
\label{eq:good_feature_scoring_function}
\end{equation}
-->


<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/good_feature_harris_scoring_comparison.png}
  \caption{Difference Between Good Feature and Harris Corner Scoring Functions.}
  \label{fig:difference_between_good_feature_and_harris_corner_scoring_functions}
\end{figure}
-->

### Optical Flow

Insert Text Explaining the Basic Intuition Behind Optical Flow.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/optical_flow_example.png}
  \caption{Example of optical flow.}
  \label{fig:example_of_optical_flow}
\end{figure}
-->

Insert Text Explaining the Basic Intuition Behind Kanade-Lucas-Tomasi Feature Tracker.

<!--
\begin{equation}
\sum_{x} [\textit{T}(\textbf{W}(\textbf{x;}\Delta\textbf{p}))-\textit{I}(\textbf{W(x;p}))]^2
\label{eq:klt1}
\end{equation}
-->

<!--
\begin{equation}
\sum_{x} \bigg[\textit{T}(\textbf{W}(\textbf{x;0}))+\nabla \textbf{T}\frac{\partial\textbf{W}}{\partial\textbf{p}}\Delta\textbf{p}-\textit{I}(\textbf{W}(\textbf{x;p}))\bigg]^2
\label{eq:klt2}
\end{equation}
-->

<!--
\begin{equation}
\Delta\textbf{p}=\textit{H}^{-1}\sum_{x} \bigg[\nabla \textbf{T}\frac{\partial\textbf{W}}{\partial\textbf{p}} \bigg]^T[\textit{I}(\textbf{W}(\textbf{x;p}))-\textit{T}(\textbf{x})]
\label{eq:klt3}
\end{equation}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/klt_optimization.png}
  \caption{Summary of Kanade-Lucas-Tomasi feature tracker.}
  \label{fig:summary_of_Kanade-Lucas-Tomasi_feature_tracker}
\end{figure}
-->

### Pose Estimation and Tracking Through Augmented Reality Tag Detection

AR tags, which are also known as fiducial markers, are artificial landmarks which are designed to be easy to detect and identify under most circumstances [#Olson2011]. Consequently, the detection
method much be robust to changes in luminance (brightness), chrominance (color), and resolution, as well as be scale and rotationally invariant. Once detected and identified, AR tags can be used to
accurately calculate the pose (location and orientation) of a camera, as well as to encode information or associate it with each tag’s specific identity [#Sanni2012]. Using AR tag to calculate the
relative pose of a camera in real-time is called marker-based tracking. Several straightforward and robust maker-based tracking toolkits exists, such as ARToolKit [#ARToolKit], ALVAR [#ALVAR], and
AprilTags [#Olson2011]. The multistage localization approach presented in this paper will use a ROS implementation of ALVAR [#ARTrackAlvar] to both calculate the pose of an AR tag in the camera’s
frame and associate information with that tag’s specific identity.

Figure <!--\ref{fig:summary_of_alvar_ar_tag_detection_framework}--> shows ALVAR’s marker-based tracking framework. The system first acquires a greyscale image, either directly or through conversion
from another image format. After which, adaptive thresholding, which can handle local changes in luminance, is used to create a binary image. A binary image consists of a background, black pixels,
and objects, sections of white pixels. Subsequently, the contours of each object are found. Due to the fact that AR systems, like ALVAR, aim for real-time processing and fast performance, time cannot
be wasted processing non-markers. As a result, fast acceptance/rejection tests are run. ALVAR uses two fast accept/reject tests including size and the four-corner test. The number of pixels in an
object’s perimeter can be efficiently used to estimate the object’s area. Objects with areas that are either too large or too small can be rejected. Even if an object, whose area is too small, is a
marker, it is too far from the camera to either be correctly identified or the pose of the camera accurately calculated. Objects, whose area is too large, can be rejected given some background knowledge
about the upper and lower range of the camera from the marker, as well as the marker’s size. The contours of all remaining markers are fitted with lines. A quadrilateral has exactly four straight lines
and four corners. The number of lines and corners of each object are calculated. Those that fail the four-corner test are dropped. After determining that an object is a marker, it is identified and the
corner locations are optimized to sub-pixel accuracy for further calculations. Even small errors in the detected 2-D locations of edges and corners can significantly affect the calculated pose of the
camera [#Sanni2012]. The pose of a calibrated camera can be uniquely determined from a minimum of four coplanar but non-collinear points [#Hartley2004]. Thus, the system can calculated a marker’s pose (relative to camera)
in 3-D coordinates using the four corner points of the marker in image coordinates.


<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/ar_track_alvar_framework.png}
  \caption{Summary of ALVAR AR Tag Detection Framework. Adapted from \protect\cite{Sanni2012}.}
  \label{fig:summary_of_alvar_ar_tag_detection_framework}
\end{figure}
-->

## Summary

Insert Text Summarizing This Chapter and Transiting to the Next.

# Multistage Localization for High Precision Mobile Manipulation Tasks

Insert Text Outlining the Upcoming Sections.

## Approach Overview

Insert Text Outlining the Steps of the Approach.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/multistage_localization_approach.png}
  \caption{Multistage Localization Approach Overview.}
  \label{fig:multistage_localization_approach_overview}
\end{figure}
-->

## Global Map Creation and Task Location Specification

The purpose of this section is reiterate why KartoSLAM was chosen, as well as describe how maps are created, the map files themselves, and how global locations are specified.

Before the manipulator can localize itself with respect to the part, the system must first navigate to the general vicinity in which the work will take place. In order to achieve this, the system utilizes odometry data, given by wheel encoders and an onboard inertial measurement unit (IMU), as well as sensor data, such as laser scans from a LIDAR or point clouds from an RGB-D sensor, to output safe velocity commands that will be sent to the mobile base of the system.

First, a Simultaneous Localization and Mapping (SLAM) technique named KartoSLAM uses the system’s odometry and laser scan or point cloud data, to create a 2-D map of the environment in which the operation(s) will take place. After which, the global location of specific operation(s) are defined, as shown in Figure <!--\ref{fig:global_map}-->.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/2d_map_with_specified_start_locations.png}
  \caption{2-D map environment with specified start locations.}
  \label{fig:global_map}
\end{figure}
-->

## Autonomous Localization and Navigation

Adaptive Monte Carlo localization (amcl) is used to localize the system within the map. Subsequently, odometry data is combined with a global and local cost map, in which obstacles and a specific distance around them represent a cost. These maps are used to plan optimal and obstacle free paths through the environment. The global path is computed before the system begins moving and takes into account all known obstacles, while the local path monitors incoming sensor data to compute suitable linear and angular velocities for the system to complete the current section of the global path. The local path is typically computed at a rate of 20 Hz; however, this parameter is adjustable given the needs of the system.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/robot_localization_framework.png}
  \caption{Robot Localization Framework.}
  \label{fig:robot_localization_framework}
\end{figure}
-->

## Task Association and A Priori Knowledge

Once arriving in the general vicinity of the task to be accomplished, the system then locates an augmented reality (AR) tag [#Siltanen], which allows it to localize and transform points of interest (POIs) associated with the AR tag into the system’s frame of reference. This general localization framework is presented in Figure <!--\ref{fig:task_association}-->.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/initial_feature_localization_framework.png}
  \caption{Initial Feature Localization Framework.}
  \label{fig:initial_feature_localization_framework}
\end{figure}
-->

## Generic Framework for Multi-Stage Computer Vision Algorithm

Insert Text Outlining the Upcoming Sections.

### Initial Feature Location Prediction

This subsection will go through AR tags and how they are used to populate point.

### Corrected Feature Locations

Currently the sponsor of this work uses a hole template on the object to be drilled to ensure accuracy within $\pm$ 0.3 mm. Using the predicted hole locations, given by the AR tag, an inverse kinematic solver is used to move the manipulator to the specified Cartesian location. A camera mounted on the manipulator is then used to further correct the positon of the end-effector. Canny Edge Detection, Hough Transforms, as well as the camera’s intrinsic characteristics and a priori knowledge of each hole’s size is used to output an adjusted Cartesian location of the circle on the templet closest to the predicted position [#Alter1992]. If a hole to be drilled is not found or is outside the range of the manipulator, that hole will be added to a list and the customer notified of all such holes after the operation is completed. In addition to the high precision achieved by the above technique, it can also provide a video log of all work done for inspection.

Insert Text Regarding Computer Vision Detection and Tracking System.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth, height=\textheight, keepaspectratio]{figures/feature_detection_and_tracking_pipeline.png}
  \caption{Feature Detection and Tracking Pipeline.}
  \label{fig:feature_detection_and_tracking_pipeline}
\end{figure}
-->

Insert Text Regarding Control Loop.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/manipulator_control_loop.png}
  \caption{Manipulator Correction Control Loop.}
  \label{fig:manipulator_correction_control_loop}
\end{figure}
-->

## Approach Implementation

Outline the upcoming sections.

### System Overview

The generic framework of the system is depicted in Figure <!--\ref{fig:general_system_overview}-->. The system allows the user to input predefined tasks. Given a priori knowledge of each tasks and their global start locations, the mobile system navigates to and performs the requested operation(s) while monitoring its battery level to ensure mission completion. The framework shown was implemented using a hierarchical state machine and was written in such a way as to make it compatible with the Robot Operating System (ROS) to ensure ease of use across differing robotic platforms.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/general_system_overview.png}
  \caption{General system overview.}
  \label{fig:system_overview}
\end{figure}
-->

### Drilling Framework

This section will go in depth into how the robot simulates drilling.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/drilling_operation_framework.png}
  \caption{Drilling operation framework.}
  \label{fig:drilling_operation_framework}
\end{figure}
-->

### Sealant Application Framework

This section will go in depth into how the robot simulates sealant application.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/sealant_application_framework.png}
  \caption{Sealant Application Framework.}
  \label{fig:sealant_application_framework.}
\end{figure}
-->

## Summary

Insert Text Summarizing This Chapter and Transiting to the Next.

# Experiments and Results

Outline the upcoming sections.

## Hardware Architecture

Insert Text Regarding Robot, Sensors, etc.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/husky_and_fetch_hardware_architecture.png}
  \caption{Clearpath Robotics' Husky and Fetch Robotics' Fetch Mobile Manipulator}
  \label{fig:clearpath_robotics'_husky_and_fetch_robotics'_fetch_mobile_manipulator}
\end{figure}
-->

## Software Architecture

Insert Text Regarding The Software Architecture.

## Experiments

Insert Text Regarding Upcoming Sections.

### Camera Calibration Setup

Insert text setup for camera calibration.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/camera_calibration_setup.png}
  \caption{Camera Calibration Setup.}
  \label{fig:camera_calibration_setup}
\end{figure}
-->

Insert text regarding methods used for camera calibration (citing paper)

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/image_extracted_corners.png}
  \caption{Extracted Corners of Calibration Pattern.}
  \label{fig:extracted_corners_of_calibration_pattern.}
\end{figure}
-->

Insert Text and Math Regarding Calibration of Laser.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/laser_extracted_corners_laser_center.png}
  \caption{Extracted Corners and Laser Center}
  \label{fig:extracted_corners_and_laser_center}
\end{figure}
-->

### Navigation System Experimental Setup

Insert Text Regarding the Navigation System Experimental Setup.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/navigation_system_experimental_setup.png}
  \caption{Navigation System Experimental Setup.}
  \label{fig:navigation_system_experimental_setup}
\end{figure}
-->

### Drilling Operation Experimental Setup

Insert Text Regarding the Drilling Operations Experimental Setup.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/drilling_operation_experimental_setup.png}
  \caption{Drilling Operations Experimental Setup.}
  \label{fig:drilling_operations_experimental_setup}
\end{figure}
-->

### Sealant Application Experimental Setup

Insert Text Regarding the Sealant Application Experimental Setup.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/sealant_application_experimental_setup.png}
  \caption{Sealant Application Experimental Setup.}
  \label{fig:sealant_application_experimental_setup}
\end{figure}
-->

## Results

### Camera Calibration Accuracy Achieved

Insert Text Regarding Pixel Error of Kinect V2.

Insert Text Regarding Pixel Error of Basler ACA150-UC.

Insert Text Regarding Pixel Error of Laser Calibration.

### Navigation System Accuracy Achieved

Insert Text Regarding Accuracy of Navigation System and Ways To Improve Accuracy.

<!--
\begin{table}[H]
\centering
\caption{Global Localization Accuracy Given Distance From Start - Clearpath Robotics' Husky.}
\label{global_localization_accuracy_given_distance_from_start_clearpath_robotics_husky}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|l|l|c|@{}}
\cmidrule(l){2-7}
                           & x (cm) & x std (cm) & y (cm) & y std (cm) & $\psi$ (rad) & $\psi$ std (rad) \\ \midrule
\multicolumn{1}{|c|}{5 m}  & 6.29   & 4.20       & 4.50   & 3.58       & 0.03         & 0.02             \\ \midrule
\multicolumn{1}{|c|}{10 m} & 4.16   & 3.16       & 2.88   & 2.97       & 0.03         & 0.03             \\ \midrule
\multicolumn{1}{|c|}{20 m} & 6.35   & 4.55       & 2.37   & 1.89       & 0.02         & 0.01             \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/husky_navigation_xy_accuracy.png}
  \caption{Global X and Y Localization Accuracy Given Distance From Start - Clearpath Robotics' Husky.}
  \label{fig:global_x_and_y_localization_accuracy_given_distance_from_start_clearpath_robotics_husky}
\end{figure}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/husky_navigation_yaw_accuracy.png}
  \caption{Global $\psi$ Localization Accuracy Given Distance From Start - Clearpath Robotics' Husky.}
  \label{fig:global_psi_localization_accuracy_given_distance_from_start_clearpath_robotics_husky}
\end{figure}
-->

<!--
\begin{table}[H]
\centering
\caption{Global Localization Accuracy Given Distance From Start - Fetch Robotics' Fetch.}
\label{global_localization_accuracy_given_distance_from_start_fetch_robotics_fetch}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|l|l|c|@{}}
\cmidrule(l){2-7}
                           & x (cm) & x std (cm) & y (cm) & y std (cm) & $\psi$ (rad) & $\psi$ std (rad) \\ \midrule
\multicolumn{1}{|c|}{5 m}  & 4.25   & 2.50       & 1.48   & 1.23       & 0.03         & 0.02             \\ \midrule
\multicolumn{1}{|c|}{10 m} & 3.93   & 2.62       & 2.57   & 1.66       & 0.03         & 0.02             \\ \midrule
\multicolumn{1}{|c|}{20 m} & 5.82   & 3.43       & 1.52   & 1.11       & 0.03         & 0.02             \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/fetch_navigation_xy_accuracy.png}
  \caption{Global X and Y Localization Accuracy Given Distance From Start - Fetch Robotics' Fetch.}
  \label{fig:global_x_and_y_localization_accuracy_given_distance_from_start_fetch_robotics_fetch}
\end{figure}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/fetch_navigation_yaw_accuracy.png}
  \caption{Global $\psi$ Localization Accuracy Given Distance From Start - Fetch Robotics' Fetch.}
  \label{fig:global_psi_localization_accuracy_given_distance_from_start_fetch_robotics_fetch}
\end{figure}
-->

### Augmented Reality Tag Accuracy Achieved

<!--
\begin{table}[H]
\centering
\caption{Augmented Reality Accuracy Given Specific Start Conditions - Primesense Carmine 1.09.}
\label{augmented_reality_accuracy_given_specific_start_conditions_primesense_carmine_1_09}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|c|c|c|@{}}
\cmidrule(l){2-7}
                        & x     & x std (mm) & y      & y std (mm) & z      & z std (mm) \\ \midrule
\multicolumn{1}{|c|}{1} & 50.41 & 22.81      & 148.87 & 12.33      & 150.96 & 12.61      \\ \midrule
\multicolumn{1}{|c|}{2} & 23.71 & 16.06      & 151.82 & 6.86       & 117.57 & 12.81      \\ \midrule
\multicolumn{1}{|c|}{3} & 23.06 & 14.85      & 128.39 & 11.48      & 173.24 & 23.24      \\ \midrule
\multicolumn{1}{|c|}{4} & 37.19 & 14.85      & 183.21 & 7.30       & 115.85 & 16.27      \\ \midrule
\multicolumn{1}{|c|}{5} & 24.36 & 17.47      & 138.66 & 6.77       & 42.80  & 16.37      \\ \midrule
\multicolumn{1}{|c|}{6} & 39.16 & 23.52      & 7.27   & 3.70       & 105.24 & 9.16       \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/ar_tag_accuracy_primesense.png}
  \caption{Augmented Reality Accuracy Given Specific Start Conditions - Primesense Carmine 1.09.}
  \label{fig:augmented_reality_accuracy_given_specific_start_conditions_primesense_carmine_1_09}
\end{figure}
-->

<!--
\begin{table}[H]
\centering
\caption{Augmented Reality Accuracy Given Specific Start Conditions - Microsoft Kinect V2.}
\label{augmented_reality_accuracy_given_specific_start_conditions_microsoft_kinect_v2}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|l|l|c|@{}}
\cmidrule(l){2-7}
                             & x (mm) & x std (mm) & y (mm) & y std (mm) & z (mm) & z std (mm) \\ \midrule
\multicolumn{1}{|c|}{Test 1} & 14.32  & 9.75       & 4.04   & 1.65       & 84.77  & 2.50       \\ \midrule
\multicolumn{1}{|c|}{Test 2} & 163.85 & 57.58      & 36.32  & 9.65       & 25.31  & 12.46      \\ \midrule
\multicolumn{1}{|c|}{Test 3} & 84.33  & 44.37      & 50.31  & 5.48       & 55.24  & 8.57       \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/ar_tag_accuracy_kinectv2.png}
  \caption{Augmented Reality Accuracy Given Specific Start Conditions - Microsoft Kinect V2.}
  \label{fig:augmented_reality_accuracy_given_specific_start_conditions_microsoft_kinect_v2}
\end{figure}
-->


### Drilling Operation Accuracy Achieved

Insert Text Regarding Accuracy of Drilling Operations and Ways To Improve Accuracy.

<!--
\begin{table}[H]
\centering
\caption{Drilling Operation Accuracy Given Distance From Work Surface.}
\label{drilling_operation_accuracy_given_distance_from_work_surface}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|c|c|c|@{}}
\cmidrule(l){2-7}
                                                                                                                 & x (mm) & x std (mm) & y (mm) & y std (mm) & z (mm) & z std (mm) \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Less Than 5 mm from\\   Hole Center\end{tabular}}       & 13.25  & 1.43       & 0.70   & 0.43       & 0.82   & 0.56       \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Between 5 and 10 mm\\   from Hole Center\end{tabular}}  & 13.62  & 2.71       & 0.66   & 0.39       & 1.89   & 0.46       \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Between 10 and 20 mm\\   from Hole Center\end{tabular}} & 10.78  & 2.55       & 0.53   & 0.30       & 4.06   & 0.70       \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Between 20 and 30 mm\\   from Hole Center\end{tabular}} & 10.55  & 2.48       & 0.31   & 0.21       & 10.25  & 1.38       \\ \midrule
\multicolumn{1}{|c|}{Mean}                                                                                       & 12.05  & 2.55       & 0.55   & 0.38       & 4.25   & 3.75       \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/drill_accuracy.png}
  \caption{Drilling Operation Accuracy Given Distance From Feature Point.}
  \label{fig:drilling_operation_accuracy_given_distance_from_feature_point}
\end{figure}
-->

### Sealant Application Accuracy Achieved

Insert Text Regarding Accuracy of Sealant Application and Ways To Improve Accuracy.

<!--
\begin{table}[H]
\centering
\caption{Sealant Application Accuracy Given Distance From Work Surface.}
\label{sealant_application_accuracy_given_distance_from_work_surface}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|c|c|c|@{}}
\cmidrule(l){2-7}
                                                                                                            & x (mm) & x std (mm) & y (mm) & y std (mm) & z (mm) & z std (mm) \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Less Than 5 mm from\\   Corner\end{tabular}}       & 10.90  & 2.44       & 1.46   & 0.21       & 1.17   & 0.71       \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Between 5 and 10 mm\\   from Corner\end{tabular}}  & 11.95  & 3.21       & 1.27   & 0.22       & 1.48   & 0.91       \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Between 10 and 20 mm\\   from Corner\end{tabular}} & 16.32  & 2.56       & 0.78   & 0.34       & 3.81   & 0.32       \\ \midrule
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Distance Between 20 and 30 mm\\   from Corner\end{tabular}} & 13.92  & 3.34       & 0.79   & 0.60       & 8.21   & 0.40       \\ \midrule
\multicolumn{1}{|c|}{Mean}                                                                                  & 13.27  & 3.33       & 1.08   & 0.48       & 3.67   & 2.88       \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/seal_accuracy.png}
  \caption{Sealant Application Accuracy Given Distance From Feature Point.}
  \label{fig:sealant_application_accuracy_given_distance_from_feature_point}
\end{figure}
-->

## Summary

Insert Text Summarizing This Chapter and Transiting to the Next.

# Conclusion and Future Work

This paper showed the general framework necessary in order to solve the two main problems preventing the development and widespread adoption of ARC. These problems have caused a decrease in productivity and increase in workplace injuries/fatalities over the past several decades in comparison to other industries [#Rojas2003]. These problems include the fact that typical construction sites tend to be unstructured and are continuously evolving versus the highly controlled environments found in manufacturing. Also, the relationship between the part and manipulator has been reversed, causing increased complexity not seen in manufacturing environments where the part arrives at a fixed manipulator [#Feng2015]. The techniques presented allow systems to create a 2-D map of their environment, localize themselves and complete the task(s) assigned. After localizing an AR tag at the work site, the system is able to use a priori knowledge to localize POIs and complete a plethora of operations achieving an accuracy of approximately $\pm$ 2 mm based on a multifaceted computer vision approach with only a USB webcam.

Future work to be explored includes increasing the accuracy of the computer vision system to the sub-millimeter levels through the use of a machine vision camera, as well as a relatively new calibration technique developed by Feng et al [#Feng2015]. In addition, automated approaches to create 3-D maps are being looked into in order to provide updated data about the robot’s surroundings and task(s) automatically without human intervention. Also, human and robot collaboration over a distributed network is being explored.
