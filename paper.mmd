Base Header Level: 2
latex input: metadata
latex author: Christopher James Mobley
latex title: Multistage Localization for High Precision Mobile Manipulation Tasks
my abstract: Abstract.tex
my public abstract: AbstractPublic.tex
bibtex: masters_thesis
myreporttype: Thesis
mydegree: Masters of Science
mydepartment: Mechanical Engineering
latex input: vtthesis/setup.tex
latex footer: vtthesis/footer.tex

# Introduction

## Background

In order for manufacturing companies to remain competitive, their production systems
need to be able to constantly evolve and to accommodate for the market's changing demands
[#Pedersen2016]. Currently, manufacturing is undergoing a paradigm shift from "mass
production" to "mass customization" [#Michalos2016], [#Cedeno-Campos2013], [#Hvilshoj2012],
[#Bogh2012]. As customers demand more customizable products, traditional "mass production"
becomes untenable. While mass production focuses on producing large volumes of identical
items with high efficiency, mass customization focuses on producing highly variable
products, while maintaining the necessary production volume [#Pedersen2014].

Robots are widely used in the manufacturing industry to perform tasks which are
dumb, dangerous, dull and/or dirty. Consequently, industrial robotics forms an essential
part of the manufacturing backbone [#Hvilshoj2012], [#Boogh2012]. Industrial robots
tend to be used as stationary units, which require significantly more time and capital
to install and integrate than that of other machines due to the multiple aspects
that must be configured. As a result, parts need to be produced continuously at
the maximum rate possible in order to ensure an adequate Return of Investment (ROI).
This contradicts the need for fluctuation of batch sizes due to product variability
and the need to keep total production volume unchanged [#Michalos2016].

Unlike industrial robots, mobile robots have the capability of moving around in
their environments. Thus, if these two types of robots were combined, they would
have the dual advantage of the mobility offered by the mobile platform and the dexterity
offered by the manipulator. Consequently, these robots, which are commonly referred
to as autonomous industrial mobile manipulators (AIMMs), enhance the capability of
traditional industrial robots and mobile robots [#Cheng2013]. AIMMs can be quickly
moved and adapted to varying industrial needs in order to provide drastically new possibilities
to manufacturing industries [#Boogh2012]. Contrary to the traditional stationary
and pre-programmed industrial robots, AIMMs can be used to provide assistance at multiple locations.
They are able to provide highly flexible logistical opportunities and can improve
productivity by providing assistance in time-consuming, dangerous, or straining situations.
This constitutes a very different use case for AIMMs than the one for traditional
fixed industrial robots [#Madsen2015]. Thus, AIMMs could be used to seek an optimum
balance between traditional automation, which can only be reconfigured with great
difficulty, and manual labor, which is very flexible but not economically viable
for large scale production, with the benefits of a compromise between efficiency
and flexibility [#Boogh2012], [#Pedersen2016]. Thereby, creating a transformable
production system, which retains a high degree of automation but also a high degree
of flexibility as seen in Figure <!--\ref{fig:vision_for_future_transformable_production_systems}-->.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/future_transformable_production_system.png}
  \caption{Vision for future transformable production systems. Adapted from \protect\cite{Bischoff2010}.}
  \label{fig:vision_for_future_transformable_production_systems}
\end{figure}
-->

The manufacturing industry is not the only industry that could benefit from the
introduction of AIMMs. Unlike the substantial benefits seen in the manufacturing
industry through the use of fixed industrial robotics, adoption of automation and robotics in construction
(ARC) has lagged far behind [#Balaguer2004]. Consequently, when compared
with other industries, construction has seen a significant decrease in productivity,
as well as an increase in workplace injuries/fatalities over the last several decades
[#Rojas2003]. Several technical complexities inherent in construction have
hindered the development and adoption of field construction robots [#Feng2015].
However, through the capitalization of advances made in other industries, such as manufacturing,
ARC can quickly close this gap. Through the adoption of these advances, dangerous
and/or mundane repetitive tasks to be accomplished autonomously, thus increasing
productivity and decreasing workplace injuries/fatalities [#Balaguer2004]. However, ARC faces
unique challenges when compared to other industries. Unlike manufacturing environments,
which are tightly controlled, typical construction sites tend to lack structure
and are continuously evolving. In addition, the reversal in relationship between
the part and AIMM's manipulator has dramatically increased the complexity of the
problem to be solved [#Feng2015].

## Objectives

The objectives of the thesis are to:

1. Navigate autonomously to specified operation start location while avoiding static and dynamic obstacles.
2. Detect a fiducial marker and populate the specific operation's feature locations in the robot's map frame.
3. Refine feature locations to $\pm$ 1 mm.
4. Simulate drilling and sealant operations.

## Summary of Original Contributions

The original contributions within this thesis are:

1. A multistage localization approach which allows for tasks with an operational
scope outside the range of the robot's manipulator to be completed without having
to recalibrate the position of the end-effector each time the robot's mobile base
moves to another position.
2. Implementation of this multistage localization approach to accomplish two tasks which are
prevalent in both manufacturing and construction industries: drilling and sealant
application operations.

## Outline

Chapter 2 presents where each component of the multistage localization technique
fits into current literature, as well as why specific techniques were chosen. Chapter
3 briefly explains all concepts needed to understand the work presented in the chapters
thereafter including the Robot Operating System (ROS), Simultaneous Localization
and Mapping (SLAM), localization and path planning, manipulation, task execution
using state machines, and the computer vision algorithms implemented. Chapter 4
thoroughly explains each component of the multistage localization approach presented
in this paper and expounds upon the implementation of this approach to accomplish two
tasks which are prevalent in both manufacturing and construction: drilling and sealant
application operations. Chapter 5 validates each stage of the multistage localization
approach and system implementation presented in Chapter 4 through testing on two
different ROS enabled robotic platforms: the Clearpath Robotics' Husky and Fetch
Robotics' Fetch. In addition, it discusses methods of improvement for each stage
of the approach. Chapter 6 summarizes the work presented and proposes future work.


# Literature Review

The purpose of this chapter is to present where each component of the multistage
localization technique fits into current literature. In addition, it will explain
why specific techniques were used over others.

## Localization and Mapping For Autonomous Mobile Manipulators in Manufacturing and Construction

Several different approaches exist to localize a mobile manipulator's base within
a large industrial or unstructured environment. Several current common techniques
used within manufacturing facilities use the triangulation of artificial landmarks
placed at specific locations throughout the facility. Ronzoni et al. and Loevsky
et al. both used a laser sensor and retro-reflective stripes strategically placed
in known locations throughout a facility in order to localize a mobile robot. They were able to achieve
an accuracy of approximately 1.5 cm [#Ronzoni2011], [#Loevsky2010]. Zhang
et al. and Okuyama et al. proposed placing fiducial 2-D marker on the ceiling, which
can be read by the robot's onboard camera system in order to determine it position to
within approximately 6 cm [#Zhang2015], [#Okuyama2011]. Another form of triangulation
proposed is through the triangulation of radio frequencies, such as RFID and Wifi
Localization. DiGiampaolo et al. used a Kalman filter to estimate the location of
the robot using the robot's odometry and RFID tags located on the ceiling.
This approach achieves an accuracy of approximately 4 cm [#Digiampaolo2014]. Park
et al. used a particle filter to estimate the location of the robot using the robot's
odometry and RFID tags embedded in the floor. This approach was able to achieve an accuracy
of approximately 2.5 cm [#Park2013]. Youssef presented the Horus WLAN localization, a
RSSI-based fingerprinting scheme, while Gao presented a WLAN fingerprinting technique
dubbed DeepFi which used Channel State Information (CSI) and deep learning. Horus was
able to achieve an accuracy of approximately 1.55 m, while DeepFi was able to achieve
an accuracy of .95 m [#Youssef2008], [#Gao2015]. Other popular methods include
following magnetic or chemical tape or an inductive wire [#Shneier2015]. However,
many of the methods mentioned require making significant modifications to the manufacturing facility.
In order to limit the modification necessary, several simultaneous and localization
(SLAM) techniques exist that allow a mobile robot to create and/or update a map
of a facility while localizing itself within that environment. Kelly demonstrated
the utility of applying SLAM to large manufacturing facilities in 2000 [#Kelly2000].
In recent years, optimization based smoothing SLAM techniques
have proved more efficient, scalable, and robust than that of filtering SLAM techniques
while still retaining a relatively high degree of accuracy, less than 8 cm [#Latif2013].
Due to the fact that SLAM techniques require no modification of the environment and
are able to either build or modify a 2-D or 3-D map of the environment, while retaining
a high level of accuracy, they are the building block for the multistage localization
presented in this paper. Specifically, a 2-D map will be made using an optimization
based smoothing SLAM technique while localization within this map will be accomplished
via adaptive Monte Carlo localization (AMCL).

## Task Association and *A Priori* Knowledge for Mobile Manipulators

Fiducial makers have a wide variety of uses due to their tri-ability of being easy to
detect under most circumstances, able to be precisely localized, and the ability
to either associate data with a specific fiducial maker's identity or encode data.
As mentioned above, Zhang et al. and Okuyama et al. placed fiducial 2-D markers
on the ceiling, which were then read by the robot's onboard camera system in order
to determine it's position to within approximately 6 cm [#Zhang2015], [#Okuyama2011].
In order to allow social robots to permeate our daily lives, Limosani et al. proposed
the use of a set of environmental tags, composed of AR Tags and QR codes, which
encoded the specific address to a cloud service from which a mobile robot could download
a map of the local areas and all other necessary information [#Limosani2016]. Feng
et al. used April Tags to autonomously identify building components, such as block
or brick; so that, a manipulator could be used to pick up specific ones and assemble
them into a pre-designed modular structure [#Feng2015]. In addition, they have been
used in order to calibrate a mobile robot to workstations as in [#Andersen2013].
In addition to using a fiducial maker to calibrate a mobile robot to a workstation,
they can also be used to specify feature locations at that workstation as they are
in this paper.

## Feature Localization Techniques for Mobile Manipulators

Depending on the complexity and required tolerances of a task, a mobile robot may
or may not need to calibrate themselves with respect to the work surface. Simple
pick and place operations, as seen in [#Cheng2013a] do not require the robot to calibrate
itself with respect to the work surface. Through the use of feedback from a camera
or torque sensor, the robot is able to perform its purpose without calibration. However,
many operations in manufacturing require high tolerances. Consequently, the mobile
manipulator must calibrate itself with respect to the environment; so that, the
robot implicitly knows where features, such as buttons or knobs, are located without
having to explicitly detect these features with high tolerances [#Pedersen2016].
Several methods exist to do this. Haptic calibration involves measuring location
on the work surface in the x, y, and z directions by moving the end-effector until
it comes in contact with prominent points on the work surface itself. This calibration
can be done in approximately 30 seconds and has an accuracy of $\pm$ 1 mm [#Pedersen2011].
Visual high-precision calibration involves taking multiple images of a calibration
plate, which is fixed to the work surface, by a camera on the end-effector. This
calibration can be done in approximately 60 seconds and has an accuracy of $\pm$
0.1 mm [#Hvilshj2010]. Visual high-speed calibration involves taking a single image
of a calibration plate, which is fixed to the work surface, as well as three depth
measurements by a camera and laser sensor respectively. This calibration can be done
in approximately 10 seconds and has an accuracy of $\pm$ 1 mm [#Hvilshj2010]. QR
calibration involves capturing a RBG and depth image of a QR code, which is fixed
to a work surface, using an RGB-D camera, whose location is known precisely in relation
to the end-effector. This calibration method can be completed in approximately 1
second and has an accuracy of $\pm$ 4 mm [#Andersen2013]. While these calibration
techniques allow for high tolerances, these calibrations are done with respect to
the end-effector. Therefore, if the reach of the manipulator is insufficient to
complete the entire operation, such as drilling and sealant application operations
in particular, the robot must move and then recalibrate the manipulator with respect
to the work surface. Consequently, calibration plates or fiducial markers must be
placed at set distance along a project, which requires significant overhead.
This paper proposes a multistage localization approach, which localizes the mobile
robot within a map, uses a fiducial marker, which can easily be replaced with a
calibration plate, to localize an operation's features in the map frame verses
with respect to the end-effector. This significantly degrades the localization of
the features. However, through the addition of robust feature detection, millimeter
to sub-millimeter accuracy will be maintained while allowing for mobility of the mobile
base.

## Summary

The purpose of this chapter is to present where each component of the multistage
localization technique fits into current literature, as well as why specific techniques
were used over others. The following chapter will present in depth all of the concepts
needed to understand the multistage localization approach; so that, anyone
can understand the work presented regardless of background.

# Fundamentals of Autonomous Robotics

The purpose of this chapter is to briefly explain all of the concepts needed to understand
the work presented in the chapters thereafter. The following subsections will explain
the basic concepts of the Robot Operating System (ROS), Simultaneous Localization
and Mapping (SLAM), Localization and Path Planning, as well as Manipulation. In addition,
Task Execution using State Machines and the specific Computer Vision Algorithms
implemented will be expounded upon.

## ROS Concepts

The Robot Operating System (ROS) [#Quigley2009] is an open-source set of software
libraries and tools that aim to simplify the task of creating robotics applications that can
be used across a wide variety of platforms. ROS was originally developed in 2007
by Willow Garage as an extension of Switchyard, a collection of robotics software
developed by the Stanford Artificial Intelligence Laboratory in support of the Stanford
AI Robot (STAIR) and Personal Robotics (PR) projects. The first distribution of
ROS, Box Turtle, was released in 2010. Currently, ROS has had ten major releases.
The most current being Kinetic Kame Turtle, which was released on 23MAY16. In addition,
ROS boasts tens of thousands of users around the world ranging from hobbyists and
researchers to the commercial and industrial industries, as well as hundreds of packages
which provide everything from hardware drivers to algorithms for autonomous navigation
and manipulation [#ROSWebsite]. ROS's large support base acts as a force multiplier, which
allows individuals, labs, and company to concentrate on one particular aspect while
capitalizing on work that has already been done.


### ROS Communication

ROS uses a name server, called the ROS Master, to maintain a list of nodes and available
topics. Nodes communicate with the master server using the XML-RPC protocol, while
peer-to-peer communications generally use TCP/IP sockets through the TCPROS protocol.
Figure <!--\ref{fig:simple_ros_node_flowchart}--> shows a simple diagram of two ROS nodes
communicating with messages and service topics. In addition to the concepts of messages
and services, ROS also employs actions. Actions are similar to service calls, but are
designed for long duration tasks that are capable of providing feedback. These communication
interfaces provide ROS a great deal of flexibility for robotic applications [#ROSConcepts], [#Burton2015].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/simple_ros_node_diagram.png}
  \caption{Simple ROS Node Flowchart. Adapted from \protect\cite{Burton2015}.}
  \label{fig:simple_ros_node_flowchart}
\end{figure}
-->

### Rigid Body Transformations

Figure <!--\ref{fig:conversion_from_coordinate_frame_a_to_b}--> shows a purple dot,
which represents a point in space. The dot's coordinates in frames a and b are different. A rigid body
transform, which can be performed using Equation <!--\ref{eq:conversion_from_coordinate_frame_a_to_b}-->,
can be used to convert one set of coordinates to another coordinate frame.

<!--
\begin{equation}
x_a=T_{b}^{a}x_b
\label{eq:conversion_from_coordinate_frame_a_to_b}
\end{equation}
-->

$T_{b}^{a}$ is equal to Equation <!--\ref{eq:simplified_transformation_matrix}-->.

<!--
\begin{equation}
\begin{bmatrix}
R_{b}^{a} & t_{b}^{a}\\
0^T & 1
\end{bmatrix}
\label{eq:simplified_transformation_matrix}
\end{equation}
-->

$R_{b}^{a}$ is the rotation matrix, which performs the rotation part of transforming
frame $b$ into alignment with frame $a$, while $t_{b}^{a}$ is the translation matrix, which performs
the translation part of transforming frame $b$ origin to frame $a$ [#Kelly2013].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/conversion_from_frame_a_b.png}
  \caption{Conversion from Coordinate Frame A to B.}
  \label{fig:conversion_from_coordinate_frame_a_to_b}
\end{figure}
-->

A robotic system, such as the Clearpath Robotics' Husky and Fetch Robotics' Fetch shown
in Figure <!--\ref{fig:robot_model_with_tfs}-->, typically has many three dimensional
coordinate frames that change over time as the robot performs different functions.
ROS's TF [#TF] and TF2 [#TF2] packages keep track of coordinate frames and allow
for data to be easily converted between these coordinate frames using rigid body transforms.
These coordinate frames, as well as the robot's links/joints they correspond to and
the rigid transformations between them are set up in the Robot's Unified Robot Description
Format (URDF) File [#URDF].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/robots_with_without_tfs.png}
  \caption{Robot Model with TFs.}
  \label{fig:robot_model_with_tfs}
\end{figure}
-->

## Simultaneous Localization and Mapping Concepts

In order for a mobile robot to operate and perform intricate tasks in a complex,
GPS-denied environment, such as that of manufacturing facility or construction site, without modifying
said environment, the robot must be able to create an accurate map of its environment
while simultaneously localizing itself within this map using only on-board sensors.
Simultaneous localization and mapping (SLAM) is the problem of building and/or updating
a map of an unknown environment while simultaneously localizing the robot within
this map [#OpenSLAM]. SLAM was pioneered in the early 1990s by Hugh F. Durrant-Whyte
and John J. Leonard [#Leonard1991], who based their work on research done by Smith
and P. Cheeseman in the mid to late-1980s [#Smith1986], [#Smith1990].

Several techniques exist to solve the SLAM problem. Most of these techniques can
be categorized into two main paradigms: filtering and optimization-based smoothing
[#Grisetti2010a], [#Latif2013]. Filtering techniques model the SLAM problem as an
incremental state estimation, where the state of the system is composed of the robot's
current pose and the map. These estimates are refined at each step by incorporating
current sensor measurements. Due to their incremental nature, these SLAM techniques
are typically referred to as on-line SLAM approaches. Popular filtering SLAM techniques
include the extended Kalman filter, particle filter, and information filters. Filtering
SLAM techniques have been used widely in past years due to their ability to model
different sources of noise and their effect on sensor measurements. However, in
recent years optimization-based smoothing techniques have proven to be more efficient,
scalable, and robust than that of filtering techniques [#Latif2013]. Unlike filtering
techniques, optimization-based smoothing techniques estimate the robot's entire
trajectory and the map. Due to the fact that the final map is based off the robot's
entire trajectory and world features instead of the most recent pose and map, these
techniques are known as full SLAM approaches. These SLAM techniques incorporate
a graph-based structure, where graph nodes represent the robot's pose and world
features, while edges represent a spatial constraint relation between two robot
poses given by sensor measurements [#Konolige2010]. The graph is optimized using
error minimization techniques, such as least-squares, in order to refine the robot's
trajectory and map.

In addition to a variety of techniques which can be used to solve the SLAM problem,
a wide range of sensors can also be used. Typically, sensors used include LIDARs,
stereo cameras, monocular camera, and RGB-D sensors.

Figure <!--\ref{fig:overview_of_slam_framework}--> shows the generic framework for
solving the SLAM problem. In the front-end, raw sensor inputs are processed in order
to extract features and perform scan matching; so that, necessary parameters and/or constraints,
as well as the system's state can be estimated. The system's state and necessary parameters
and/or constraints are sent to the back-end of the SLAM algorithm where the system's
state is refined and returned based on the parameters and/or constraints.


<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/overview_of_slam_framework.png}
  \caption{Overview of SLAM Framework.}
  \label{fig:overview_of_slam_framework}
\end{figure}
-->

ROS includes several 2-D SLAM packages. These packages are used to build accurate
2-D occupancy grid maps, an example of which is shown in Figure <!--\ref{fig:example_of_2d_cccupancy_grid_map_produced}-->.
These maps are then used to localize the robot within its environment, as well as
to plan and execute appropriate trajectories in order for the robot to reach its destination.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/2d_occupancy_grid_map.png}
  \caption{Example of 2-D Occupancy Grid Map Produced.}
  \label{fig:example_of_2d_cccupancy_grid_map_produced}
\end{figure}
-->

Five of the most common 2-D SLAM ROS packages are HectorSLAM, GMapping, KartoSLAM,
CoreSLAM, and LagoSLAM. HectorSLAM [#HectorSLAM] is neither a filtering nor an optimization-based
smoothing technique, but rather relies solely on robust scan matching [#Kohlbrecher2011].
CoreSLAM [#CoreSLAM] and GMapping [#GMapping] are filtering SLAM techniques. Both
CoreSLAM and GMapping utilize a particle filter. While CoreSLAM employs a very simple
particle filter [#Steux2010], GMapping uses a more complex and efficient Rao-Blackwellized
particle filter [#Grisetti2007]. Both KartoSLAM [#KartoSLAM] and LagoSLAM [#LagoSLAM]
are optimization-based smoothing SLAM techniques. However, KartoSLAM, which was
developed by SRI robotics, uses a highly-optimized and non-iterative Cholesky matrix
decomposition for sparse linear systems, known as Sparse Pose Adjustment (SPA) [#Konolige2010],
[#Vincent2010], while LagoSLAM uses LAGO optimizer developed by Carlone et al. [#Carlone2011].

Figure <!--\ref{fig:real_world_performance_analysis_of_ros_available_slam_algorithms}-->
and Table <!--\ref{table:real_world_error_estimation_for_ros_available_slam_algorithms}-->
show the aforementioned 2-D SLAM ROS packages' real world performance based on testing
done by Santos et al. [#Santos2013]. KartoSLAM achieved the smallest error demonstrating
the robustness of its sparse pose adjustment (SPA) and that of full SLAM techniques
in general. As a result, KartoSLAM was used to generate Figure <!--\ref{fig:example_of_2d_cccupancy_grid_map_produced}-->
as well as the maps used during testing of the multistage localization approach
presented in this paper.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/real_world_performance_analysis_ros_slam_algorithms.png}
  \caption{Real World Performance Analysis of ROS Available SLAM Algorithms. Adapted from \protect\cite{Santos2013}.}
  \label{fig:real_world_performance_analysis_of_ros_available_slam_algorithms}
\end{figure}
-->

<!--
\begin{table}[H]
\centering
\caption{Real World Error Estimation for ROS Available SLAM Algorithm. Adapted from \protect\cite{Santos2013}.}
\label{table:real_world_error_estimation_for_ros_available_slam_algorithms}
\resizebox{0.75\linewidth}{!}{\begin{tabular}{@{}|c|c|c|c|c|@{}}
\toprule
\multicolumn{5}{|c|}{Real World Experiments}            \\ \midrule
HectorSLAM & GMapping & KartoSLAM & CoreSLAM & LagoSLAM \\ \midrule
1.1972     & 2.1716   & 1.0318    & 14.75333 & 3.0264   \\ \midrule
0.5094     & 0.6945   & 0.3742    & 7.9463   & 0.8181   \\ \midrule
1.0656     & 1.6354   & 0.9080    & 7.5824   & 2.5236   \\ \bottomrule
\end{tabular}}
\end{table}
-->

## Localization and Path Planning Concepts

ROS's Navigation Stack [#Navigation] is a collection of packages, which uses odometry
and laser scan data, as well as a goal position and orientation in order to output
the velocity commands needed to reach the specified goal. Figure <!--\ref{fig:ros_navigation_stack_setup}-->
shows an overview of how the individual packages work together to achieve this objective.
The Map_Server node [#MapServer] loads a previously generated two-dimensional grid map.
Once the AMCL [#AMCL] node receives the map, odometry, and laser scan data, it is
able to localize the robot within the provided map, using the Adaptive Monte Carlo
Localization technique from which it gets its name. The Move_Base [#MoveBase] node
maintains both global and local planners and costmaps. Information about obstacles
in the world are stored in these costmaps. The global costmap is used for long-term
planning, while the local costmap is used for short-term planning and obstacle avoidance.
The global planner computes an optimal path to the goal given the starting state
of the robot and the global costmap, while the local planner computes shorter trajectories
given the current state of the robot and the local costmap. Once a path is developed,
the Move_Base node outputs the necessary velocity commands needed to reach the specified
destination [#Navigation].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/ros_navigation_stack_setup.png}
  \caption{ROS Navigation Stack setup \protect\cite{Navigation}.}
  \label{fig:ros_navigation_stack_setup}
\end{figure}
-->

After using SLAM to build a two-dimensional occupancy grid map, shown in Figure <!--\ref{fig:example_of_2d_cccupancy_grid_map_produced}-->,
it becomes crucial to accurately localize the robot within this predefined
map; so that, the robot can both plan and execute appropriate trajectories to reach
its destination. Localization involves estimating the position and orientation of
the robot, known collectively as pose, while the robot moves throughout its environment.
One routine localization technique used involves tracking the robot from an initial
known starting pose. Through the measurement of wheel rotation and the integration
of accelerations provided by an inertial measurement unit (IMU), the distance traveled
by the robot from the initial position can be calculated and the robot's pose in
the map estimated with some certainty. However, these methods do not account for
wheel slippage or measurement error. As a result, the accuracy of the pose estimate
will degrade over time. Consequently, a solution which can compensate for the accumulated
odometry error and inaccuracies in the initial starting pose is needed. One accepted
solution to this problem is Monte Carlo Localization (MCL), which utilizes a particle
filter to keep track of the robot's pose. However, additional options include Kalman
Filters and Markov Localization, which employ Gaussian distributions and histograms
respectively.

Figure <!--\ref{fig:one_dimensional_monte_carlo_localization_example}--> depicts
MCL using a one-dimensional corridor with a few doors. The robot initially has no
information about where it is in this corridor. As a result, the graph of the robot's
belief states, which defines the probability of the robot being at a particular
position, is drawn from a uniform distribution of discretely sampled positions along
the corridor. A measurement update is performed at each step. A measurement
update involves convolving the measurement model, the probability of receiving a
specific sensor measurement in the corridor, with the belief states to get an updated
belief state. The updated belief state is the same as the pervious belief state;
however, the weight of each particle have been updated based on the sensor reading.
At step $k=1$ the robot senses a door; so, the weights of the particles around each of the three door
are increased. At the next step, a motion model update is performed. The odometry
indicated that the robot moved forward a specified distance $d$. As a result, the
belief state is updated by moving the particles forward by distance $d$ with
noise added to account for the aforementioned odometry errors. It should be noted
that the particles at this stage in Figure <!--\ref{fig:one_dimensional_monte_carlo_localization_example}-->
were also resampled, which will be covered in the following paragraphs. The motion
model update is followed by a measurement update. The robot again senses a door.
As a result, the measurement model is the same as the pervious time step. After
convolving the current measurement model with the current belief state, the cumulative
probability mass is centered at door two indicating that the robot is likely at
this location [#Thrun2005].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/particle_filter_1d_example.png}
  \caption{One-Dimensional Monte Carlo Localization Example. Adapted from \protect\cite{Thrun2005}.}
  \label{fig:one_dimensional_monte_carlo_localization_example}
\end{figure}
-->

Two-dimensional MCL follows the same format of a motion model and then measurement
update. The initial particles are drawn from the current odometry with added noise.
At each step, the particles are updated via the odometry and then corrected via
a measurement update. For each particle, the correlation between the two-dimensional
occupancy grid map, seen in Figure <!--\ref{fig:example_of_2d_cccupancy_grid_map_produced}-->,
and laser scan is calculated using Equation <!--\ref{eq:weight_each_particle}-->,
where $A$ is the predefined map, $B$ is the map created by the current laser scan,
and $\bar{A}$ and $\bar{B}$ are the mean values of the pixels of both maps respectively.
Note that while obstacle pixels are black and have a value of 1, free space pixels
are grey and have a value of 0. The x and y values of each pixel are represented by $m$ and $n$.
The particle (pose) with the highest correlations score is chosen as the pose for
the current step. The weight of each particle at step $k$ is found by multiplying
the particle's weight at step $k-1$ by its normalized correlation score at step
$k$, as seen in Equation <!--\ref{eq:laser_scan_correlation}-->.


<!--
\begin{equation}
s = \frac{\sum_{m}\sum_{n}(A_{mn} - \bar{A})(B_{mn} - \bar{B})}{\sqrt{(\sum_{m}\sum_{n}(A_{mn} - \bar{A})^2)(\sum_{m}\sum_{n}(B_{mn} - \bar{B})^2)}}
\label{eq:weight_each_particle}
\end{equation}
-->

<!--
\begin{equation}
W_k \leftarrow W_{k-1}s
\label{eq:laser_scan_correlation}
\end{equation}
-->

As weights are multiplied over steps, the particles with consecutive small correlation
scores are reduced to very small weight values. As a result, the particle filter eventually
has very few effective particles to ensure that good results are produced. Consequently,
re-sampling, shown in Figure <!--\ref{fig:particle_filter_resampling_example}-->,
is performed when the number of effective particles becomes too small. This is done
by drawing samples close to the particles that have higher weights, indicated
by their size. Thus the new sample has a higher density near the positions where
the particles with higher weights existed. Each particle after resampling has the same
weight. As a result, the particle filter begins from scratch.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/amcl_resampling.png}
  \caption{Particle Filter Resampling Example}
  \label{fig:particle_filter_resampling_example}
\end{figure}
-->

Due to the computational complexity inherent in iteratively calculating
the correlation score for each particle, an optimization technique known as *KLD-sampling*
is used. *KLD-sampling*, derived from *Kullback-Leibler divergence*, is a technique
that determines the number of particles needed such that the error between the sample
and true posterior is less than $\epsilon$ [#Thrun2005]. KLD-sampling basically
controls the number of particles based on the difference in odometry and particle
base location.

Figure <!--\ref{fig:visual_of_amcl_in_rviz}--> shows how KLD-sampling
works. Initially when the position is unknown, the particle cloud is large due to
the uncertainty in the position and orientation of the robot. However, as the robot
moves, the particles converge and the particle cloud size reduces as KLD-sampling
removes the redundant particles and improves computational performance.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/amcl_visual_in_rviz.png}
  \caption{Visual of AMCL in RVIZ}
  \label{fig:visual_of_amcl_in_rviz}
\end{figure}
-->

After calculating the local and global costmaps, as well as the location of the
robot within the given map, the robot must now both plan and execute appropriate
trajectories to its destination. Two frequently used techniques are the Dynamic Window
Approach (DWA) and Trajectory Rollout. Both sample the space of feasible controls.
For a differential drive robot, such as the Clearpath Robotics' Husky and Fetch Robotics'
Fetch, this controls space is 2D and consists of translational and rotational velocities,
$\dot{x}, \dot{\theta}$, which are limited by the robot's capabilities. Each sampled
velocity is forward simulated from the robot's current position for a short period
of time in order to generate simulated trajectories as shown in Figure <!--\ref{fig:trajectory_rollout_path_planning_framework}-->.
These simulated trajectories are then scored using the cost function in Equation
<!--\ref{eq:trajectory_rollout_cost_function}-->,

<!--
\begin{equation}
C(\textit{k}) = \alpha{} Obs + \beta{} Gdist +\gamma{} Pdist + \delta{} \frac{1}{\dot{x}^2}
\label{eq:trajectory_rollout_cost_function}
\end{equation}
-->

where *Obs* is the sum of grid cell cost through which the trajectory passes (taking
into account the robot's actual footprint in the grid); *Gdist* and *Pdist* are the
estimated shortest distance from the endpoint of the trajectory to the goal and
the optimal path, respectively; and $\dot{x}$ is the translation component of the
velocity command that produces the trajectory.

The simulated trajectory that minimizes this cost function is chosen. As a result,
chosen trajectories tend to keep obstacles at a distance, proceed towards the goal,
remain near the optimal path, and have higher velocities [#Gerkey2008].
DWA and Trajectory Rollout differ in that Trajectory Rollout samples achievable
velocities over the entire forward simulation, while DWA samples only from achievable
velocities for just one simulation step [#BaseLocalPlanner].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/trajectory_rollout_depiction.png}
  \caption{Trajectory Rollout Path Planning Framework.}
  \label{fig:trajectory_rollout_path_planning_framework}
\end{figure}
-->

## Manipulation Concepts

ROS's MoveIt! [#MoveIt] is a collection of packages for mobile manipulation, which
incorporate the latest advances in motion planning, manipulation, 3-D perception,
kinematics, control, and navigation. Figure <!--\ref{fig:moveit!'s_system_architecture}-->
shows Moveit's overall system architecture. The move_group node integrates all individual
packages together in order to provide the user a set of ROS actions and services.
The user interface allows the user to interact with the move_group node through
the ROS interface by using C++, Python, or RVIZ [#RVIZ], which is ROS's graphical
user interface and 3-D visualization tool. ROS's Parameter Server [#ParamServer]
provides the move_group node the robot's URDF and Semantic Robot Description Format
(SRDF) files, as well as Moveit!'s specific configuration files. The SRDF is used
to represent information about the robot that is not included in the URDF file,
such as a set of links or joints, known collectively as a group, that make up the
manipulator, predefined group states, and a list of links between which collision
checking should be disabled [#SRDF]. Moveit!'s configuration files
set joint limits, as well as kinematics, motion planning and perception parameters.
The SRDF file and Moveit!'s configuration files are set up through Moveit!'s
Setup Assistant, which is Moveit!'s graphical user interface for configuring any
robot for use with Moveit!. The robot interface allows Moveit! to send commands to
and receive feedback from the robot. Moveit!'s Planning Scene Monitor, which monitors
information from the robot's sensors, is used to maintain a planning scene, which
represents the world around the robot and stores the state of the robot itself. This
information is used by Moveit!'s Flexible Collision Library, as well as the motion
and kinematic planning plugins to plan and execute obstacle free paths for the
manipulator [#MoveItConcepts].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/moveit_system_architecture.png}
  \caption{Moveit!'s System Architecture. Adapted from \protect\cite{MoveItConcepts}}
  \label{fig:moveit!'s_system_architecture}
\end{figure}
-->

Moveit! allows the user to plan trajectories in either joint space, forward kinematic (FK),
or Cartesian space, inverse kinematics (IK). Forward kinematics uses the manipulator's
kinematic equations to compute the positon of the end-effector given specific predetermined
joint angles [#FK]; whereas, inverse kinematics uses the manipulator's kinematic
equation to determine the joint angles necessary for the end-effector to arrive at
the desired position [#IK]. Figure <!--\ref{fig:forward_and_inverse_kinematics_example}-->
illustrates the difference between forward and inverse kinematics. In most cases,
including the multistage localization approach presented in this paper, the target
joint angles are not known ahead of time. As a result, a target pose is specified
for the manipulator's end-effector in Cartesian space and Moveit!'s IK solver will
determine the appropriate joint angles needed in order to reach the desired pose [#Goebel2015].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/forward_and_inverse_kinematics_depiction.png}
  \caption{Forward and Inverse Kinematics Example}
  \label{fig:forward_and_inverse_kinematics_example}
\end{figure}
-->

There are two main approaches for solving IK problems: numerical and analytical
(closed form) solutions. Due to the recursive nature of numerical solvers, they
tend to be slower than analytical solvers and are prone to getting trapped
in local minima [#Asadi2015]. However, analytical solvers require that a separate
kinematic solver be created for each manipulator and manipulator configuration.
In order to ensure cross platform functionality with minimum overhead, only available
numerical kinematic solvers were examined. However, OpenRAVE (Open Robotics Automation
Virtual Environment) [#Diankov2008], which is software for simulating and deploying
planning algorithm, provides a tool called IKFast [#IKFast], which allows users
to generate custom analytical kinematic solvers, which can produce solutions within
approximately four microseconds. Table <!--\ref{table:comparison_between_ros_available_moveit!_inverse_kinematic_plugins}-->
shows three common ROS Moveit! numerical kinematic solver implementations, Orocos
KDL [#KDL], as well as the TRACLab's [#TRACLab] KDL-RR and TRAC_IK [#TracIK]. Orocos
Kinematics and Dynamics Library (KDL) is a joint-limit-constrained pseudoinverse
Jacobian solver, whose convergence algorithms are based on Newton's method. However,
KDL suffers from the following issues, which results in the high failure rates and
slow average speed seen in Table <!--\ref{table:comparison_between_ros_available_moveit!_inverse_kinematic_plugins}-->.

1. Frequent convergence failures due to manipulator joint limits
2. No action is taken when the search gets trapped in a local minima
3. Lack of tolerance support and/or utilization in the solver itself

The TRACLab's KDL-RR kinematic solver implementation attempts to solve issues two
and three by altering Orocos's KDL implementation to detect and "untick" the iterative
convergence algorithms, as well as to loop for a maximum time versus a maximum number
of iterations. KDL-RR is able to detect local minima by monitoring when the difference
between joint angles across iterations becomes approximately zero. When this situation
occurs, random seed angles are introduced, which "unstick" the convergence algorithms.
In addition, KDL-RR loops for a maximum time versus a maximum number of iterations due to the fact
that the maximum number of iterations is an arbitrary number, while the maximum time
can be computed based on the size and complexity of each manipulator. In addition,
using maximum time makes comparing implementations easier. Table <!--\ref{table:comparison_between_ros_available_moveit!_inverse_kinematic_plugins}-->
shows that KDL-RR had a dramatic increase in solve rate percentage, as well as a
modest decrease in runtime compared to KDL. However, KDL-RR doesn't solve issue
one. Consequently, the failure rate is still unacceptably high. TRACLab's TRAC-IK
kinematic solver attempts to solve issue one by using two separate IK solvers concurrently.
It uses KLD-RR, as well as a sequential quadratic programming (SQP) IK implementation,
which is an iterative algorithm for nonlinear optimization. The SQP implementation,
which incorporates the same local minimum detection, is able to deal with issue one.
However, SQP can have a much longer solve rate than KDL or KDL-RR. As a result,
TRAC-IK implements both of these IK method concurrently and waits for either to
converge. As a result, its solve rate ranges between 99.1% and 99.9 % and its average
convergence times are well below one millisecond [#Beeson2015].


<!--
\begin{table}[H]
\centering
\caption{Comparison Between ROS Available Moveit! Inverse Kinematic Plugins. Adapted from \protect\cite{Beeson2015}.}
\label{table:comparison_between_ros_available_moveit!_inverse_kinematic_plugins}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}|c|c|c|c|c|c|c|c|@{}}
\toprule
\multicolumn{2}{|c|}{Kinematics Chain}                     & \multicolumn{6}{c|}{IK Technique}                                                                                                                                                                                                 \\ \midrule
\multirow{2}{*}{Robot}             & \multirow{2}{*}{DOFs} & \multicolumn{2}{c|}{Orocos' KDL}                                          & \multicolumn{2}{c|}{KDL-RR}                                               & \multicolumn{2}{c|}{TRAC-IK}                                              \\ \cmidrule(l){3-8}
                                   &                       & \multicolumn{1}{l|}{Solve Rate (\%)} & \multicolumn{1}{l|}{Avg Time (ms)} & \multicolumn{1}{l|}{Solve Rate (\%)} & \multicolumn{1}{l|}{Avg Time (ms)} & \multicolumn{1}{l|}{Solve Rate (\%)} & \multicolumn{1}{l|}{Avg Time (ms)} \\ \midrule
Atlas 2013 Arm                     & 6                     & 75.54                                & 1.35                               & 97.13                                & 0.39                               & 99.97                                & 0.33                               \\ \midrule
Atlas 2015 Arm                     & 7                     & 75.71                                & 1.50                               & 93.13                                & 0.81                               & 99.18                                & 0.48                               \\ \midrule
Baxter Arm                         & 7                     & 61.07                                & 2.21                               & 89.52                                & 1.02                               & 99.17                                & 0.60                               \\ \midrule
Denso VS-068                       & 6                     & 27.92                                & 3.69                               & 98.13                                & 0.42                               & 99.78                                & 0.38                               \\ \midrule
Fanuc M-430iA/2F                   & 5                     & 21.07                                & 3.99                               & 88.34                                & 0.92                               & 99.16                                & 0.58                               \\ \midrule
Fetch Arm                          & 7                     & 92.49                                & 0.73                               & 93.82                                & 0.72                               & 99.96                                & 0.44                               \\ \midrule
Jaco2                              & 6                     & 26.23                                & 3.79                               & 97.66                                & 0.58                               & 99.51                                & 0.58                               \\ \midrule
LBR IIWA 14 R820                   & 7                     & 37.71                                & 3.37                               & 94.02                                & 0.73                               & 99.63                                & 0.56                               \\ \midrule
KUKA LWR 4+                        & 7                     & 67.80                                & 1.88                               & 95.40                                & 0.62                               & 99.95                                & 0.38                               \\ \midrule
PR2 Arm                            & 7                     & 83.14                                & 1.37                               & 86.96                                & 1.27                               & 99.84                                & 0.59                               \\ \midrule
NASA Robonaut2 'Grasping Leg'      & 7                     & 61.27                                & 2.29                               & 87.57                                & 1.10                               & 99.31                                & 0.67                               \\ \midrule
NASA Robonaut2 'Leg + Waist + Arm' & 15                    & 97.99                                & 0.80                               & 98.00                                & 0.84                               & 99.86                                & 0.79                               \\ \midrule
NASA Robonaut2 Arm                 & 7                     & 86.28                                & 1.02                               & 94.26                                & 0.73                               & 99.25                                & 0.50                               \\ \midrule
NASA Robosimian Arm                & 7                     & 61.74                                & 2.44                               & 99.87                                & 0.36                               & 99.93                                & 0.44                               \\ \midrule
TRACLabs Modular Arm               & 7                     & 79.11                                & 1.35                               & 95.12                                & 0.63                               & 99.80                                & 0.53                               \\ \midrule
UR10                               & 6                     & 36.16                                & 3.29                               & 88.05                                & 0.82                               & 99.47                                & 0.49                               \\ \midrule
UR5                                & 6                     & 35.88                                & 3.30                               & 88.69                                & 0.78                               & 99.55                                & 0.42                               \\ \midrule
NASA Valkyrie Arm                  & 7                     & 45.18                                & 3.01                               & 90.05                                & 1.29                               & 99.63                                & 0.61                               \\ \bottomrule
\end{tabular}}
\end{table}
-->

## Task Execution Concepts

It is relatively straightforward to program a robot to execute an individual action.
However, programming a fully autonomous robot that will be expected to select which
individual action it will perform depending upon the task at hand and the current
conditions isn't so straight forward. The task execution system must be able to
rank tasks by priority, break down tasks into subtasks, execute tasks in parallel,
monitor conditions and react according, as well as pause and resume tasks at a later time
if necessary [#Goebel2015]. A commonly used construct to achieve the aforementioned
requirements is a hierarchical state machine.

Finite state machines are used to model control and sequencing in a system [#Gomaa2016].
In a state machine, the robot occupies one state, which has set behaviors or tasks
associated with it. As long as the robot remains in that state, it will continue
to carry out the same behavior or tasks. States are connected together by transitions.
When certain predefined conditions are met, a transition is triggered and the system
changes from its current state to the target state. State machines are a powerful
tool; however, it can be difficult to express some behaviors, such as "alarm behaviors".
Imagine an autonomous industrial mobile manipulator (AIMM) that is responsible for
drilling specific patterns on different sections of plane wings. This can be easily
implemented using a normal state machine. The AIMM will need to navigate to the
specific starting location of each wing, determine the appropriate drill locations
based on the associated pattern, and finally carry out the individual drilling operations.
This can be easily implemented using a normal state machine. Unfortunately, the AIMM's
battery does not provide power indefinitely. When the AIMM's power level drops to
a certain level, it will need to stop and navigate to the nearest charging location
to be recharged, regardless of what it is doing at the time. When it is fully recharged,
it will need to pick up exactly where it left off. The recharging period is an
alarm mechanics, which is something that interrupts normal behavior to respond to something
important. Representing this in a state machine leads to a doubling the number of
states. With only one level of "alarm behaviors', this is no problem; however, if
we add levels of "alarm behaviors", the number of states will increase exponentially.
So, rather than combining all the logic into a single state machine, the logic can
be separated it into several. Each alarm mechanism should have its own state machine,
along with the original behavior. These individual state machines are arrange into
a hierarchy, so the next state machine down is only considered when the higher level
state machine is not responding to its alarm. The nesting of state machines inside
another makes what is called a hierarchical state machine [#Millington2016].

ROS's SMACH [#SMACH] package includes a standalone python library for programming
hierarchical state machines, as well as a ROS wrapper for integrating the library
with ROS topics, services, and actions. Figure <!--\ref{table:comparison_between_ros_available_moveit!_inverse_kinematic_plugins}-->
shows a graphical view of a hierarchical state machine set up using SMACH. This
hierarchical state machine simulates a robot performing tasks such as drilling and
sealing in a manufacturing environment while having the option to change its tool
or recharge if necessary.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/graphical_view_of_state_machine_using_smach.png}
  \caption{Graphical View of State Machine using SMACH}
  \label{fig:graphical_view_of_state_machine_using_smach}
\end{figure}
-->

## Camera Concepts

An image is the optical representation of an object illuminated by a radiating source.
Figure <!--\ref{fig:photmetric_image_formation}--> shows a simplified model of how
photometric images are formed.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/photometric_image_formation.png}
  \caption{Photmetric Image Formation. Adapted from \protect\cite{Szeliski2010}.}
  \label{fig:photmetric_image_formation}
\end{figure}
-->

Visible light is emitted by one or more sources, which is then reflected off an
object's surface. This reflected light, is the optical image which is the input
of a digital image formation systems [#Pitas2000], such as a digital camera, depicted
in Figure <!--\ref{fig:digital_camera_diagram}-->.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/digital_camera_diagram.png}
  \caption{Digital Camera Diagram. Adapted from \protect\cite{DigitalCamera}.}
  \label{fig:digital_camera_diagram}
\end{figure}
-->

The light passes through a convex lens which focuses all of the rays through the
optical center point of the lens and onto a silicon sensor. This sensor, also referred
to as the sensor plane, is essentially a 2-D array, which consists of thousands
to millions of solar cells that convert the light into electrons. The accumulated
charge of each cell is transmitted to an onboard computer which digitizes this
information; so that, a computer can render the digital image of the world [#HowDigitalCamerasWork].
Since the sensor plane is vertically rotated $180^{\circ}$, due to the principle
of the pinhole camera model [#Hartley2004], the image plane is introduced for mathematically
convenience. The image plane, also known as the focal plane, is a virtual plane
that is located in front of the optical center of the camera lens and is equivalent
to the sensor plane, in that it represents a projection of the world that is mapped
onto a plane. In addition, it accounts for the rotation of the image in the sensor
plane.

In order for the multistage localization technique presented in this paper to achieve
the desired accuracy, a digital RGB camera with the appropriate sensor size, resolution,
focal length, frame rate, and shutter type must be chosen. The focal length, which
is the distance between the optical center point of the camera lens to the image
plane [#Jacobs2012], and sensor size of the RGB camera are used to determine both
the angular horizontal and vertical field of view (FOV), which can be calculated
using Equations <!--\ref{eq:angular_horizontal_field_of_view}--> and <!--\ref{eq:angular_vertical_field_of_view}--> respectively,

<!--
\begin{equation}
HFOV_{angular} = 2\tan^{-1}(\frac{w_{s}}{2f})
\label{eq:angular_horizontal_field_of_view}
\end{equation}
-->

<!--
\begin{equation}
VFOV_{angular} = 2\tan^{-1}(\frac{h_{s}}{2f})
\label{eq:angular_vertical_field_of_view}
\end{equation}
-->

where $w_{s}$ and $h_{s}$ are the weight and heights of the sensor, and $f$ is the
focal length. These angular FOVs can be converted from angular to distance measurements,
using Equations <!--\ref{eq:distance_horizontal_field_of_view}--> and <!--\ref{eq:distance_vertical_field_of_view}-->,

<!--
\begin{equation}
HFOV_{distance} = 2d\tan(\frac{HFOV_{angular}}{2})
\label{eq:distance_horizontal_field_of_view}
\end{equation}
-->

<!--
\begin{equation}
VFOV_{distance} = 2d\tan(\frac{VFOV_{angular}}{2})
\label{eq:distance_vertical_field_of_view}
\end{equation}
-->

where $d$ is distance from camera to work surface. In addition, Equations <!--\ref{eq:horizontal_ground_sample_distance}-->
and <!--\ref{eq:vertical_ground_sample_distance}-->, where $r_{h}$ and $r_{v}$ are the
horizontal and vertical resolution respectively, can be used to calculate the horizontal
and vertical ground sample distance (GSD). GSD is the distance between two consecutive
pixels centers. Whereas spatial resolution is the area of each pixel, which is
a function of the horizontal and vertical GSD [#Schowengerdt2012]. The ground sample
distance is the highest accuracy a localization system can achieve. Figure <!--\ref{fig:ground_sample_distance_effects_on_image_quality}-->
shows the effect that GSD can have on the detail of an image and consequently the
accuracy of a localization system.

<!--
\begin{equation}
HGSD = \frac{w_{s}d}{f*r_{h}}
\label{eq:horizontal_ground_sample_distance}
\end{equation}
-->

<!--
\begin{equation}
VGSD = \frac{h_{s}d}{f*r_{v}}
\label{eq:vertical_ground_sample_distance}
\end{equation}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/spatial_resolution.png}
  \caption{Ground Sample Distance Effects on Image Quality. Adapted from \protect\cite{Quadri}.}
  \label{fig:ground_sample_distance_effects_on_image_quality}
\end{figure}
-->

Equations <!--\ref{eq:distance_horizontal_field_of_view}--> and <!--\ref{eq:vertical_ground_sample_distance}-->,
all depend on $d$, the distance from the work surface. Knowing the desired accuracy,
as well as horizontal and vertical FOV of a certain application, an appropriate $d$
can be calculated by rearranging the equations above.

In order for the localization technique to be able to update in real time, even
as the system moves, two additional camera characteristics much be considered, frame rate
and shutter type. An appropriate frame rate is necessary to ensure an adequate image
overlap; so that, feature points can be tracked using optical flow, which will
be discussed in Section 3.7.7, instead of having to redetect the feature each frame.
The required frame rate can be calculated using Equation <!--\ref{eq:required_frame_rate}-->,

<!--
\begin{equation}
FR = \frac{v}{FOV_{distance}(1-o)}
\label{eq:required_frame_rate}
\end{equation}
-->

where $v$ is velocity and $o$ is percent overlap. However, due to the fact that
while localizing the system may move in both the horizontal and vertical planes,
Equation <!--\ref{eq:required_frame_rate_both_planes}-->, where $v_{h}$ and $v_{v}$
are horizontal and vertical velocity respectively and $o_{h}$ is horizontal percent
overlap, while $o_{v}$ is vertical percent overlap, should be used to ensure an
adequate frame rate in both planes [#Smith2016].

<!--
\begin{equation}
FR = \sqrt[]{\frac{v_{h}}{HFOV_{distance}(1-o_{h})}+\frac{v_{v}}{VFOV_{distance}(1-o_{v})}}
\label{eq:required_frame_rate_both_planes}
\end{equation}
-->

There are two types of camera shutters, a rolling shutter (RS), in which the horizontal
rows of the sensor array are scanned at different times, and a global shutter (GS),
in which all pixels are exposed at the same time. Due to the RS not exposing all
pixels simultaneously, they are susceptible to motion blur [#Fleet2014] as seen
in Figure <!--\ref{fig:shutter_type_effects_on_image_quality}-->. Consequently,
in order to ensure features are not lost between frames as the system moves, a digital
RGB camera with a GS is needed.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/global_vs_rolling_shutter.png}
  \caption{Shutter Type Effects on Image Quality. Adapted from \protect\cite{RSGSDiff}.}
  \label{fig:shutter_type_effects_on_image_quality}
\end{figure}
-->


## Computer Vision Concepts

The purpose of this subsection is to briefly explain all the computer vision concepts
needed to understand the multistage localization approach presented in the subsequent
chapters. The following subsections will explain the basic concepts and rational
behind the use of certain color spaces, filters, and binary operations. In addition,
Canny Edge Detection, Hough Circle Detection, Good Features to Track, and Optical
Flow, as well as Augmented Reality tag detection and pose estimation will be expounded
upon.

### Color Spaces

While the RGB color space is the primary color space used to describe spectral content
of color signals, a variety of other representations have been developed [#Szeliski2010].
Each color space has unique advantages and disadvantages that must be considered
before using a specific color space for a given application. The two color spaces
used in the multistage localization approach presented in this paper are the RGB
and the HSV color spaces, depicted in Figure <!--\ref{fig:RGB_and_HSV_color_space_models}-->.
The RGB color space is an additive color space based on the RGB color model, in
which red, green, and blue light is added together to produce any color that is
the triangle defined by those three primary colors [#RGBColorSpace], [#RGBColorModel].
The HSV color space, which was developed by Alvy Ray Smith in 1978, is a cylindrical-coordinate
representation of points in the RGB color model [#HSVColorSpace]. In some situations,
such as color picking, the HSV model is more intuitive, as it mirrors traditional
color mixing methods [#Smith1978]. HSV stands for hue, saturation, and value. Hue
represent angular position on the color wheel, where red starts at $0^{\circ}$,
green at $120^{\circ}$, and blue at $240^{\circ}$. Saturation, or distance from
the center axis, indicates the amount of grey in a color, where 0 is the color grey
and 1 is the primary color. Value, or distance along the center axis, represents
the brightness of a color, where 0 is black and 1 is white [#HSVColorSpace].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/rgb_and_hsv_color_space_models.png}
  \caption{RGB and HSV color space models}
  \label{fig:RGB_and_HSV_color_space_models}
\end{figure}
-->

### Linear and Non-Linear Filters

Images are often corrupted by random variations in intensity, illumination, as well
poor contrast that must be dealt with in the early stages of image processing
before higher level computer vision techniques, such as feature detection and tracking,
can be reliably used [#Jain1995]. Random variation in the intensity of an image is called noise.
Common types of image noise include salt and pepper, impulse, and Gaussian noise.
While salt and pepper noise is the random occurrence of both black and white pixels,
impulse noise is merely the random occurrence of white pixels. Gaussian noise, on
the other hand, contains variation in intensities drawn from a Gaussian distribution,
which can be used to model many kinds of sensor noise [#Parikh2015]. In most cases,
convolving an image with a linear low-pass filter, a filter which replaces each
pixel value with the weighted sum of all pixel values in its neighborhood, or a
spatially invariant non-linear filter, which does not use a weighted sum but rather
preforms the same operation at each pixel, is able to remove these types of noise.
Figure <!--\ref{fig:common_filters}--> shows that a Gaussian filter is apt at removing
noise drawn from a Gaussian distribution, such as white noise. On the contrary, a median filter,
which replaces each pixel with the median value in its neighborhood, is suitable
for removing salt and pepper, as well as impulse noise. In addition to removing
noise, filters can be used to enhance an image, such as sharpen it as seen in Figure
<!--\ref{fig:common_filters}-->, extract information, like edges and texture, as
well as detect patterns through template matching [#Parikh2015].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/common_linear_filters.png}
  \caption{Effects of common filters.}
  \label{fig:common_filters}
\end{figure}
-->

### Morphological Operations

In addition, to enhancing images, non-linear filters are also used extensively to
process binary images [#Szeliski2010]. A binary image is an image with only two
possible values for each pixel, typically black, background pixels with a value
of 0, and white, foreground pixels with a value of 1. Binary images often result
after operations such as segmentation, thresholding, and dithering [#BinaryImage].
The most common binary image operations are called morphological operations, since
they change the shape of the underlying binary objects [#Szeliski2010]. Whether
or not a given foreground or background pixel value changes depend on the image,
as well as the morphological operations and the structuring element chosen. The
structuring element is a rectangular array of pixels, which contain values of either
1 or 0. When the structuring element is centered on the pixel under consideration,
that pixel's neighborhood is determined by the pixels in the structuring element
that have a value of 1 [#Solomon2011]. The two fundamental morphological operations
are dilation, which thickens foreground pixels, and erosion, which thins foreground
pixels. These two operations can be applied in different combinations in order to
obtain more sophisticated operations, such as closing, dilation then erosion, and
opening, erosion then dilation [#Wilson2000]. The morphological operations, depicted
in Figure <!--\ref{fig:common_binary_operations}-->, are typically used to clean
up binary images; so that, higher level computer vision techniques can be used robustly
[#Szeliski2010].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/binary_operations.png}
  \caption{Effects of common binary operations.}
  \label{fig:common_binary_operations}
\end{figure}
-->

### Canny Edge Detection

Insert Text Explaining the Basic Intuition Behind Canny Edge Detection.

<!--
\begin{equation}
G_{x} = \begin{bmatrix}
-1 & 0 & +1\\
-2 & 0 & +2\\
-1 & 0 & +1
\end{bmatrix}
\label{eq:sobel_x}
\end{equation}
-->

<!--
\begin{equation}
G_{y} = \begin{bmatrix}
-1 & -2 & -1\\
 0 & 0 & 0\\
+1 & +2 & +1
\end{bmatrix}
\label{eq:sobel_y}
\end{equation}
-->

<!--
\begin{equation}
G = \sqrt{G_{x}^2+G_{y}^2}
\label{eq:edge_gradient}
\end{equation}
-->

<!--
\begin{equation}
\theta = \tan^{-1}(\frac{G_{y}}{G_{x}})
\label{eq:edge_direction}
\end{equation}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/canny_edge_detection.png}
  \caption{Summary of Canny Edge Detection Framework.}
  \label{fig:summary_of_canny_edge_detection_framework}
\end{figure}
-->

### Hough Circle Detection

Insert Text Explaining The Basic Preprocessing Operation Performed by Hough Circle Transform In Order to Get Edges.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/edge_detection_hough_circle.png}
  \caption{Summary of preprocessing operation performed by Hough Circle detector.}
  \label{fig:summary_of_preprocessing_operation_performed_by_Hough_Circle_detector}
\end{figure}
-->


Insert Text Explaining the Basic Intuition Behind Hough Circle with Know Radius.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/hough_circle_summary_known_r.png}
  \caption{Summary of Hough Circle detector with known radius.}
  \label{fig:summary_of_hough_circle_detector_with_known_radius}
\end{figure}
-->

Insert Text Expanding Basic Intuition Behind Hough Circle with Unknown Radius.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\linewidth]{figures/hough_circle_summary_unknown_r.png}
  \caption{Summary of Hough Circle detector with unknown radius.}
  \label{fig:summary_of_hough_circle_detector_with_unknown_radius}
\end{figure}
-->


### Good Feature To Track

Insert Text Explaining the Basic Intuition Behind Good Feature.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/good_feature_basic_intuition.png}
  \caption{Summary of Good Feature detector.}
  \label{fig:summary_of_good_feature_detector}
\end{figure}
-->

Insert Text Explaining an Image Gradient/Derivative and Why We Look for Them.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/image_gradient_example.png}
  \caption{Image Gradient Example.}
  \label{fig:image_gradient_example}
\end{figure}
-->

Insert Text Explaining Math.

<!--
\begin{equation}
E(u,v) = \sum_{x,y}w(x,y)[\textit{I}(x+u,y+v)-\textit{I}(x,y)]^2
\label{eq:change_of_intensity_for_the_shift_uv}
\end{equation}
-->

<!--
\begin{equation}
\sum_{x,y}[\textit{I}(x+u,y+v)-\textit{I}(x,y)]^2
\label{eq:maximize_intensity}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\approx\sum_{x,y}[\textit{I}(x,y)+uI_{x}+vI_{y}-\textit{I}(x,y)]^2
\label{eq:taylor_series_expansion}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\approx\sum_{x,y}u^2I_{x}^2+2uvI_{x}I_{y}+v^2I_{y}^2
\label{eq:expanding_and_cancelling_properly}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\approx\begin{bmatrix}
u & v
\end{bmatrix}\bigg(\sum_{x,y}w(x,y)\begin{bmatrix}
I_{x}^2 & I_{x}I_{y}\\
I_{x}I_{y} & I_{y}^2
\end{bmatrix} \bigg)\begin{bmatrix}
u\\
v
\end{bmatrix}
\label{eq:express_in_matrix_form}
\end{equation}
-->

<!--
\begin{equation}
E(u,v)\cong\begin{bmatrix}
u & v
\end{bmatrix}M\begin{bmatrix}
u\\
v
\end{bmatrix}
\label{eq:simplify_matrix}
\end{equation}
-->

Where $$M=\sum_{x,y}w(x,y)\begin{bmatrix}
I_{x}^2 & I_{x}I_{y}\\
I_{x}I_{y} & I_{y}^2
\end{bmatrix}$$

Insert Text Explaining How Both Harris Corner and Good Feature Scoring Functions Work and Why Good Feature Performs Slightly Higher.

<!--
\begin{equation}
R = \textup{det}(M) - k(\textup{trace}(M))^2
\label{eq:harris_corner_scoring_function}
\end{equation}
-->

Where $\textup{det}(M) = \lambda_{1}\lambda_{2}$ and $\textup{trace}(M) = \lambda_{1} + \lambda_{2}$. So,

<!--
\begin{equation}
R = \lambda_{1}\lambda_{2} - k(\lambda_{1}+\lambda_{2})^2
\label{eq:simplified_harris_corner_scoring_function}
\end{equation}
-->

<!--
\begin{equation}
R = \textup{min}(\lambda_{1},\lambda_{2})
\label{eq:good_feature_scoring_function}
\end{equation}
-->


<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/good_feature_harris_scoring_comparison.png}
  \caption{Difference Between Good Feature and Harris Corner Scoring Functions.}
  \label{fig:difference_between_good_feature_and_harris_corner_scoring_functions}
\end{figure}
-->

### Optical Flow

Insert Text Explaining the Basic Intuition Behind Optical Flow.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/optical_flow_example.png}
  \caption{Example of optical flow.}
  \label{fig:example_of_optical_flow}
\end{figure}
-->

Insert Text Explaining the Basic Intuition Behind Kanade-Lucas-Tomasi Feature Tracker.

<!--
\begin{equation}
\sum_{x} [\textit{T}(\textbf{W}(\textbf{x;}\Delta\textbf{p}))-\textit{I}(\textbf{W(x;p}))]^2
\label{eq:klt1}
\end{equation}
-->

<!--
\begin{equation}
\sum_{x} \bigg[\textit{T}(\textbf{W}(\textbf{x;0}))+\nabla \textbf{T}\frac{\partial\textbf{W}}{\partial\textbf{p}}\Delta\textbf{p}-\textit{I}(\textbf{W}(\textbf{x;p}))\bigg]^2
\label{eq:klt2}
\end{equation}
-->

<!--
\begin{equation}
\Delta\textbf{p}=\textit{H}^{-1}\sum_{x} \bigg[\nabla \textbf{T}\frac{\partial\textbf{W}}{\partial\textbf{p}} \bigg]^T[\textit{I}(\textbf{W}(\textbf{x;p}))-\textit{T}(\textbf{x})]
\label{eq:klt3}
\end{equation}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/klt_optimization.png}
  \caption{Summary of Kanade-Lucas-Tomasi feature tracker.}
  \label{fig:summary_of_Kanade-Lucas-Tomasi_feature_tracker}
\end{figure}
-->

### Pose Estimation and Tracking Through Augmented Reality Tag Detection

AR tags, which are also known as fiducial markers, are artificial landmarks which
are designed to be easy to detect and identify under most circumstances [#Olson2011].
Consequently, the detection method must be robust to changes in luminance (brightness),
chrominance (color), and resolution, as well as be scale and rotationally invariant.
Once detected and identified, AR tags can be used to accurately calculate the pose
(location and orientation) of a camera, as well as to encode information or associate
it with each tag's specific identity [#Sanni2012]. Using an AR tag to calculate the
relative pose of a camera in real-time is called marker-based tracking. Several
straightforward and robust maker-based tracking toolkits exist, such as ARToolKit
[#ARToolKit], ALVAR [#ALVAR], and AprilTags [#Olson2011]. The multistage localization
approach presented in this paper will use a ROS implementation of ALVAR [#ARTrackAlvar]
to both calculate the pose of an AR tag in the camera's frame and associate information
with that tag's specific identity.

Figure <!--\ref{fig:summary_of_alvar_ar_tag_detection_framework}--> shows ALVAR's
marker-based tracking framework. The system first acquires a greyscale image, either
directly or through conversion from another image format. After this, adaptive
thresholding, which can handle local changes in luminance, is used to create a binary
image. A binary image consists of a background, black pixels, and objects, sections
of white pixels. Subsequently, the contours of each object are found. Due to the
fact that AR systems, like ALVAR, aim for real-time processing and fast performance,
time cannot be wasted processing non-markers. As a result, fast acceptance/rejection
tests are run. ALVAR uses two fast accept/reject tests including size and the four-corner
test. The number of pixels in an object's perimeter can be efficiently used to estimate
the object's area. Objects with areas that are either too large or too small can
be rejected. Even if an object, whose area is too small, is a marker, it is too
far from the camera to either be correctly identified or the pose of the camera
accurately calculated. Objects, whose area is too large, can be rejected given some
background knowledge about the upper and lower range of the camera from the marker,
as well as the marker's size. The contours of all remaining markers are fitted with
lines. A quadrilateral has exactly four straight lines and four corners. The number
of lines and corners of each object are calculated. Those that fail the four-corner
test are dropped. After determining that an object is a marker, it is identified
and the corner locations are optimized to sub-pixel accuracy for further calculations.
Even small errors in the detected 2-D locations of edges and corners can significantly
affect the calculated pose of the camera [#Sanni2012]. The pose of a calibrated
camera can be uniquely determined from a minimum of four coplanar but non-collinear
points [#Hartley2004]. Thus, the system can calculate a marker's pose (relative
to camera) in 3-D coordinates using the four corner points of the marker in image
coordinates.


<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/ar_track_alvar_framework.png}
  \caption{Summary of ALVAR AR Tag Detection Framework. Adapted from \protect\cite{Sanni2012}.}
  \label{fig:summary_of_alvar_ar_tag_detection_framework}
\end{figure}
-->

## Summary

Insert Text Summarizing This Chapter and Transiting to the Next.

# Multistage Localization for High Precision Mobile Manipulation Tasks

The purpose of this chapter is to thoroughly explain each component of the multistage
localization approach presented in this paper, as well as how these individual
components fit together in order to enable high precision 3-D feature point localization
without having to recalibrate the positon of the end-effector each time the robot's
mobile bases moves. In addition, this chapter will expound upon the implementation
of this approach to two tasks which are prevalent in both manufacturing and construction
industries: drilling and sealant application operations.

## Approach Overview

Figure <!--\ref{fig:multistage_localization_approach_overview}--> summarizes the
multistage localization approach's framework. The mobile manipulator first localizes
itself within the area of operation (AO) using adaptive Monte Carlo localization,
which relies on the fused odometry and sensor messages published by the robot, as
well as a 2-D map of the AO, normally generated using an optimization-based smoothing
simultaneous localization and mapping (SLAM) technique. The robot navigates to a
predefined start location in the map using a technique called trajectory rollout.
Once there, the robot uses an RGB-D sensor to localize an augmented reality (AR)
tag in the map frame. Once localized, the identity and the 3-D position and orientation,
collectively known as pose, of the tag are used to generate a list of initial feature
points and their locations based off of *a priori* knowledge. After the end-effector
moves to the rough location of a feature point provided by the AR localization,
the feature point's location, as well as the end-effector positon and orientation are
refined to within a user specified tolerance through the use of a control loop,
which utilizes images from a calibrated machine vision camera and a laser pointer,
simulating stereo vision, to localize the feature point in 3-D space using computer vision techniques.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/multistage_localization_approach.png}
  \caption{Multistage Localization Approach Overview.}
  \label{fig:multistage_localization_approach_overview}
\end{figure}
-->

## Global Map Creation and Task Location Specification

In order for a mobile manipulator to operate and perform intricate tasks in a complex,
GPS-denied environment, such as that of manufacturing facility or construction site, without modifying
said environment, the robot must be able to create an accurate map of its environment.
Simultaneous localization and mapping (SLAM) is the process of building or
updating a map of an unknown environment while simultaneously
localizing the robot within this map. Most SLAM techniques can be categorized into
two main paradigms: filtering and optimization-based smoothing. While filtering
SLAM techniques have been widely used in the past due to their ability to model
different sources of sensor noise, in recent years optimization-based smoothing
techniques have proven to be more efficient, scalable, and robust than that of filtering
techniques [#Latif2013]. ROS [#Quigley2009], through which the multistage localization
approach was implemented, includes several 2-D SLAM packages. Santos et al. performed
a thorough real world evaluation of these available 2-D SLAM packages in [#Santos2013],
in which KartoSLAM [#KartoSLAM], [#Konolige2010], an optimization-based smoothing SLAM
technique, outperformed the other ROS implemented SLAM algorithms in terms of accuracy
and computational efficiency. This not only demonstrates the robustness of its sparse pose
adjustment (SPA), but that of full SLAM techniques in general. As a result, optimization-based
smoothing techniques, such as KartoSLAM, should be used to generate 2-D occupancy
grid maps of the robot's environment. After which, start positions and orientations
of specific operations are defined in the map as seen in Figure <!--\ref{fig:global_map}-->.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/2d_map_with_specified_start_locations.png}
  \caption{2-D map environment with specified start locations.}
  \label{fig:global_map}
\end{figure}
-->

## Autonomous Localization and Navigation

After building a 2-D occupancy grid map, shown in Figure <!--\ref{fig:global_map}-->,
using an optimization-based smoothing SLAM technique, it becomes crucial to accurately
localize the robot within this predefined map; so that, the robot can both plan
and execute appropriate trajectories to reach its destination. Figure <!--\ref{fig:robot_localization_framework}-->
shows how the multistage localization approach localizes the robot within the map
frame. Through the measurement of wheel rotation and the integration of accelerations
provided by an inertial measurement unit (IMU), the distance traveled by the robot
from its initial position can be calculated and the robot's pose in the map estimated.
However, these methods do not account for wheel slippage or measurement error. As a result,
the accuracy of the pose estimation will degrade over time. Consequently, the odometry
measurements from these two sources are fused together through an Extended Kalman
filter to provide an accurate pose estimate [#Moore2014]. The corrected odometry,
as well as laser scan measurements from a LIDAR and the prebuilt map are used by Adaptive
Monte Carlo Localization (AMCL) [#AMCL] to localize the robot within this prebuilt
map. AMCL utilizes a particle filter to keep track of the robot's pose and *KLD-sampling*
to improve computational efficiency by removing redundant particles.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/robot_localization_framework.png}
  \caption{Robot Localization Framework.}
  \label{fig:robot_localization_framework}
\end{figure}
-->

After localizing the robot within the predefined map, and computing local and global
costmaps, in which obstacles and a specified distance around them represent a cost,
the robot must now both plan and execute appropriate trajectories to reach its destination.
Trajectory Rollout [#BaseLocalPlanner] samples the space of feasible controls. For
a differential drive robot, this controls space is 2D and consists of translations
and rotational velocities, $\dot{x}, \dot{\theta}$, which are limited by the robot's
capabilities. This sampled velocity is forward simulated from the robot's current
position for a short period of time in order to generate simulated trajectories.
These simulated trajectories are then scored using the cost function in Equation
<!--\ref{eq:trajectory_rollout_cost_function}-->,

<!--
\begin{equation}
C(\textit{k}) = \alpha{} Obs + \beta{} Gdist +\gamma{} Pdist + \delta{} \frac{1}{\dot{x}^2}
\label{eq:trajectory_rollout_cost_function}
\end{equation}
-->

where *Obs* is the sum of grid cell costs through which the trajectory passes, *Gdist*
and *Pdist* are the estimated shortest distance from the endpoint of the trajectory
to the goal and the optimal path, respectively, and $\dot{x}$ is the translation
component of the velocity command that produces the trajectory. The simulated trajectory
that minimizes this cost function is chosen. As a result, chosen trajectories tend
to keep obstacles at a distance, proceed towards the goal, remain near the optimal
path, as well as have higher velocities [#Gerkey2008].

ROS's Navigation Stack [#Navigation] is a collection of packages, which the aforementioned
techniques can be implemented in, in order to localize the robot within a prebuild map,
as well as plan and execute trajectories by outputting the velocity commands needed;
so that, the robot can reach the specified goal.

## Initial Feature Localization Framework

Each Augmented Reality (AR) tag has a unique identity. As a result, relevant information,
such as feature locations relative to the AR tag's pose for a specific operation,
can be associated with a predetermined AR tag identity. So, once the robot has arrived
at the operation's specified start positions and orientation, the robot uses the
ALVAR package [#ARTrackAlvar] and an RGB-D sensor to detect, identify, and calculate
the Augmented Reality (AR) tag pose. Upon identification of the AR tag at the operation
start position, the feature locations for that operation are populated and then
localized on the work surface given the AR tag's pose in the map frame. Figure
<!--\ref{fig:initial_feature_localization_framework}--> shows this initial feature
localization framework.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/initial_feature_localization_framework.png}
  \caption{Initial Feature Localization Framework.}
  \label{fig:initial_feature_localization_framework}
\end{figure}
-->

## Motion Planning

Once the operation's feature locations have been populated and localized on the work
surface in the map frame, the manipulator must be able to move the end-effector
to these locations. The manipulator's motion planning is done through ROS's Moveit!
package [#MoveIt], which was set up to use the TRACLabs' numerical inverse kinematic
(IK) solver, TRAC_IK [#TracIK]. Due to the recursive nature of numerical solvers,
they tend to be slower than that of analytical solvers and are prone to get trapped
in local minima. However, analytical solvers require a separate kinematic solver
be created for each manipulator and manipulator configuration. Consequently, in
order to ensure cross platform functionality with minimum overhead, numerical solvers
were used. Orocos Kinematics and Dynamics Library (KDL) [#KDL], Moveit! default
numerical solver, is arguably the most widely-used generic numerical solver
worldwide. However, it suffers from an unacceptably high failure rate and slow average speed.
Beeson et al. determined in [#Beeson2015] the causes behind the high failure rates
and slow average speed, as well as developed TRAC-IK to overcome them. Beeson et al.
revealed that KDL had the following flaws:

1. Frequent convergence failures due to manipulator joint limits
2. No action is taken when the search gets trapped in a local minima
3. Lack of tolerance support and/or utilization in the solver itself

TRAC-IK solves these issues by using two separate IK solvers concurrently, as well
as incorporating local minimum detection in both. It uses KDL-RR, a rewrite of KDL
which incorporates local minimum detection in order to solve problems two and three
by "unsticking" the convergence algorithm through the introduction of random seed
angles when a local minimum is detected, as well as a sequential quadratic programming
(SQP) IK implementation, which is an iterative algorithm for nonlinear optimization.
The SQP implementation, which incorporates the same local minimum detection as KLD-RR,
is able to deal with issue one. However, SQP can have a much longer solve time than
KDL or KDL-RR. As a result, TRAC-IK implements both of these IK methods concurrently
and waits for either IK to converge. As a result, the solve rate ranges between 99.1%
and 99.9 % and the average convergence times are well below one millisecond.

## Feature Correction Framework

Unfortunately, localizing features in the map frame based on the AR tag localization
is not sufficient for tasks that require very high precision due to errors associated
in localizing the AR tag in the map frame. Sources of error include, error in the
RGB-D camera's extrinsic calibration, error associated in localizing the AR tag
itself, error in the location estimate of the camera with respect to the robot's
base link, and finally error in the location estimate of robot's base link in the
map. Consequently, a secondary localization method is required to refine the feature
pose estimates; so that, they can be used for high precision tasks, such as drilling
and sealant application operations in the manufacturing and construction industries.

In order to achieve high precision 3-D localization, the feature must be localized
with a high degree of accuracy in the end-effector frame of reference. Consequently,
a machine vision camera and a laser pointer, simulating stereo vision, were added
to the end-effector. Figure <!--\ref{fig:feature_detection_and_tracking_pipeline}-->
shows the 3-D localization pipeline developed for high precision feature localization.
The 3-D feature localization pipeline runs two separate feature detection algorithms
concurrently. The first feature detection algorithm is responsible for locating
the center pixel of the laser point produced by the laser pointer. It does this
by first converting each RGB frame to the HSV color space. The laser pointer is
affixed so that the center of the laser point is located within the center region
of each frame; so, the outside edges of each frame is masked. After which, the image
is thresholded for the specific color range of the laser point. This produces a
binary image, in which only pixels within the specific color range are considered
foreground. Morphological filters are used to remove noise and fill in
any holes in the laser point created by the laser pointer; so that, its center can
be found. The center of the laser point is found by calculating the centroid of the largest blob in the
binary image. The second feature detection algorithm is responsible for locating
the center pixel of the operation's specific feature. It does this by first converting
each RGB frame to gray scale, after which, preprocessing operations, such as a Gaussian
and Median filter, are used to minimize the noise in the image without degrading
the features. Then Canny Edge Detection [#Canny1986] is used to find appropriate
edges in the image, as well as to produce a binary image, which is used to reduce the
computational complexity of the feature detection algorithm used. At which point,
the outside edges of the image are masked, in order to decrease the likelihood that
features other than the one desired are detected. After which, the appropriate feature detection
algorithm, such as Hough Circle Detector [#Illingworth1987] or Good Feature to Track
[#Tomasi1994], is run in order to find the center pixel of the feature closest to
the laser point produced by the laser pointer. Once these features' pixel locations
have been found, they are tracked using the Kanade Lucas Tomasi (KLT) tracker [#Baker2004],
based on the optical flow of the image, in order to reduce complexation complexity
that would be associated with running the aforementioned feature detection algorithms
on each frame. The location of the center of the laser point can be found by converting
the pixel to 3-D camera coordinates using the camera's intrinsic parameters. However,
in order to find the depth, the line-line intersection between the 3-D ray of the
laser pointer in the camera frame, which was found during calibration, and the laser
points 3-D ray, which is calculated by finding the line connecting the camera's
center point and the location of laser point in camera coordinates on the image
plane, must be found. If the two lines do not intersect, the midpoint of the shortest
line segment between the two lines should be used. The camera calibration procedure
and the calculation of the laser pointer's 3-D ray equation in the camera frame are
detailed in Section 5.3.1. Given the 3-D location of the laser point in the camera
frame, as well as the surface normal of the laser pointer, which was calculated
during calibration, the plane that the laser point lies on is calculated. Due to
the proximity of the feature to the laser point, they are assumed to lie on the
same plane. Consequently, the 3-D location of the feature point in the camera frame
is calculated by finding the line-plane intersection between the 3-D ray of the
feature, which is calculated by finding the line connecting the camera's center point
and the location of feature point in camera coordinates on the image plane, and
the plane of the laser point.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth, height=\textheight, keepaspectratio]{figures/feature_detection_and_tracking_pipeline.png}
  \caption{Feature Detection and Tracking Pipeline.}
  \label{fig:feature_detection_and_tracking_pipeline}
\end{figure}
-->

After the feature has been localized in the camera frame, the feature is
transformed into the end-effectors frame. The manipulator then updates the position
of the end-effector, while remaining a set distance off the work surface, through
the use of the control loop depicted in Figure <!--\ref{fig:manipulator_correction_control_loop}-->.
Once the end-effector is within the reference tolerance set, the control loop
terminates and the specified operation begins. This reference tolerance should be set
no lower than the maximum accuracy achievable by either the detection algorithm
or the end-effector itself, whichever is higher.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/manipulator_control_loop.png}
  \caption{Manipulator Correction Control Loop.}
  \label{fig:manipulator_correction_control_loop}
\end{figure}
-->

## Approach Implementation

The purpose of this section is to briefly explain how the multistage localization
approach presented in Sections 4.1 through 4.6 can be implemented on a robotic system,
as well as to provide an expandable framework; so that, the mobile manipulator can
be easily programmed to conduct multiple operations within a manufacturing facility
or construction site. Two operations, drilling and sealing, will be explored in depth.

### System Overview

The generic framework of the system is depicted in Figure <!--\ref{fig:system_overview}-->.
The system allows the user to input a number of predefined tasks. Given *a priori*
knowledge of each tasks, as well as their global start locations, the mobile manipulator
will navigate to and perform the requested operation(s) while monitoring its battery
level to ensure mission completion. The framework shown was implemented using a
hierarchical state machine and was written using the Python library SMACH [#SMACH]
in order to ensure compatibility with ROS; thereby, ensuring seamless integration
with the aforementioned packages and ease of use across differing robotic platforms.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/general_system_overview.png}
  \caption{General system overview.}
  \label{fig:system_overview}
\end{figure}
-->

### Drilling Framework

Figure <!--\ref{fig:drilling_operation_framework}--> shows the basic drilling operation
and error handling framework. Once the mobile manipulator has arrived at the drilling
operation's start location and orientated itself in the prescribed direction, its
RGB-D camera will detect, identify, and localize the AR tag located at the work
station. Using this AR tag's location and identity, the specific features associated
with that work station will be populated and localized on the work surface in the
map frame. After these initial feature locations have been populated, the robot
will move within range of the first feature location in the list. The manipulator
will move the end-effector to the specified Cartesian location specified using Moveit!
and the TRAC-IK numerical solver. Currently, many companies use hole templates to ensure
that objects are drilled within necessary tolerances. Consequently, the feature
correction framework will use these features as landmarks and detect them using
Hough Circle Detector [#Illingworth1987]. Using these landmarks, the feature locations
are refined to within a user specified accuracy. After which, the manipulator will
drill the hole to the specified depth and at the specified feed rate requested by
the user. This will continue until all features have been drilled. However, if the
robot due to obstruction or another reason is unable to navigate within range of a
feature, the machine vision camera is unable to detect the landmark, or the manipulator
is unable to reach the location, these specified features will be added to a flag
list and the customer notified after the operation.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/drilling_operation_framework.png}
  \caption{Drilling operation framework.}
  \label{fig:drilling_operation_framework}
\end{figure}
-->

### Sealant Application Framework

Figure <!--\ref{fig:sealant_application_framework}--> shows the basic sealant application
operation and error handling framework. Once the mobile manipulator has arrived
at the sealant applicant operation's start location and orientated itself in the
prescribed direction, its RGB-D camera will detect, identify, and localize the AR
tag located at the work station. Using this AR tag's location and identity, the specific
features associated with that work station will be populated and localized on the
work surface in the map frame. After these initial feature locations have been populated,
the robot will move within range of the first feature location in the list. The
manipulator will move the end-effector to specified Cartesian location using Moveit!
and the TRAC-IK numerical solver. Most sealant applicant operations, such as sealing
a window frame, require the application of sealant from one corner to another. Consequently,
the feature correction framework will use these features, corners, as landmarks
and detect them using Good Feature to Track [#Tomasi1994]. Using these landmarks,
the feature locations are refined to within a user specified accuracy. After which,
the manipulator will move to the next corner location in the list. After this corner's
location has been refined, the manipulator will transition back to the previous
corner's location and seal from that start corner to the end corner. This will continue
until all connected corners have sealant applied between them. However, if the robot
due to obstruction or another reason is unable to navigate within range of a feature,
the machine vision camera is unable to detect the landmark, or the manipulator is
unable to reach the location, these specified features will be added to a flag list
and the customer notified after the operation.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/sealant_application_framework.png}
  \caption{Sealant Application Framework.}
  \label{fig:sealant_application_framework}
\end{figure}
-->

## Summary

The purpose of this chapter was to thoroughly explain each component of the multistage
localization approach presented in this paper, as well as how these individual components
fit together in order to enable high precision 3-D feature point localization without
having to recalibrate the positon of the end-effector each time the robot's mobile
bases moves. In addition, this chapter expounded upon the implementation of
this approach to two tasks which are prevalent in both manufacturing and construction
industries: drilling and sealant application operations. The following chapter will
discuss the experiment conducted in order to validate the approach using the two
aforementioned operations.

# Experiments and Results

The purpose of this chapter is to validate each stage of multistage localization
approach and system implementation presented in Chapter 4 through testing. The accuracy
of each component of the multistage localization technique will be determined on
two different ROS enabled robotic platforms, the Clearpath Robotics' Husky and Fetch
Robotics' Fetch. In addition, these accuracies will be discussed and methods to
improve them to the accuracy limits of the manipulators used will be presented.

## Hardware Architecture

Figure <!--\ref{fig:clearpath_robotics'_husky_and_fetch_robotics'_fetch_mobile_manipulator}-->
shows the Clearpath Robotics' Husky and Fetch Robotics' Fetch with the necessary
equipment to simulate high precision operations, such as drilling and sealing. Each
robot is equipped with an onboard computer, as well as an RGB-D and RGB camera,
a laser pointer, LIDAR, IMU, wheel encoders, and a manipulator.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/husky_and_fetch_hardware_architecture.png}
  \caption{Clearpath Robotics' Husky and Fetch Robotics' Fetch Mobile Manipulator}
  \label{fig:clearpath_robotics'_husky_and_fetch_robotics'_fetch_mobile_manipulator}
\end{figure}
-->

The Clearpath Robotics' Husky in particular is equipped with a Microsoft Kinect
V2, Baser ACA1920-150uc machine vision camera, Sick LMS-151 LIDAR, MicroStrain 3DM
GX3-25 IMU, four high precision wheel encoders, and the Universal Robotics UR5 6
DOF manipulator with a 5kg payload capacity. In regards to the specific computer
hardware, the Clearpath Robotics' Husky is equipped with a Gigibyte BRIX GB-BXi7G3-760
which includes an Intel i7 processor, NVIDIA GeForce GTX 760 GPU, 16 GB DDR3 RAM,
250 GB SSD, and two USB3.0 ports.

The Fetch Robotics' Fetch in particular is equipped with a Primesense Carmine 1.09,
Baser ACA1920-150uc machine vision camera, Sick TIM571 LIDAR, two 6 DOF IMUs, two
high precision wheel encoders, and Fetch Robot's proprietary 7 DOF manipulator with
a 6 kg payload capacity. In regards to the specific computer hardware, The Fetch
Robotics' Fetch is equipped with a standard Mini-ITX motherboard, which includes
an Intel i5 processer, 16 GB DDR3 RAM, 120 GB SSD, and three USB 2.0 ports.

## Software Architecture

The Clearpath Robotics' Husky and Fetch Robotics' Fetch used Ubuntu 14.04 and ROS
Indigo for the experiments. The Clearpath Robotics' Husky used an Extended Kalman
Filter through ROS's robot_localization package [#RobotLocalization] to fuse its
wheel encoder and IMU measurements together, while Fetch Robotics' Fetch used an
Unscented Kalman Filter. Both platforms used this corrected odometry, as well as
laser scans from their respective LIDAR to produce a map of the area of operation
(AO) using ROS's KartoSLAM package [#KartoSLAM]. Both robot's use ROS's Navigation
Stack [#Navigation] to localize themselves in the map using the AMCL package [#AMCL]
and implement trajectory rollout through the base_local_planner package [#BaseLocalPlanner]
for path planning and obstacle avoidance. Both robots used ROS's ar_track_alvar
package [#ARTrackALVAR] to identify and localize augmented reality (AR) tags. For
motion planning, both robot's used ROS's Moveit! [# MoveIt], which was setup to
use the TRAC-IK [#TracIK] numerical solver. For perception, both robot's used OpenCV
2.4 [#Itseez2015] to perform necessary operations on the images provided by the
Basler machine vision camera.

## Experiments

The purpose of this section is to explain the experimental setup of each test, as
well as how the data presented in Section 5.4 was collected. In addition, the camera
calibration setup and the software used for the calibration will be presented.

### Camera Calibration Setup

Figure <!--\ref{fig:camera_calibration_setup}--> shows the experimental setup used
to calibrate both the Microsoft Kinect V2, the Baser ACA1920-150uc, as well as the
laser pointer in the Baser ACA1920-150uc's camera frame. For each calibration the
camera and checkerboard pattern were affixed to individual tri-pods. The Microsoft
Kinect V2 was calibrated using the kinect2_calibration package [#iai_kinect2], while
the Baser ACA1920-150 was calibrated using Jean-Yves Bouguet camera calibration
toolbox for Matlab [#Bouguet2015].

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/camera_calibration_setup.png}
  \caption{Camera Calibration Setup.}
  \label{fig:camera_calibration_setup}
\end{figure}
-->

The kinect2_calibration package and Bouguet's camera calibration toolbox both follow
the calibration procedure proposed by Dr. Zhang in [#Zhang1999]. First, images of
a checkerboard pattern affixed to a planar surface are taken by the camera at different
distances and orientations. In these images, the feature points in the checkerboard
pattern are detected, as depicted in Figure <!--\ref{fig:extracted_corners_of_calibration_pattern}-->.
These features are used to estimate the five intrinsic parameter of the camera,
as well as all the extrinsic parameters using the closed-form equations developed
by Dr. Zhang in [#Zhang1999]. The coefficients of radial distortion are then solved
using linear least-square. After which, all parameters are refined through nonlinear
minimization.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/image_extracted_corners.png}
  \caption{Extracted Corners of Calibration Pattern.}
  \label{fig:extracted_corners_of_calibration_pattern}
\end{figure}
-->

After the Baser ACA1920-150 camera had been calibrated, the camera and the laser
pointer were affixed to the mount shown in Figure <!--\ref{fig:clearpath_robotics'_husky_and_fetch_robotics'_fetch_mobile_manipulator}-->.
After which, the laser pointer 3-D ray equation in camera frame can be calculated
by minimizing the error between at least 2 known 3-D coordinates in the camera frame.
Thirty points were used to calibrate the 3-D rays equation of the laser pointer
for testing. Figure <!--\ref{fig:extracted_corners_and_laser_center}--> shows a
checkboard, along with a laser point produced by the laser pointer. The extrinsic
parameters of each image are calculated using the feature in the checkerboard. Using
the extrinsic, the plane equation of the checkerboard was calculated in camera coordinates.
These 3-D camera coordinates points were found by finding line-plane intersection
between the plane of the checkerboard and 3-D ray of the laser point, which is calculated
by finding line connecting the camera's center point and the locations of laser
point in camera coordinates. The same procedure presented in Section 4.6 was used
to find the center pixel of the laser point.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/laser_extracted_corners_laser_center.png}
  \caption{Extracted Corners and Laser Center}
  \label{fig:extracted_corners_and_laser_center}
\end{figure}
-->

### Navigation System Experimental Setup

Figure <!--\ref{fig:navigation_system_experimental_setup}--> shows the experimental
setup for the navigation system test. Both the Clearpath Robotics' Husky, as well
as Fetch Robotics' Fetch navigated to specified positions 5, 10, and 20 meters away
from a specified start position. The absolute difference in the x and y directions,
as well as the angular yaw between the robot's final positons and the set reference
point were measured. This test was run continually as the robot proceed to each
5, 10, and 20 meter location, 20 time each.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/navigation_system_experimental_setup.png}
  \caption{Navigation System Experimental Setup.}
  \label{fig:navigation_system_experimental_setup}
\end{figure}
-->

### Augmented Reality Tag Detection, Identification, and Localization System Experimental Setup

Figure <!--\ref{fig:drilling_operations_experimental_setup}--> shows the experimental
setup for the augmented reality (AR) tag detection, identification, and localization
tests. Both the Clearpath Robotics' Husky, as well as Fetch Robotics' Fetch were
placed at locations a specific x and y distance away, as well as at a specific angle
from the drilling station's augmented reality (AR) tag, given the maximum error
from the navigation test. Each robot's RGB-D sensor was used to detect, identify
and localize the specific AR tag. After the feature points for that operation were
populated, the absolute difference in x, y, and z directions were measured. For
each location, the experiment was conducted five times with 50 features, including
the AR tag.

### Drill Operation Experimental Setup

Figure <!--\ref{fig:drilling_operations_experimental_setup}--> shows the experimental
setup for the drilling operation test. First the sub-millimeter accuracy of the
Universal Robotics' 6 DOF UR5 and Fetch Robotics' 7DOF manipulator was verified.
After which, the accuracy of feature detection, tracking, and localization pipeline
was tested using the Baser ACA1920-150uc camera and laser pointer. This accuracy
was determined by calculating the difference between the distance measured between
the laser point and hole center produced by the feature detection, tracking and
localization framework and the ground truth distance between the laser center and
hole center measured by hand. This test was performed with the end-effector a set
distance off the work surface for 20 locations with the following distances between
the laser point and corner: less than 5 mm, between 5 and 10 mm, between 10 and
15 mm, and between 15 and 20 mm.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/drilling_operation_experimental_setup.png}
  \caption{Drilling Operations Experimental Setup.}
  \label{fig:drilling_operations_experimental_setup}
\end{figure}
-->

### Sealant Application Experimental Setup

Figure <!--\ref{fig:sealant_application_experimental_setup}--> shows the experimental
setup for the sealant operation test. First the sub-millimeter accuracy of the Universal
Robotics' 6 DOF UR5 and Fetch Robotics' 7DOF manipulator was verified. After which,
the accuracy of feature detection, tracking, and localization pipeline was tested
using the Baser ACA1920-150uc camera and laser pointer. This accuracy was determined
by calculating the difference between the distance measured between the laser point
and corner produced by the feature detection, tracking and localization framework
and the ground truth distance between the laser center and corner measured by hand.
This test was performed with the end-effector a set distance off the work surface
for 20 locations with the following distances between the laser point and corner:
less than 5 mm, between 5 and 10 mm, between 10 and 15 mm, and between 15 and 20
mm.

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/sealant_application_experimental_setup.png}
  \caption{Sealant Application Experimental Setup.}
  \label{fig:sealant_application_experimental_setup}
\end{figure}
-->

## Results and Discussion

The purpose of this section is to present the accuracies of the multistage localization
approach and system implementation presented in Chapter 4 through the tests presented
in Section 5.3. In addition, steps to improve these accuracy will be presented.
It should be noted that the coordinate frame used in these tests, have positive
x as forward, positive y as left, and positive z up with respect to the robot.

### Camera Calibration Accuracy Achieved

The Microsoft Kinect V2 incorporates both an RGB and an IR camera. Each camera's
intrinsic parameters and distortion coefficients were calculated through calibrated
individually. After which, each calibrated camera took pictures of a checkerboard
at the same distance and orientation. The images extrinsic parameters were used
to calculate the pose of each camera in the other's camera frame. The Microsoft
Kinect V2's RGB, IR, and Pose pixel error from the calibrations were 0.376 pixels,
0.126 pixels, and .284719 pixels respectively. The Baser ACA1920-150uc pixel error
from calibration was .418. While the laser pointer 3-D ray in Baser ACA1920-150uc
frame minimum mean square orthogonal distance to the line was 0.6468 mm.

### Navigation System Accuracy Achieved

Table <!--\ref{tabel:global_localization_accuracy_given_distance_from_start_clearpath_robotics_husky}-->
and Figures <!--\ref{fig:global_x_and_y_localization_accuracy_given_distance_from_start_clearpath_robotics_husky}-->
and <!--\ref{fig:global_psi_localization_accuracy_given_distance_from_start_clearpath_robotics_husky}-->
show the x, y and $\psi$ error associated with the Clearpath Robotics' Husky in
reaching the specified positions 5, 10, and 20 meters away from a specified start
position. While Table <!--\ref{tabel:global_localization_accuracy_given_distance_from_start_fetch_robotics_fetch}-->
and Figures <!--\ref{fig:global_x_and_y_localization_accuracy_given_distance_from_start_fetch_robotics_fetch}-->
and <!--\ref{fig:global_psi_localization_accuracy_given_distance_from_start_fetch_robotics_fetch}-->
show the x, y and yaw error associated with the Fetch Robotics' Fetch in reaching
the specified positions 5, 10, and 20 meters away from a specified start position.
The following Tables and Figures show that neither the Husky nor the Fetch average
error pert test exceeded 6.4 cm in the x, 4.5 cm in the y, and .03 radians in the
$\psi$. Given that these tests were done consecutively, equating to nearly a kilometer,
and that y and $\psi$ deviation decreased over time, while the error in x seems
to have a more sinusoidal pattern, which probably equates to resampling of the particle
filters, the error is fairly low and did not shown any significant effect on the
steps in the multistage localization that followed it. In addition, these tests
were conducted in a long hallway with very few artificial features. A manufacturing
facility or construction site tend to be filled with artificial features, thus improvement
in the accuracy of the adaptive Monte Carlo localization should be seen in these
environments. Additional performance would be seen by increasing the number of sensors
fused together by the Kalman filter to improve the filtered odometry sent to the
AMCL as seen in [#Moore2014]. It should be noted that the increased error seen by
the Husky over the Fetch was most likely due to the IMU being unresponsive during
the tests conducted, which supports the theory above that fusing additional sensors
would decrease error.

<!--
\begin{table}[H]
\centering
\caption{Global Localization Accuracy Given Distance From Start - Clearpath Robotics' Husky.}
\label{tabel:global_localization_accuracy_given_distance_from_start_clearpath_robotics_husky}
\resizebox{0.75\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|c|c|c|@{}}
\cmidrule(l){2-7}
                                    & \textbf{x (cm)} & \textbf{x std (cm)} & \textbf{y (cm)} & \textbf{y std (cm)} & \textbf{$\psi$ (rad)} & \textbf{$\psi$ std (rad)} \\ \midrule
\multicolumn{1}{|c|}{\textbf{5 m}}  & 6.29            & 4.20                & 4.50            & 3.58                & 0.03                  & 0.02                      \\ \midrule
\multicolumn{1}{|c|}{\textbf{10 m}} & 4.16            & 3.16                & 2.88            & 2.97                & 0.03                  & 0.03                      \\ \midrule
\multicolumn{1}{|c|}{\textbf{20 m}} & 6.35            & 4.55                & 2.37            & 1.89                & 0.02                  & 0.01                      \\ \midrule
\multicolumn{1}{|c|}{\textbf{Mean}} & \textbf{5.60}   & \textbf{3.97}       & \textbf{3.25}   & \textbf{2.81}       & \textbf{0.03}         & \textbf{0.02}             \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/husky_navigation_xy_accuracy.png}
  \caption{Global X and Y Localization Accuracy Given Distance From Start - Clearpath Robotics' Husky.}
  \label{fig:global_x_and_y_localization_accuracy_given_distance_from_start_clearpath_robotics_husky}
\end{figure}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/husky_navigation_yaw_accuracy.png}
  \caption{Global $\psi$ Localization Accuracy Given Distance From Start - Clearpath Robotics' Husky.}
  \label{fig:global_psi_localization_accuracy_given_distance_from_start_clearpath_robotics_husky}
\end{figure}
-->

<!--
\begin{table}[H]
\centering
\caption{Global Localization Accuracy Given Distance From Start - Fetch Robotics' Fetch.}
\label{tabel:global_localization_accuracy_given_distance_from_start_fetch_robotics_fetch}
\resizebox{0.75\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|c|c|c|@{}}
\cmidrule(l){2-7}
                                    & \textbf{x (cm)} & \textbf{x std (cm)} & \textbf{y (cm)} & \textbf{y std (cm)} & \textbf{$\psi$ (rad)} & \textbf{$\psi$ std (rad)} \\ \midrule
\multicolumn{1}{|c|}{\textbf{5 m}}  & 4.25            & 2.50                & 1.48            & 1.23                & 0.03                  & 0.02                      \\ \midrule
\multicolumn{1}{|c|}{\textbf{10 m}} & 3.93            & 2.62                & 2.57            & 1.66                & 0.03                  & 0.02                      \\ \midrule
\multicolumn{1}{|c|}{\textbf{20 m}} & 5.82            & 3.43                & 1.52            & 1.11                & 0.03                  & 0.02                      \\ \midrule
\multicolumn{1}{|c|}{\textbf{Mean}} & \textbf{4.67}   & \textbf{2.85}       & \textbf{1.85}   & \textbf{1.33}       & \textbf{0.03}         & \textbf{0.02}             \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/fetch_navigation_xy_accuracy.png}
  \caption{Global X and Y Localization Accuracy Given Distance From Start - Fetch Robotics' Fetch.}
  \label{fig:global_x_and_y_localization_accuracy_given_distance_from_start_fetch_robotics_fetch}
\end{figure}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/fetch_navigation_yaw_accuracy.png}
  \caption{Global $\psi$ Localization Accuracy Given Distance From Start  - Fetch Robotics' Fetch.}
  \label{fig:global_psi_localization_accuracy_given_distance_from_start_fetch_robotics_fetch}
\end{figure}
-->

### Augmented Reality Tag Accuracy Achieved

Table <!--\ref{tabel:augmented_reality_accuracy_given_specific_start_conditions_primesense_carmine_1_09}-->
and Figure <!--\ref{fig:augmented_reality_accuracy_given_specific_start_conditions_primesense_carmine_1_09}-->
show the augmented reality (AR) tag average localization x, y and z error using
250 features for each of 6 different tests using the Primesense Carmine 1.09. Test
1 and 2, had the RGB-D sensor centered on the AR tag but offset in the negative
x direction (backwards) 1.0 and 0.75 m respectively. While Test 3 and 5, as well
as 5 and 6 were offsets of Test 1 and 2 respectively. Test 3 and 4, were offset
16.1 cm in the negative x direction, 7.5 cm in the positive y direction (left),
and 0.09 radian in the negative $\psi$ direction. While test 5 and 6, were offset
16.1 cm in the negative x direction, 7.5 cm in the negative y direction (right),
and 0.09 radians in the positive $\psi$ direction. These distance offsets were determined
by taking the average error from the Fetch's navigation tests and adding three standard
deviations. The tests show an average error of approximately 3.3 cm in the x direction,
12.6 cm in the y direction, and 11.8 cm in the z. The difference in error between
Tests 1 and 2, 3 and 4, 5 and 6, are either approximately the same or lower as the
x distance between is decreased. This makes mathematically sense in that as the
sensor is closer to the wall as it is in Tests 2, 4, and 6, the ground sample distance
(GSD) is lower equating to a higher spatial resolution. Also, it appears that moving
the RGB-D sensor slightly in the positive y direction (left) and negative $\psi$
seems to decrease error.

<!--
\begin{table}[H]
\centering
\caption{Augmented Reality Accuracy Given Specific Start Conditions - Primesense Carmine 1.09.}
\label{tabel:augmented_reality_accuracy_given_specific_start_conditions_primesense_carmine_1_09}
\resizebox{0.75\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|c|c|c|@{}}
\cmidrule(l){2-7}
                                      & \textbf{x (mm)} & \textbf{x std (mm)} & \textbf{y (mm)} & \textbf{y std (mm)} & \textbf{z (mm)} & \textbf{z std (mm)} \\ \midrule
\multicolumn{1}{|c|}{\textbf{Test 1}} & 50.41           & 22.81               & 148.87          & 12.33               & 150.96          & 12.61               \\ \midrule
\multicolumn{1}{|c|}{\textbf{Test 2}} & 23.71           & 16.06               & 151.82          & 6.86                & 117.57          & 12.81               \\ \midrule
\multicolumn{1}{|c|}{\textbf{Test 3}} & 23.06           & 14.85               & 128.39          & 11.48               & 173.24          & 23.24               \\ \midrule
\multicolumn{1}{|c|}{\textbf{Test 4}} & 24.36           & 17.47               & 138.66          & 6.77                & 42.80           & 16.37               \\ \midrule
\multicolumn{1}{|c|}{\textbf{Test 5}} & 37.19           & 14.85               & 183.21          & 7.30                & 115.85          & 16.27               \\ \midrule
\multicolumn{1}{|c|}{\textbf{Test 6}} & 39.16           & 23.52               & 7.27            & 3.70                & 105.24          & 9.16                \\ \midrule
\multicolumn{1}{|c|}{\textbf{Mean}}   & \textbf{32.98}  & \textbf{18.26}      & \textbf{126.37} & \textbf{8.07}       & \textbf{117.61} & \textbf{15.08}      \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/ar_tag_accuracy_primesense.png}
  \caption{Augmented Reality Accuracy Given Specific Start Conditions - Primesense Carmine 1.09.}
  \label{fig:augmented_reality_accuracy_given_specific_start_conditions_primesense_carmine_1_09}
\end{figure}
-->

Table <!--\ref{tabel:augmented_reality_accuracy_given_specific_start_conditions_microsoft_kinect_v2}-->
and Figure <!--\ref{fig:augmented_reality_accuracy_given_specific_start_conditions_microsoft_kinect_v2}-->
show the augmented reality (AR) tag average localization x, y and z, error using
250 features for each of 3 different tests using the Microsoft Kinect V2. Test 1,
had the RGB-D sensor centered on the AR tag but offset in the negative x direction
(backwards) 1.0. While Test 2 and 3, were offsets of Test 1. Test 2, were offset
20.0 cm in the negative x direction, 15.2 cm in the positive y direction (left),
and 0.115 radian in the negative $\psi$ direction. While test 3, were offset 20.0
cm in the negative x direction, 15.2 cm in the negative y direction (right), and
.115 radians in the positive $\psi$ direction. These distance offsets were determined
by taking the average error from the Husky's navigation tests and adding three standard
deviations. The tests show an average error of approximately 8.8 cm in the x direction,
3.0 cm in the y direction, and 5.5 cm in the z. Also, it appears that moving the
RGB-D sensor slightly in the positive y direction (left) and negative $\psi$ seems
to decrease error overall error in the y and z as seen with the Primesense Carmine
1.09.

<!--
\begin{table}[H]
\centering
\caption{Augmented Reality Accuracy Given Specific Start Conditions - Microsoft Kinect V2.}
\label{tabel:augmented_reality_accuracy_given_specific_start_conditions_microsoft_kinect_v2}
\resizebox{0.75\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|c|c|c|@{}}
\cmidrule(l){2-7}
                                      & \textbf{x (mm)} & \textbf{x std (mm)} & \textbf{y (mm)} & \textbf{y std (mm)} & \textbf{z (mm)} & \textbf{z std (mm)} \\ \midrule
\multicolumn{1}{|c|}{\textbf{Test 1}} & 14.32           & 9.75                & 4.04            & 1.65                & 84.77           & 2.50                \\ \midrule
\multicolumn{1}{|c|}{\textbf{Test 2}} & 163.85          & 57.58               & 36.32           & 9.65                & 25.31           & 12.46               \\ \midrule
\multicolumn{1}{|c|}{\textbf{Test 3}} & 84.33           & 44.37               & 50.31           & 5.48                & 55.24           & 8.57                \\ \midrule
\multicolumn{1}{|c|}{\textbf{Mean}}   & \textbf{87.50}  & \textbf{37.24}      & \textbf{30.22}  & \textbf{5.59}       & \textbf{55.10}  & \textbf{7.84}       \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/ar_tag_accuracy_kinectv2.png}
  \caption{Augmented Reality Accuracy Given Specific Start Conditions - Microsoft Kinect V2.}
  \label{fig:augmented_reality_accuracy_given_specific_start_conditions_microsoft_kinect_v2}
\end{figure}
-->

Overall, the Microsoft Kinect V2 performs significantly better than of the Primesense
Carmine 1.09. This is due to the fact that the Microsoft Kinect V2 is a newer sensor,
which has greater resolution, horizontal and vertical field of view (FOV), and onboard
processing. However, neither the Primesense Carmine 1.09 nor the Microsoft Kinect
V2 performed exceptionally. Sources of error include, error in the RGB-D camera's
extrinsic calibration, error associated in localizing the AR tag itself, error in
the location estimate of the camera with respect to the robot's base link, and finally
error in the location estimate of robot's base link in the map. In order to reduce
these sources of error, the RGB-D camera would need to be calibrated on rig which
was certified to achieve sub-millimeter to millimeter level accuracy. In order to
address the accuracy error inherent in the localization using the AR tag, different
AR tag detection library could be explored, such as AprilTags [#Olson2011] and ARToolKit
[#ARToolKit], or using QR codes with libraries like ZBar [#ZBar]. The RGB-D sensor's
position with respect to robot's base would need to be measure accurately to within
sub-millimeter to millimeter level accuracy. This can be done through using ROS's
robot_calibration package [#RobotCalibration] and a certified calibrated camera
to adjust the robot's URDF. In addition, newer RGB-D sensor such as the Orbbec Astra,
Intel's RealSense R200, and Structure's Sensor or a stereo vision camera such as
ZED's Stereo Vision Camera, Point Grey's Bumblebee2, and the DUO MLX. In addition,
to providing depth information the stereos cameras have the ability to work indoor
and outdoor, which could be extremely advantageous depending on the environment.
While getting millimeter level accuracy could be achieved, getting sub-millimeter
accuracy at distance of 1 m or would be nearly impossible with currently technology.
Consequently, the multistage localization approach would still be necessary for
high precision operations.

### Drilling Operation Accuracy Achieved

Table <!--\ref{tabel:drilling_operation_accuracy_given_distance_from_work_surface}-->
and Figure <!--\ref{fig:drilling_operation_accuracy_given_distance_from_feature_point}-->
show the x, y and z, error associated with the feature localization, detection,
and tracking pipeline using 20 features for each of 4 different tests done using
the Basler ACA1920-150uc and laser pointer, which were approximately 20 cm away
from the work surface. This give the Basler ACA1920-150uc with a 16 mm focal length
lens, a horizontal and vertical field of view (FOV) of 11.9 and 7.6 cm respectively,
as well as a ground sample distance (GSD) of .0062 cm. While the y accuracy increases
slightly as the distance away from the feature is increased, the x decreases slightly
and the z decreased significantly. However, due to the fact that z and x have decreasing
curves as the laser point get closer to the feature, the system is able to achieve
sub-millimeter level accuracy in the y and z, and very low millimeter level accuracy
in the x due to the method's control loop. The Basler ACA1920-150uc with a 16 mm
focal length lens should be able to achieve localization accuracies close to its
GSD of .006 mm, which is exceeds the capability of both manipulators used. In order
to improve the accuracy to this level, the camera and laser should be calibrated
on rig certified to this level of accuracy. Alternatively, a stereo camera rig could
be developed through the purchase of another machine vision camera and those camera
calibrated on a rig certified to the accuracy mentioned above. In addition, the
detection and localization pipeline would need to be improved by using more recent
and robust computer vision technique in order to find the laser point and feature
to within pixel, if not sub-pixel resolution.

<!--
\begin{table}[H]
\centering
\caption{Drilling Operation Accuracy Given Distance From Work Surface.}
\label{tabel:drilling_operation_accuracy_given_distance_from_work_surface}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|c|c|c|@{}}
\cmidrule(l){2-7}
                                                                                                                          & \textbf{x (mm)} & \textbf{x std (mm)} & \textbf{y (mm)} & \textbf{y std (mm)} & \textbf{z (mm)} & \textbf{z std (mm)} \\ \midrule
\multicolumn{1}{|c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Distance Less Than 5 mm from\\   Hole Center\end{tabular}}}       & 1.44            & 1.25                & 0.70            & 0.43                & 0.82            & 0.56                \\ \midrule
\multicolumn{1}{|c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Distance Between 5 and 10 mm\\   from Hole Center\end{tabular}}}  & 1.89            & 1.56                & 0.66            & 0.39                & 1.89            & 0.46                \\ \midrule
\multicolumn{1}{|c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Distance Between 10 and 20 mm\\   from Hole Center\end{tabular}}} & 2.15            & 1.83                & 0.53            & 0.30                & 4.06            & 0.70                \\ \midrule
\multicolumn{1}{|c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Distance Between 20 and 30 mm\\   from Hole Center\end{tabular}}} & 2.71            & 2.71                & 0.31            & 0.21                & 10.25           & 1.38                \\ \midrule
\multicolumn{1}{|c|}{\textbf{Mean}}                                                                                       & \textbf{2.05}   & \textbf{1.83}       & \textbf{0.55}   & \textbf{0.33}       & \textbf{4.25}   & \textbf{0.77}       \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/drill_accuracy.png}
  \caption{Drilling Operation Accuracy Given Distance From Feature Point.}
  \label{fig:drilling_operation_accuracy_given_distance_from_feature_point}
\end{figure}
-->

### Sealant Application Accuracy Achieved

Table <!--\ref{tabel:sealant_application_accuracy_given_distance_from_work_surface}-->
and Figure <!--\ref{fig:sealant_application_accuracy_given_distance_from_feature_point}-->
show the x, y and z, error associated with the feature localization, detection,
and tracking pipeline using 20 features for each of 4 different tests done using
the Basler ACA1920-150uc and laser pointer, which were approximately 20 cm away
from the work surface. This give the Basler ACA1920-150uc with a 16 mm focal length
lens, a horizontal and vertical field of view (FOV) of 11.9 and 7.6 cm respectively,
as well as a ground sample distance (GSD) of .0062 cm. While the y accuracy increases
slightly as the distance away from the feature is increased, the x decreases slightly
and the z decreased significantly. However, due to the fact that z and x have decreasing
curves as the laser point get closer to the feature, the system is able to achieve
very low millimeter level accuracy in the x, y, and z due to the method's control
loop. The Basler ACA1920-150uc with a 16 mm focal length lens should be able to
achieve localization accuracies close to its GSD of .006 mm, which is exceeds the
capability of the both manipulator used. In order to improve the accuracy to this
level, the camera and laser should be calibrated on rig certified to this level
of accuracy. Alternatively, a stereo camera rig could be developed through the purchase
of another machine vision camera and those camera calibrated on a rig certified
to the accuracy mentioned above. In addition, the detection and localization pipeline
would need to be improved by using more recent and robust computer vision technique
in order to find the laser point and feature to within pixel, if not sub-pixel resolution.

<!--
\begin{table}[H]
\centering
\caption{Sealant Application Accuracy Given Distance From Work Surface.}
\label{tabel:sealant_application_accuracy_given_distance_from_work_surface}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}c|c|c|c|c|c|c|@{}}
\cmidrule(l){2-7}
                                                                                                                     & \textbf{x (mm)} & \textbf{x std (mm)} & \textbf{y (mm)} & \textbf{y std (mm)} & \textbf{z (mm)} & \textbf{z std (mm)} \\ \midrule
\multicolumn{1}{|c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Distance Less Than 5 mm from\\   Corner\end{tabular}}}       & 2.34            & 1.29                & 1.46            & 0.21                & 1.17            & 0.71                \\ \midrule
\multicolumn{1}{|c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Distance Between 5 and 10 mm\\   from Corner\end{tabular}}}  & 1.60            & 2.22                & 1.27            & 0.22                & 1.48            & 0.91                \\ \midrule
\multicolumn{1}{|c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Distance Between 10 and 20 mm\\   from Corner\end{tabular}}} & 4.38            & 2.46                & 0.78            & 0.34                & 3.81            & 0.32                \\ \midrule
\multicolumn{1}{|c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Distance Between 20 and 30 mm\\   from Corner\end{tabular}}} & 3.32            & 1.95                & 0.79            & 0.60                & 8.21            & 0.40                \\ \midrule
\multicolumn{1}{|c|}{\textbf{Mean}}                                                                                  & \textbf{2.91}   & \textbf{1.98}       & \textbf{1.08}   & \textbf{0.34}       & \textbf{3.67}   & \textbf{0.59}       \\ \bottomrule
\end{tabular}}
\end{table}
-->

<!--
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/seal_accuracy.png}
  \caption{Sealant Application Accuracy Given Distance From Feature Point.}
  \label{fig:sealant_application_accuracy_given_distance_from_feature_point}
\end{figure}
-->

## Summary

The purpose of this chapter was to validate each stage of multistage localization
approach and system implementation presented in Chapter 4 through testing. The accuracy
of each component of the multistage localization technique was determined on two
different ROS enabled robotic platforms, the Clearpath Robotics' Husky and Fetch
Robotics' Fetch. In addition, methods to improve the localization approach to the
accuracies limits of both manipulator's were presented.

# Conclusion and Future Work

The purpose of this paper was to present a multistage localization approach for
an autonomous industrial mobile manipulator (AIMM). This approach allows tasks with
operational scope outside the range of the robot's manipulator to be complete without
have to recalibrate the positon of the end-effector each time the robot's mobile
base moves to another position in order to complete said operation. This is achieved
through localizing the AIMM within its area of operation (AO) using adaptive Monte
Carlo localization (AMCL), which relies on the fused odometry and sensor messages
published by the robot, as well as a 2-D map of the AO, which is normally generated
using an optimization-based smoothing simultaneous localization and mapping (SLAM)
technique. The robot navigates to a predefined start location in the map incorporating
obstacle avoidance through the use of a technique called trajectory rollout. Once
there, the robot uses its RGB-D sensor to localize an augmented reality (AR) tag
in the map frame. One localized, the identity and the 3-D position and orientation,
collectively known as pose, of the tag are used to generate a list of initial feature
points and their locations based off of *a priori* knowledge. After the end-effector
moves to the rough location of a feature point provided by the AR tag localization,
the feature point's location, as well as the end-effector's positon and orientation
are refined to within a user specified tolerance through the use of a control loop,
which utilized images from a calibrated machine vision camera and a laser pointer,
simulating stereo vision, to localize the feature point in 3-D space using computer
vision. This approach was implemented on two different ROS enabled robots, Clearpath
Robotics' Husky and Fetch Robotics' Fetch , in order to show the utility of the
multistage localization approach in executing two tasks which are prevalent in both
manufacturing and construction: drilling and sealant application. The proposed approach
was able to achieve an average accuracy of $\pm$ 1 mm in these operations, verifying
it's efficacy for tasks which have a larger operational scope than that of the range
of the AIMM's manipulator and its robustness to general applications in manufacturing.

However, through the steps mentioned in Section 5.4 such as adding additional sensors,
like an additional IMU, LIDAR, or RGB-D camera in order to increase the accuracy
of the AMCL from approximately $\pm$ 6 centimeter to $\pm$ 5 millimeter. Additionally, in order
to improve the accuracy of the initial feature localization, sensors can be calibrated
on certified rigs, addition fiducial makers localization systems can be explored
or the initial feature location can be localized using the visual high-precision technique
presented in Section 2.3, which uses a calibration plate to localize the feature
within $\pm$ 0.1 mm. Further improvements in the initial feature localization could
be achieved by calibration the RGB-D and RGB cameras locations with respect to the
base-link need to within $\pm$ 1 mm through the use of packages like ROS's robot_calibration,
as well as upgrading to newer RGB-D sensor with better resolution, such as the ZED
stereo vision camera. The refined feature position's accuracy could conceivable
be brought down to the ground sample distance (GSD) of the camera, $/pm$ .006 mm
at 20 cm away from the work surface with the Basler ACA1920-150uc machine vision
camera used, which exceeds the capability of manipulators used by both robot platform
tested. In order to achieve this the calibration of the camera and laser must be
done on a certified rig or a high precision calibrated stereo camera used instead.
In addition, more recent computer vision and machine learning techniques should
be used to localize the feature point.

Before the proposed system can be implemented inside a manufacturing or construction
environment, addition future work besides increasing the accuracy of the system,
must be done. In order to make AIMMs financial viable they must be able to collaborate
together to complete a specified task, as well as be easily reprogramming through
methods like those suggested in [#Pedersen2016] and [#Pedersen2014]. Other difficult
hurdles exist, such as ensuring that the robot is 100% safe to use around human. Many
methods exist or have been proposed to ensure this; however, options such as a disabling
the robot when a human enters is area of operation as proposed by Kallweit in [#Sbanca2016]
while ensuring 100% safety will not produce the transformable production system
needed. AIMMs must be co-Robot, able to interact with human, while ensuring safety.
This can be achieve through the use of gesture controls, which if given act as alarm
behavior stopping whatever the robot is doing in favor of the desired action as
seen presented by Camprodon in [#Camprodon2015]. Additionally, this could be achieve
through speech recognition as presented by Mericli in [#Mericli2013].
